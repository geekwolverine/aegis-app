{"version":3,"sources":["webpack://microlib-example/./node_modules/kafkajs/index.js","webpack://microlib-example/./node_modules/kafkajs/src/admin/index.js","webpack://microlib-example/./node_modules/kafkajs/src/admin/instrumentationEvents.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/index.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/saslAuthenticator/awsIam.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/saslAuthenticator/index.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/saslAuthenticator/oauthBearer.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/saslAuthenticator/plain.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/saslAuthenticator/scram.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/saslAuthenticator/scram256.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/saslAuthenticator/scram512.js","webpack://microlib-example/./node_modules/kafkajs/src/cluster/brokerPool.js","webpack://microlib-example/./node_modules/kafkajs/src/cluster/connectionBuilder.js","webpack://microlib-example/./node_modules/kafkajs/src/cluster/index.js","webpack://microlib-example/./node_modules/kafkajs/src/constants.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/assignerProtocol.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/assigners/index.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/assigners/roundRobinAssigner/index.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/barrier.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/batch.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/consumerGroup.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/filterAbortedMessages.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/index.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/instrumentationEvents.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/offsetManager/index.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/offsetManager/initializeConsumerOffsets.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/offsetManager/isInvalidOffset.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/runner.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/seekOffsets.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/subscriptionState.js","webpack://microlib-example/./node_modules/kafkajs/src/env.js","webpack://microlib-example/./node_modules/kafkajs/src/errors.js","webpack://microlib-example/./node_modules/kafkajs/src/index.js","webpack://microlib-example/./node_modules/kafkajs/src/instrumentation/emitter.js","webpack://microlib-example/./node_modules/kafkajs/src/instrumentation/event.js","webpack://microlib-example/./node_modules/kafkajs/src/instrumentation/eventType.js","webpack://microlib-example/./node_modules/kafkajs/src/loggers/console.js","webpack://microlib-example/./node_modules/kafkajs/src/loggers/index.js","webpack://microlib-example/./node_modules/kafkajs/src/network/connection.js","webpack://microlib-example/./node_modules/kafkajs/src/network/connectionStatus.js","webpack://microlib-example/./node_modules/kafkajs/src/network/instrumentationEvents.js","webpack://microlib-example/./node_modules/kafkajs/src/network/requestQueue/index.js","webpack://microlib-example/./node_modules/kafkajs/src/network/requestQueue/socketRequest.js","webpack://microlib-example/./node_modules/kafkajs/src/network/socket.js","webpack://microlib-example/./node_modules/kafkajs/src/network/socketFactory.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/createTopicData.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/eosManager/index.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/eosManager/transactionStateMachine.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/eosManager/transactionStates.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/groupMessagesPerPartition.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/index.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/instrumentationEvents.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/messageProducer.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/partitioners/default/index.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/partitioners/default/murmur2.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/partitioners/default/partitioner.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/partitioners/default/randomBytes.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/partitioners/defaultJava/index.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/partitioners/defaultJava/murmur2.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/partitioners/index.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/responseSerializer.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/sendMessages.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/aclOperationTypes.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/aclPermissionTypes.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/aclResourceTypes.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/configResourceTypes.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/configSource.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/coordinatorTypes.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/crc32.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/encoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/error.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/isolationLevel.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/compression/gzip.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/compression/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/v0/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/v0/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/v1/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/v1/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/messageSet/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/messageSet/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/crc32C/crc32C.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/crc32C/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/header/v0/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/header/v0/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/record/v0/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/record/v0/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/v0/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/v0/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/alterConfigs/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/alterConfigs/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/alterConfigs/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/alterConfigs/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/alterConfigs/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiKeys.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiVersions/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiVersions/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiVersions/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiVersions/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiVersions/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiVersions/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiVersions/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createAcls/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createAcls/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createAcls/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createAcls/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createAcls/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createPartitions/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createPartitions/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createPartitions/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createPartitions/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createPartitions/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteAcls/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteAcls/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteAcls/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteAcls/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteAcls/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteGroups/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteGroups/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteGroups/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteGroups/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteGroups/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteRecords/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteRecords/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteRecords/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteRecords/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteRecords/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteTopics/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteTopics/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteTopics/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteTopics/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteTopics/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeAcls/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeAcls/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeAcls/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeAcls/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeAcls/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeConfigs/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeGroups/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeGroups/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeGroups/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeGroups/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeGroups/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeGroups/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeGroups/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/endTxn/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/endTxn/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/endTxn/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/endTxn/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/endTxn/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v10/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v10/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v11/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v11/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v4/decodeMessages.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v4/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v4/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v5/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v5/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v6/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v6/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v7/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v7/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v8/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v8/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v9/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v9/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/findCoordinator/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/heartbeat/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/heartbeat/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/heartbeat/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/heartbeat/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/heartbeat/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/heartbeat/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/heartbeat/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/heartbeat/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/heartbeat/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/initProducerId/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/initProducerId/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/initProducerId/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/initProducerId/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/initProducerId/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v4/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v4/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v5/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v5/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/leaveGroup/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listGroups/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listGroups/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listGroups/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listGroups/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listGroups/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listGroups/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listGroups/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v4/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v4/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v5/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v5/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v6/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v6/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v4/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v4/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v5/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v5/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v4/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v4/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v4/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v4/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v5/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v5/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v6/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v6/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v7/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v7/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslHandshake/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslHandshake/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslHandshake/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslHandshake/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslHandshake/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/syncGroup/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/syncGroup/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/syncGroup/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/syncGroup/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/syncGroup/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/syncGroup/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/syncGroup/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/syncGroup/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/syncGroup/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/resourcePatternTypes.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/resourceTypes.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/awsIam/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/awsIam/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/awsIam/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/oauthBearer/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/oauthBearer/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/oauthBearer/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/plain/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/plain/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/plain/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/scram/finalMessage/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/scram/finalMessage/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/scram/firstMessage/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/scram/firstMessage/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/scram/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/timestampTypes.js","webpack://microlib-example/./node_modules/kafkajs/src/retry/defaults.js","webpack://microlib-example/./node_modules/kafkajs/src/retry/defaults.test.js","webpack://microlib-example/./node_modules/kafkajs/src/retry/index.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/arrayDiff.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/bufferedAsyncIterator.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/concurrency.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/flatten.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/groupBy.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/lock.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/long.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/sharedPromiseTo.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/shuffle.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/sleep.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/swapObject.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/waitFor.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/websiteUrl.js","webpack://microlib-example/./node_modules/nanoid/index.js","webpack://microlib-example/./node_modules/nanoid/url-alphabet/index.js"],"names":[],"mappings":";;;;;;;;;;;;;AAAA,cAAc,mBAAO,CAAC,kDAAO;AAC7B,2BAA2B,mBAAO,CAAC,wFAA0B;AAC7D,yBAAyB,mBAAO,CAAC,gGAAiC;AAClE,qBAAqB,mBAAO,CAAC,8FAA6B;AAC1D,oBAAoB,mBAAO,CAAC,4GAAoC;AAChE,sBAAsB,mBAAO,CAAC,0FAA8B;AAC5D,4BAA4B,mBAAO,CAAC,sGAAoC;AACxE,qBAAqB,mBAAO,CAAC,wFAA6B;AAC1D,yBAAyB,mBAAO,CAAC,gGAAiC;AAClE,0BAA0B,mBAAO,CAAC,kGAAkC;AACpE,2BAA2B,mBAAO,CAAC,oGAAmC;AACtE,6BAA6B,mBAAO,CAAC,wGAAqC;AAC1E,eAAe,mBAAO,CAAC,0DAAc;AACrC,OAAO,SAAS,GAAG,mBAAO,CAAC,kEAAe;;AAE1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ACrCA,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,uBAAuB,mBAAO,CAAC,iEAAa;AAC5C,oCAAoC,mBAAO,CAAC,yFAA4B;AACxE,OAAO,+CAA+C,GAAG,mBAAO,CAAC,0FAAyB;AAC1F,OAAO,SAAS,GAAG,mBAAO,CAAC,+DAAY;AACvC;AACA;AACA;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,uDAAW;AACvB,OAAO,gBAAgB,GAAG,mBAAO,CAAC,uEAAmB;AACrD,8BAA8B,mBAAO,CAAC,mGAAiC;AACvE,2BAA2B,mBAAO,CAAC,6FAA8B;AACjE,4BAA4B,mBAAO,CAAC,+FAA+B;AACnE,6BAA6B,mBAAO,CAAC,iGAAgC;AACrE,+BAA+B,mBAAO,CAAC,qGAAkC;AACzE,OAAO,iCAAiC,GAAG,mBAAO,CAAC,6DAAc;;AAEjE,OAAO,sBAAsB;;AAE7B;;AAEA,OAAO,wBAAwB;AAC/B;AACA;AACA,8BAA8B,IAAI;AAClC;;AAEA,gDAAgD;AAChD;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,cAAc;AACzB;AACA;AACA;AACA;AACA,WAAW,sBAAsB,yBAAyB,eAAe,WAAW,EAAE;AACtF;AACA;;AAEA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,6BAA6B;AACxC,WAAW,4BAA4B;AACvC,WAAW,mCAAmC;AAC9C,WAAW,8BAA8B;AACzC;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;;AAEA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA,cAAc;AACd;AACA;AACA;AACA;AACA;;AAEA;AACA,cAAc;AACd;AACA;AACA,WAAW,gBAAgB;AAC3B;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,MAAM;AACnB,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,cAAc;AACd;AACA,+BAA+B,uDAAuD;AACtF;AACA,iEAAiE,OAAO;AACxE;;AAEA,wBAAwB,QAAQ;AAChC;AACA;AACA;AACA;;AAEA,4CAA4C,QAAQ;AACpD;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,mCAAmC,gCAAgC;;AAEnE;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;;AAEA;AACA,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA,aAAa,MAAM;AACnB,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,cAAc;AACd;AACA,mCAAmC,yCAAyC;AAC5E;AACA,2EAA2E,gBAAgB;AAC3F;AACA;AACA;AACA;;AAEA,iCAAiC,QAAQ;AACzC;AACA;AACA;AACA;;AAEA,qDAAqD,QAAQ;AAC7D;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,uCAAuC,yCAAyC;AAChF,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,SAAS;AACtB,aAAa,OAAO;AACpB,cAAc;AACd;AACA,+BAA+B,kBAAkB;AACjD;AACA,iEAAiE,OAAO;AACxE;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,mCAAmC,kBAAkB;;AAErD;AACA;AACA;AACA;;AAEA;AACA,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,OAAO;AACpB;;AAEA;AACA;AACA,0DAA0D,MAAM;AAChE;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,4CAA4C,2BAA2B;AACvE,WAAW;AACX;;AAEA;AACA;AACA;AACA;AACA,4CAA4C,2BAA2B;AACvE,WAAW;AACX;;AAEA,eAAe,6BAA6B;AAC5C,eAAe,4BAA4B;AAC3C,oCAAoC,oBAAoB;AACxD;AACA;AACA;AACA,oCAAoC,0BAA0B;AAC9D;AACA,SAAS;AACT,OAAO;AACP;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;;AAEA;AACA;AACA,0DAA0D,MAAM;AAChE;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,+CAA+C,2BAA2B;;AAE1E;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA,eAAe,6BAA6B;;AAE5C;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA,eAAe,4BAA4B;;AAE3C,mCAAmC,oBAAoB;AACvD;AACA;AACA;AACA;AACA,sCAAsC,2BAA2B;AACjE;AACA,SAAS;AACT,OAAO;AACP;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,aAAa,QAAQ;AACrB,cAAc;AACd;AACA,+BAA+B,iDAAiD;AAChF;AACA,4DAA4D,QAAQ;AACpE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,4DAA4D,UAAU;AACtE;AACA;AACA;AACA,gEAAgE,YAAY;AAC5E,gBAAgB;AAChB,OAAO;AACP;AACA,SAAS,6BAA6B;AACtC;AACA;AACA,KAAK;;AAEL;AACA;AACA,oCAAoC,oBAAoB;AACxD;AACA,0DAA0D,8BAA8B;AACxF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX,4BAA4B,qDAAqD;;AAEjF;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;;AAEA,yCAAyC,oBAAoB;AAC7D,kDAAkD,8BAA8B;AAChF;AACA;AACA;AACA,OAAO;;AAEP,cAAc;AACd,KAAK;;AAEL;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,cAAc;AACd;AACA,+BAA+B,mCAAmC;AAClE;AACA,4DAA4D,QAAQ;AACpE;;AAEA;AACA,0DAA0D,MAAM;AAChE;;AAEA;AACA;AACA;AACA,qCAAqC,0BAA0B;AAC/D,KAAK;;AAEL,uBAAuB,+CAA+C;AACtE;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,iBAAiB;AAC9B,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,6BAA6B,6BAA6B;AAC1D;AACA,4DAA4D,QAAQ;AACpE;;AAEA;AACA,0DAA0D,MAAM;AAChE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL,8BAA8B,6BAA6B;AAC3D;;AAEA;AACA;AACA,6EAA6E,kBAAkB;AAC/F;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA,uBAAuB,QAAQ;;AAE/B;AACA,uBAAuB,qBAAqB;AAC5C;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA,mCAAmC,2BAA2B;AAC9D,+BAA+B,qBAAqB;AACpD;AACA,oCAAoC,yBAAyB;AAC7D;AACA,KAAK;;AAEL;AACA,aAAa,2BAA2B;AACxC,aAAa,QAAQ;AACrB,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,mBAAmB;AACnC,gBAAgB,OAAO;AACvB,gBAAgB,cAAc;AAC9B;AACA,kCAAkC,6BAA6B;AAC/D;AACA,oEAAoE,UAAU;AAC9E;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,iCAAiC,iBAAiB,IAAI,4BAA4B;AAClF;AACA;;AAEA;;AAEA;AACA;AACA,iCAAiC,iBAAiB,IAAI,4BAA4B;AAClF;AACA;;AAEA;AACA;AACA;;AAEA;AACA,aAAa,cAAc;AAC3B;AACA,wCAAwC,YAAY,IAAI,+BAA+B;AACvF;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;;AAEA;AACA;AACA;AACA,oBAAoB,YAAY;AAChC;AACA;;AAEA,gBAAgB;AAChB,OAAO;AACP;AACA,qDAAqD,0CAA0C;AAC/F;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,sBAAsB;AACnC,aAAa,QAAQ;AACrB,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,mBAAmB;AACnC,gBAAgB,OAAO;AACvB,gBAAgB,2BAA2B;AAC3C;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,+BAA+B,0BAA0B;AACzD;AACA,oEAAoE,UAAU;AAC9E;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,iCAAiC,iBAAiB,IAAI,4BAA4B;AAClF;AACA;;AAEA;;AAEA;AACA;AACA,iCAAiC,iBAAiB,IAAI,4BAA4B;AAClF;AACA;;AAEA;;AAEA;AACA,aAAa,gBAAgB;AAC7B;AACA,0CAA0C,cAAc,IAAI,+BAA+B;AAC3F;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,0CAA0C,mCAAmC;AAC7E;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;;AAEA;AACA;AACA;AACA,oBAAoB,YAAY;AAChC;AACA;;AAEA,gBAAgB;AAChB,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,qBAAqB;AACrC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,yBAAyB;AACzC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,cAAc;AAC9B,gBAAgB,cAAc;AAC9B;AACA;AACA,WAAW,SAAS;;AAEpB;AACA;AACA;AACA;AACA,gEAAgE,MAAM;AACtE;;AAEA;AACA;AACA,WAAW;AACX,sDAAsD,MAAM,IAAI,UAAU;AAC1E;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,qBAAqB;AACrC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,yBAAyB;AACzC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,cAAc;AAC9B,gBAAgB,cAAc;AAC9B;AACA,qCAAqC,cAAc,KAAK;AACxD;AACA;AACA;AACA,8DAA8D,MAAM;AACpE;AACA,OAAO;AACP;;AAEA,6CAA6C,SAAS;;AAEtD;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,cAAc;AAC9B,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA,WAAW,0CAA0C,2BAA2B,aAAa;AAC7F,gCAAgC,qBAAqB;AACrD;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,iBAAiB;AACjC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA;AACA;AACA;AACA,+CAA+C,SAAS;AACxD;AACA;AACA;;AAEA,YAAY;AACZ;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B;AACA,eAAe,OAAO;AACtB,gBAAgB,wBAAwB;AACxC;AACA,cAAc;AACd;AACA;AACA;AACA;AACA,gEAAgE,UAAU;AAC1E;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA,kDAAkD,uBAAuB;AACzE;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,8CAA8C;AAC9C;AACA;AACA,OAAO,IAAI;AACX;;AAEA;AACA,sCAAsC,wBAAwB;AAC9D;AACA,eAAe,SAAS,mDAAmD,WAAW;AACtF;AACA,OAAO;AACP;;AAEA;;AAEA,YAAY;AACZ;;AAEA;AACA;AACA;AACA,aAAa,SAAS;AACtB,cAAc;AACd;AACA,eAAe,MAAM;AACrB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA;AACA,mEAAmE,SAAS;AAC5E;;AAEA;;AAEA;AACA,kEAAkE,+BAA+B;AACjG;;AAEA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,6DAA6D,UAAU;AACvE;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,oBAAoB,UAAU;AAC9B,0BAA0B,4BAA4B;AACtD,sBAAsB;AACtB,aAAa;AACb;AACA,mBAAmB,YAAY;;AAE/B,sCAAsC,UAAU;;AAEhD;;AAEA,oCAAoC,UAAU;;AAE9C;AACA,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,iBAAiB;AAC9B,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,qCAAqC,oBAAoB;AACzD;AACA,2DAA2D,MAAM;AACjE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,yBAAyB,oBAAoB;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;AACT;AACA;AACA,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,gDAAgD,0CAA0C;AAC1F;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,qBAAqB,oBAAoB;AACzC,qDAAqD,SAAS;AAC9D,wCAAwC,WAAW,oBAAoB,GAAG;AAC1E;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,uCAAuC,2BAA2B;AAClE;AACA;AACA;AACA;AACA,mBAAmB;AACnB,iBAAiB;AACjB,eAAe;AACf;AACA;AACA;AACA,aAAa;AACb;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA,OAAO;AACP;AACA;AACA;AACA,cAAc,QAAQ;AACtB;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,gBAAgB;AAC7B,cAAc;AACd;AACA,eAAe,OAAO;AACtB;AACA,6BAA6B,MAAM;AACnC;AACA,8DAA8D,IAAI;AAClE;AACA;AACA;AACA;;AAEA;AACA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;;AAEA;AACA,mBAAmB,OAAO;AAC1B;AACA;;AAEA;AACA,mBAAmB,eAAe;AAClC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,kCAAkC,sBAAsB,IAAI,4BAA4B;AACxF;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,yCAAyC,gCAAgC,IAAI;AAC7E;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,mCAAmC,2BAA2B,IAAI,4BAA4B;AAC9F;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,iCAAiC,yBAAyB,IAAI,4BAA4B;AAC1F;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,iCAAiC,MAAM;;AAEvC;AACA,OAAO;AACP;AACA,+CAA+C,0CAA0C;AACzF;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,iBAAiB;AAC9B,aAAa,OAAO;AACpB,aAAa,wBAAwB;AACrC,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,kBAAkB;AAC/B,aAAa,mBAAmB;AAChC,cAAc;AACd;AACA,eAAe,OAAO;AACtB,eAAe,OAAO;AACtB,eAAe,OAAO;AACtB,eAAe,OAAO;AACtB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,mEAAmE,UAAU;AAC7E;;AAEA;AACA;AACA;AACA;AACA,gDAAgD,oBAAoB;AACpE;AACA;;AAEA;AACA;AACA;AACA,oEAAoE,eAAe;AACnF;;AAEA;AACA;AACA;AACA,kEAAkE,aAAa;AAC/E;;AAEA;;AAEA;AACA;AACA;AACA;AACA,eAAe,YAAY;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,gBAAgB;AAChB,OAAO;AACP;AACA,iDAAiD,0CAA0C;AAC3F;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,iBAAiB;AAC9B,cAAc;AACd;AACA,eAAe,OAAO;AACtB;AACA,6BAA6B,UAAU;AACvC;AACA,qEAAqE,QAAQ;AAC7E;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU,YAAY;AACtB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,uBAAuB,OAAO;AAC9B;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU,eAAe;AACzB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,kCAAkC,sBAAsB,IAAI,4BAA4B;AACxF;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,yCAAyC,gCAAgC,IAAI;AAC7E;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,mCAAmC,2BAA2B,IAAI,4BAA4B;AAC9F;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,iCAAiC,yBAAyB,IAAI,4BAA4B;AAC1F;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,eAAe,kBAAkB,4BAA4B,UAAU;AACvE,gBAAgB;AAChB,OAAO;AACP;AACA,+CAA+C,0CAA0C;AACzF;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA,aAAa,kCAAkC;AAC/C;AACA;AACA,wEAAwE,UAAU;AAClF;;AAEA;AACA;AACA;AACA,oDAAoD,UAAU;AAC9D;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;AACA,cAAc,OAAO;AACrB;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACr+CA,mBAAmB,mBAAO,CAAC,2EAAqB;AAChD,sBAAsB,mBAAO,CAAC,qGAAkC;AAChE,iCAAiC,mBAAO,CAAC,6FAA8B;AACvE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA,aAAa,mBAAO,CAAC,+DAAe;AACpC,aAAa,mBAAO,CAAC,+DAAe;AACpC,OAAO,qBAAqB,GAAG,mBAAO,CAAC,yGAAiC;AACxE,OAAO,mBAAmB,GAAG,mBAAO,CAAC,mFAAsB;AAC3D,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;AACxD,gBAAgB,mBAAO,CAAC,6FAA8B;AACtD,0BAA0B,mBAAO,CAAC,yFAAqB;AACvD,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,OAAO,iCAAiC,GAAG,mBAAO,CAAC,6FAA8B;AACjF,wBAAwB,mBAAO,CAAC,qFAA0B;;AAE1D;AACA;AACA;AACA;AACA;;AAEA,WAAW,sCAAsC;AACjD;AACA;AACA;;AAEA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,gCAAgC;AAC7C,aAAa,6BAA6B;AAC1C,aAAa,OAAO;AACpB,aAAa,kCAAkC;AAC/C;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB;AACA;AACA,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,4BAA4B,qBAAqB,GAAG,qBAAqB;;AAEzE;AACA;AACA,wCAAwC,mBAAmB;AAC3D,KAAK;;AAEL;;AAEA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA,WAAW,kBAAkB;AAC7B;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,2DAA2D,4BAA4B;AACvF;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA,YAAY;AACZ,aAAa,SAAS;AACtB;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,8EAA8E;AAC9F;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yCAAyC,uBAAuB;AAChE,yCAAyC,uBAAuB;AAChE;AACA,qCAAqC;AACrC;AACA;AACA;AACA;AACA,yCAAyC,uBAAuB;AAChE;AACA;AACA;AACA,iCAAiC;AACjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yCAAyC,wBAAwB;AACjE;AACA,qCAAqC;AACrC;AACA,iCAAiC;AACjC;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,uCAAuC;AACpD,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA,sEAAsE,oBAAoB;AAC1F;AACA,8BAA8B,mBAAmB;AACjD,OAAO;AACP;AACA,KAAK;;AAEL;;AAEA;AACA;AACA,yBAAyB,mBAAmB;AAC5C;;AAEA;AACA;AACA,SAAS;AACT,gCAAgC,iCAAiC;AACjE;;AAEA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,mBAAmB,uCAAuC;AAC1D;AACA,uDAAuD,uCAAuC;AAC9F;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,uDAAuD;AACpE,eAAe;AACf;AACA,8BAA8B,2BAA2B;AACzD;AACA;AACA,6DAA6D,2BAA2B;AACxF;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,aAAa,OAAO;AACpB;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,MAAM;AACnB,sCAAsC,mCAAmC,2BAA2B,GAAG;AACvG,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,oBAAoB,oBAAoB;AACxC;AACA,wDAAwD,oBAAoB;AAC5E;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,mBAAmB,mDAAmD;AACtE;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,uBAAuB;AACpC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,kBAAkB;AAClC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA,eAAe;AACf;AACA,qBAAqB,oCAAoC;AACzD;AACA;AACA,mBAAmB,oCAAoC;AACvD;;AAEA;AACA;AACA;AACA,sDAAsD,4BAA4B;AAClF,0BAA0B,0CAA0C;AACpE,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA,6BAA6B;AAC7B;AACA;AACA;AACA,eAAe;AACf;AACA,sBAAsB,8DAA8D;AACpF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA,6BAA6B;AAC7B;AACA;AACA;AACA,eAAe;AACf;AACA,qBAAqB,kBAAkB;AACvC;AACA,yDAAyD,kBAAkB;AAC3E;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,MAAM;AACnB,eAAe;AACf;AACA,wBAAwB,WAAW;AACnC;AACA,4DAA4D,WAAW;AACvE;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,QAAQ;AACrB;AACA,aAAa,OAAO;AACpB;AACA,eAAe;AACf;AACA,sBAAsB,+CAA+C;AACrE;AACA,0DAA0D,gCAAgC;AAC1F;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,QAAQ;AACrB;AACA,aAAa,OAAO;AACpB;AACA,eAAe;AACf;AACA,0BAA0B,wDAAwD;AAClF;AACA;AACA,wBAAwB,yCAAyC;AACjE;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,aAAa,OAAO;AACpB;AACA;AACA,eAAe;AACf;AACA,sBAAsB,yBAAyB;AAC/C;AACA,0DAA0D,kBAAkB;AAC5E;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,4CAA4C;AACzD;AACA;AACA;AACA;AACA,sCAAsC;AACtC,aAAa,QAAQ;AACrB,eAAe;AACf;AACA,yBAAyB,qCAAqC;AAC9D;AACA,6DAA6D,6BAA6B;AAC1F;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,wCAAwC;AACrD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sCAAsC;AACtC,aAAa,QAAQ;AACrB,eAAe;AACf;AACA,sBAAsB,kCAAkC;AACxD;AACA,0DAA0D,0BAA0B;AACpF;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,wBAAwB,sCAAsC;AAC9D;AACA,4DAA4D,sCAAsC;AAClG;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA,4BAA4B,qDAAqD;AACjF;AACA;AACA;AACA;AACA;AACA,0BAA0B,qDAAqD;AAC/E;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,yBAAyB,sDAAsD;AAC/E;AACA;AACA,uBAAuB,sDAAsD;AAC7E;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,oBAAoB;AACjC,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,oBAAoB;AACjC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,6BAA6B;AAC7C;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,eAAe;AACf;AACA,yBAAyB,8DAA8D;AACvF;AACA;AACA,uBAAuB,8DAA8D;AACrF;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,eAAe;AACf;AACA,gBAAgB,gEAAgE;AAChF;AACA;AACA,cAAc,gEAAgE;AAC9E;AACA;;AAEA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,SAAS;AACtB;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,wBAAwB;AACrC;AACA;AACA;AACA;AACA,qCAAqC,yBAAyB;AAC9D,qCAAqC,yBAAyB;AAC9D;AACA;AACA;AACA,eAAe,eAAe;AAC9B;AACA;AACA;AACA;AACA;AACA;AACA,sCAAsC,iDAAiD;AACvF,sCAAsC,iDAAiD;AACvF;AACA,kCAAkC;AAClC;AACA;AACA;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,kBAAkB;AAClC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,uBAAuB,SAAS;AAChC;AACA,2DAA2D,SAAS;AACpE;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,iCAAiC;AAC9C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA,oBAAoB,MAAM;AAC1B;AACA,wDAAwD,iBAAiB;AACzE;;AAEA;AACA;AACA,aAAa,+BAA+B;AAC5C,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,iCAAiC;AAC9C,eAAe;AACf;AACA,oBAAoB,UAAU;AAC9B;AACA,wDAAwD,UAAU;AAClE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC37BA,eAAe,mBAAO,CAAC,4FAA4B;AACnD,OAAO,iCAAiC,GAAG,mBAAO,CAAC,0DAAc;;AAEjE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,aAAa;AACxB,sBAAsB,KAAK,GAAG,KAAK;;AAEnC;AACA,2DAA2D,SAAS;AACpE,mCAAmC,oBAAoB;AACvD,mEAAmE,SAAS;AAC5E,KAAK;AACL;AACA,+CAA+C,UAAU;AACzD;AACA,wCAAwC,SAAS;AACjD;AACA;AACA;AACA;;;;;;;;;;;;;;AC1CA,OAAO,mBAAmB,GAAG,mBAAO,CAAC,sFAAyB;AAC9D,gBAAgB,mBAAO,CAAC,gGAAiC;AACzD,2BAA2B,mBAAO,CAAC,6EAAS;AAC5C,8BAA8B,mBAAO,CAAC,mFAAY;AAClD,8BAA8B,mBAAO,CAAC,mFAAY;AAClD,4BAA4B,mBAAO,CAAC,+EAAU;AAC9C,iCAAiC,mBAAO,CAAC,yFAAe;AACxD,OAAO,iCAAiC,GAAG,mBAAO,CAAC,0DAAc;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,gBAAgB,UAAU;AAC1B;AACA;;AAEA,qEAAqE,YAAY;AACjF;AACA;AACA,gBAAgB,UAAU;AAC1B;AACA;;AAEA,qCAAqC,wCAAwC;AAC7E;AACA,eAAe,2BAA2B;AAC1C;AACA,uCAAuC,8BAA8B;AACrE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,eAAe,+BAA+B;AAC9C;AACA;AACA;;AAEA,2CAA2C,wCAAwC;AACnF;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1EA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,oBAAoB,mBAAO,CAAC,sGAAiC;AAC7D,OAAO,iCAAiC,GAAG,mBAAO,CAAC,0DAAc;;AAEjE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;;AAEA,WAAW,sBAAsB;;AAEjC;;AAEA;AACA;AACA;;AAEA;AACA;AACA,WAAW,aAAa;AACxB,sBAAsB,KAAK,GAAG,KAAK;;AAEnC;AACA,+DAA+D,SAAS;AACxE,mCAAmC,oBAAoB;AACvD,uEAAuE,SAAS;AAChF,KAAK;AACL;AACA,mDAAmD,UAAU;AAC7D;AACA,wCAAwC,SAAS;AACjD;AACA;AACA;AACA;;;;;;;;;;;;;;ACvDA,cAAc,mBAAO,CAAC,0FAA2B;AACjD,OAAO,iCAAiC,GAAG,mBAAO,CAAC,0DAAc;;AAEjE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA;AACA;;AAEA;AACA;AACA,WAAW,aAAa;AACxB,sBAAsB,KAAK,GAAG,KAAK;;AAEnC;AACA,yDAAyD,SAAS;AAClE,mCAAmC,oBAAoB;AACvD,iEAAiE,SAAS;AAC1E,KAAK;AACL;AACA,6CAA6C,UAAU;AACvD;AACA,wCAAwC,SAAS;AACjD;AACA;AACA;AACA;;;;;;;;;;;;;;ACjCA,eAAe,mBAAO,CAAC,sBAAQ;AAC/B,cAAc,mBAAO,CAAC,0FAA2B;AACjD,OAAO,2DAA2D,GAAG,mBAAO,CAAC,0DAAc;;AAE3F;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mBAAmB,YAAY;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,WAAW;AACxB,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,aAAa,iBAAiB;AAC9B;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,gCAAgC,WAAW;;AAE3C;AACA;;AAEA;AACA,WAAW,SAAS;AACpB,WAAW,mBAAmB;AAC9B,sBAAsB,KAAK,GAAG,KAAK;;AAEnC;AACA,kDAAkD,YAAY;AAC9D;;AAEA;AACA,4DAA4D,SAAS;AACrE;;AAEA,kDAAkD,SAAS;AAC3D;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,2BAA2B,OAAO,eAAe,SAAS;AAC1D,KAAK;AACL,0DAA0D,OAAO,WAAW,UAAU;AACtF,wCAAwC,SAAS;AACjD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,kCAAkC,WAAW,EAAE,wBAAwB;AACvE,gDAAgD,qBAAqB;AACrE;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,WAAW,SAAS;AACpB;AACA,WAAW,gBAAgB;;AAE3B;AACA;AACA,WAAW,OAAO;AAClB;AACA;;AAEA;AACA;AACA,WAAW,OAAO,gCAAgC,WAAW,4BAA4B,cAAc;AACvG;AACA;;AAEA;AACA;AACA,4BAA4B,yBAAyB,KAAK,YAAY;AACtE,gDAAgD,eAAe;AAC/D;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,gBAAgB,uBAAuB,KAAK,kBAAkB;AAC9D;;AAEA;AACA;AACA;AACA;AACA;AACA,gBAAgB,qBAAqB,KAAK,OAAO;AACjD;;AAEA;AACA;AACA;AACA;AACA,WAAW,WAAW;AACtB;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW,WAAW;AACtB;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrUA,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6EAAS;;AAE5C;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6EAAS;;AAE5C;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,eAAe,mBAAO,CAAC,6DAAW;AAClC,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,kBAAkB,mBAAO,CAAC,yEAAoB;AAC9C,OAAO,8CAA8C,GAAG,mBAAO,CAAC,uDAAW;;AAE3E,OAAO,uBAAuB;AAC9B,wCAAwC,mBAAmB;AAC3D;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,gDAAgD;AAC7D,aAAa,6BAA6B;AAC1C,aAAa,mCAAmC;AAChD,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,wCAAwC;;AAExC;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA,eAAe,mBAAmB;AAClC;AACA,eAAe,4CAA4C;AAC3D;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,sFAAsF,UAAU;AAChG,aAAa;AACb;AACA,SAAS;AACT,wCAAwC,wBAAwB;AAChE;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,gBAAgB,aAAa;AAC7B;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe;AACf;AACA;AACA;AACA,WAAW,iCAAiC;;AAE5C;AACA;AACA;AACA;;AAEA;;AAEA;AACA,iCAAiC,2BAA2B;AAC5D;;AAEA;AACA,0DAA0D,mBAAmB;AAC7E;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;;AAEA;AACA;AACA;AACA;AACA;AACA,gEAAgE,mBAAmB;AACnF;AACA,eAAe;AACf,aAAa;AACb,WAAW;AACX;AACA;;AAEA,2DAA2D,SAAS,QAAQ,OAAO;AACnF;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;;AAET;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,oBAAoB,SAAS;AAC7B;;AAEA;AACA,gDAAgD,OAAO;AACvD;;AAEA;AACA;AACA;;AAEA;AACA;AACA,aAAa,UAAU,iCAAiC,gBAAgB;AACxE,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,4CAA4C,SAAS;AACrD;AACA,+BAA+B,iBAAiB;AAChD,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA,oCAAoC,4BAA4B;AAChE;;AAEA;AACA;AACA;AACA,sCAAsC,SAAS;AAC/C,OAAO;AACP;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX,0EAA0E,wBAAwB;AAClG,6CAA6C,kBAAkB;AAC/D;;AAEA;AACA,8BAA8B,wCAAwC;AACtE;AACA;AACA,KAAK;AACL;AACA;;;;;;;;;;;;;;AC3VA,mBAAmB,mBAAO,CAAC,+EAAuB;AAClD,OAAO,mDAAmD,GAAG,mBAAO,CAAC,uDAAW;;AAEhF;AACA,aAAa,OAAO;AACpB,cAAc,gBAAgB,8CAA8C,yBAAyB;AACrG;;AAEA;AACA,WAAW,OAAO;AAClB,WAAW,qCAAqC;AAChD,WAAW,0BAA0B;AACrC,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,QAAQ;AACnB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,mCAAmC;AAC9C,WAAW,6BAA6B;AACxC,WAAW,qCAAqC;AAChD,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,kDAAkD,MAAM,eAAe,cAAc;AACrF;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA,wDAAwD,UAAU;AAClE;AACA,gCAAgC,kBAAkB,iBAAiB,QAAQ;AAC3E;AACA;AACA,KAAK;AACL;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA,mBAAmB,mBAAmB,KAAK;AAC3C;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;AACA;;;;;;;;;;;;;;ACjHA,mBAAmB,mBAAO,CAAC,sEAAc;AACzC,aAAa,mBAAO,CAAC,+DAAe;AACpC,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,0BAA0B,mBAAO,CAAC,oFAAqB;AACvD,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,OAAO,iCAAiC,GAAG,mBAAO,CAAC,6DAAc;AACjE;AACA;AACA;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,uDAAW;AACvB,0BAA0B,mBAAO,CAAC,6FAA8B;;AAEhE,OAAO,OAAO;;AAEd,2BAA2B,oBAAoB;AAC/C;AACA;AACA,CAAC;;AAED;AACA;AACA,aAAa,OAAO;AACpB,aAAa,cAAc;AAC3B,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,mCAAmC;AAChD,aAAa,6BAA6B;AAC1C,aAAa,qCAAqC;AAClD,aAAa,IAAI;AACjB,aAAa,qCAAqC;AAClD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,gBAAgB,aAAa;AAC7B,kCAAkC,aAAa;AAC/C;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA,kBAAkB,cAAc,KAAK;AACrC;AACA;AACA;AACA,kDAAkD,SAAS;AAC3D,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,cAAc;AACd;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,SAAS;AACtB,cAAc;AACd;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,oBAAoB,SAAS;AAC7B;AACA,+CAA+C,SAAS;AACxD,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA,WAAW,WAAW;;AAEtB;AACA;AACA;;AAEA,0CAA0C,gCAAgC;;AAE1E;AACA;AACA,qCAAqC,sBAAsB;AAC3D;AACA;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,eAAe,0CAA0C;AACzD;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB;AACxB;AACA;AACA,WAAW,WAAW;AACtB;AACA,4EAA4E,QAAQ;AACpF;;AAEA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,kBAAkB;AAC/B,eAAe,OAAO;AACtB;AACA,0BAA0B;AAC1B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,8DAA8D,+BAA+B;AAC7F;;AAEA,aAAa,SAAS;AACtB;AACA,cAAc;AACd,KAAK,IAAI;AACT;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,uDAAuD;AACpE,eAAe;AACf;AACA,8BAA8B,qDAAqD;AACnF;AACA;AACA,eAAe,cAAc;AAC7B;AACA;AACA,SAAS;AACT,sCAAsC,6BAA6B;AACnE,OAAO;AACP;AACA;AACA;AACA,+BAA+B,UAAU;AACzC;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;;AAEA;AACA,oEAAoE;AACpE;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,uDAAuD;AACpE,eAAe;AACf;AACA,sCAAsC,2BAA2B;AACjE,oEAAoE,iBAAiB;AACrF;AACA;AACA,oEAAoE,2BAA2B;AAC/F;AACA;AACA;AACA,WAAW;AACX;AACA,SAAS;AACT;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA;AACA;AACA,aAAa;;AAEb;AACA;;AAEA;AACA;AACA,OAAO;AACP,KAAK;;AAEL;AACA;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA,iBAAiB,gBAAgB;AACjC;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B;AACA;AACA;AACA,gDAAgD,eAAe;AAC/D;AACA;AACA;AACA,eAAe,8CAA8C;AAC7D;AACA;AACA;AACA;AACA,qCAAqC,4BAA4B;AACjE,qCAAqC,4BAA4B;AACjE,qCAAqC,4BAA4B;AACjE;AACA,iCAAiC;AACjC;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,YAAY;AACzB,cAAc;AACd;;AAEA;AACA;AACA,aAAa,kDAAkD;AAC/D;AACA;AACA;AACA;AACA;AACA,oEAAoE,gBAAgB;;AAEpF,oCAAoC;;AAEpC;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA,4CAA4C,SAAS;AACrD;;AAEA,aAAa,0BAA0B;AACvC;AACA;AACA;AACA;AACA,SAAS;AACT,OAAO;;AAEP;AACA,KAAK;;AAEL;AACA;AACA,wEAAwE;;AAExE;AACA;AACA,kDAAkD,oBAAoB;AACtE;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,oBAAoB,UAAU;AAC9B;AACA,kDAAkD;AAClD;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,cAAc;AAC3B,aAAa,OAAO;AACpB;AACA,yBAAyB,oCAAoC;AAC7D,oDAAoD,UAAU;;AAE9D;AACA;AACA;AACA;;;;;;;;;;;;;;ACzfA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACRA,gBAAgB,mBAAO,CAAC,2EAAqB;AAC7C,gBAAgB,mBAAO,CAAC,2EAAqB;;AAE7C;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,cAAc;AAC3B,aAAa,OAAO;AACpB;AACA;AACA;AACA,UAAU,8CAA8C;AACxD;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,qBAAqB;AAClC;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA,UAAU,kDAAkD;AAC5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,mCAAmC,oBAAoB;AACvD,0BAA0B,sBAAsB;;AAEhD;AACA;AACA;;AAEA;AACA;AACA,gFAAgF;AAChF;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrFA,mBAAmB,mBAAO,CAAC,uGAAsB;;AAEjD;AACA;AACA;;;;;;;;;;;;;;ACJA,OAAO,mCAAmC,GAAG,mBAAO,CAAC,uFAAwB;AAC7E,gBAAgB,mBAAO,CAAC,2EAAwB;;AAEhD;AACA;AACA,WAAW,QAAQ;AACnB,aAAa;AACb;AACA,mBAAmB,UAAU;AAC7B;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,MAAM;AACnB,gCAAgC,oDAAoD;AACpF,aAAa,MAAM;AACnB,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;AACA;AACA,4BAA4B;AAC5B,0BAA0B;AAC1B;AACA;AACA;AACA;AACA;AACA,4BAA4B;AAC5B;AACA;AACA;AACA,gBAAgB,kBAAkB;AAClC;AACA,wCAAwC,WAAW;AACnD;;AAEA;AACA;AACA,0CAA0C,2CAA2C;AACrF,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL,GAAG;;AAEH,YAAY,SAAS;AACrB;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AClFD;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH,UAAU;AACV;;;;;;;;;;;;;;ACbA,aAAa,mBAAO,CAAC,+DAAe;AACpC,8BAA8B,mBAAO,CAAC,6FAAyB;;AAE/D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,gCAAgC;;AAE3C;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,yDAAyD,kBAAkB;AAC3E;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/GA,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,cAAc,mBAAO,CAAC,iEAAgB;AACtC,8BAA8B,mBAAO,CAAC,iGAAgC;AACtE,mBAAmB,mBAAO,CAAC,2EAAqB;AAChD,kBAAkB,mBAAO,CAAC,yEAAoB;AAC9C,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,wBAAwB,mBAAO,CAAC,qFAA0B;;AAE1D,sBAAsB,mBAAO,CAAC,mFAAiB;AAC/C,cAAc,mBAAO,CAAC,6DAAS;AAC/B,oBAAoB,mBAAO,CAAC,yEAAe;AAC3C,0BAA0B,mBAAO,CAAC,qFAAqB;AACvD;AACA,WAAW,+DAA+D;AAC1E,CAAC,GAAG,mBAAO,CAAC,6FAAyB;AACrC,OAAO,mBAAmB,GAAG,mBAAO,CAAC,mFAAoB;AACzD;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,uDAAW;;AAEvB,OAAO,OAAO;;AAEd;AACA;AACA;AACA;AACA,0BAA0B,eAAe;AACzC;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,eAAe,8BAA8B;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+CAA+C;AAC/C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe,sBAAsB,sBAAsB;AAC3D;AACA;AACA;AACA;;AAEA;;AAEA,4DAA4D,WAAW;AACvE,aAAa,kCAAkC;AAC/C;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,4CAA4C;;AAEvD,gEAAgE,UAAU;;AAE1E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,oBAAoB;AAC/B;AACA,yCAAyC,oBAAoB;AAC7D;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,mDAAmD,0CAA0C;AAC7F,6CAA6C,OAAO;;AAEpD;AACA;AACA,6CAA6C,cAAc;AAC3D;AACA;;AAEA;AACA,0CAA0C,oCAAoC;;AAE9E;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA,WAAW,mBAAmB;AAC9B;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA,aAAa,wCAAwC;AACrD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gDAAgD,QAAQ;AACxD;AACA;AACA;AACA;AACA;AACA;AACA,6BAA6B,oBAAoB;AACjD;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,oBAAoB,oBAAoB,OAAO,iCAAiC;AAChF;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,qCAAqC;AAClD;AACA,eAAe,mBAAmB;AAClC,oCAAoC,mBAAmB;AACvD;;AAEA;AACA,aAAa,2CAA2C;AACxD;AACA,iBAAiB,2BAA2B;AAC5C,sCAAsC,2BAA2B;AACjE;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,2CAA2C;AACxD;AACA,QAAQ,2BAA2B;AACnC;AACA;;AAEA;AACA,8CAA8C,uBAAuB;AACrE;AACA,KAAK;AACL;AACA;;AAEA;AACA,+CAA+C,uBAAuB;AACtE;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,mBAAmB,WAAW;AAC9B,0CAA0C,WAAW;AACrD;;AAEA;AACA;AACA,aAAa,gEAAgE;AAC7E,kBAAkB,mBAAmB,4BAA4B,mBAAmB,qBAAqB,mBAAmB,GAAG,IAAI;AACnI;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA;;AAEA,mEAAmE,aAAa;AAChF;AACA,kBAAkB,aAAa;AAC/B,eAAe,QAAQ;;AAEvB;AACA,sEAAsE,iBAAiB;AACvF;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;;AAEA;;AAEA;AACA,yBAAyB,wBAAwB,kBAAkB,oBAAoB,UAAU,cAAc;AAC/G;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;;AAEb;AACA,wCAAwC,0CAA0C;AAClF;AACA;;AAEA;AACA,sDAAsD,SAAS;AAC/D,eAAe,YAAY;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET,oDAAoD,wBAAwB;AAC5E,kEAAkE,QAAQ;AAC1E;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB,kCAAkC;AACvD;AACA,uBAAuB,sCAAsC;AAC7D;AACA;AACA,oEAAoE,qBAAqB;AACzF;AACA;AACA;AACA;AACA,mBAAmB;AACnB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,YAAY;AAC9B;;AAEA;AACA;AACA,aAAa;AACb,SAAS;;AAET;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;;AAEA;AACA,0BAA0B,UAAU;AACpC;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;AACA;;AAEA;AACA,iCAAiC,6BAA6B;AAC9D;;AAEA;AACA,2BAA2B,UAAU;AACrC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA,iBAAiB,mBAAmB;AACpC;AACA;;AAEA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,gBAAgB,4BAA4B;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,8DAA8D,+BAA+B;AAC7F;;AAEA;AACA;AACA;AACA,eAAe,yCAAyC;AACxD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb,WAAW;AACX;AACA;AACA;AACA;AACA;AACA,cAAc;AACd,KAAK,IAAI;AACT;AACA;;;;;;;;;;;;;;AC5tBA,aAAa,mBAAO,CAAC,+DAAe;AACpC;;AAEA,wBAAwB,MAAM;AAC9B;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,WAAW,UAAU;AACrB,WAAW,cAAc;AACzB,aAAa,UAAU;AACvB;AACA,aAAa,OAAO;AACpB,WAAW,OAAO;AAClB,WAAW,WAAW;AACtB,WAAW,YAAY;AACvB;AACA,aAAa,OAAO;AACpB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA,aAAa,OAAO;AACpB,WAAW,OAAO;AAClB,WAAW,QAAQ;AACnB;AACA,mBAAmB,gCAAgC;AACnD;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,aAAa;AAC1B;AACA;;AAEA,WAAW,4BAA4B;;AAEvC;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA,GAAG;AACH;;;;;;;;;;;;;;AC/DA,aAAa,mBAAO,CAAC,+DAAe;AACpC,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,OAAO,mBAAmB,GAAG,mBAAO,CAAC,uEAAmB;AACxD,sBAAsB,mBAAO,CAAC,6EAAiB;AAC/C,eAAe,mBAAO,CAAC,+DAAU;AACjC,OAAO,+CAA+C,GAAG,mBAAO,CAAC,6FAAyB;AAC1F,oCAAoC,mBAAO,CAAC,yFAA4B;AACxE,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;AACxD,OAAO,aAAa,GAAG,mBAAO,CAAC,2EAAa;AAC5C,OAAO,iCAAiC,GAAG,mBAAO,CAAC,6DAAc;AACjE,wBAAwB,mBAAO,CAAC,yFAA4B;;AAE5D,OAAO,eAAe;AACtB,OAAO,mCAAmC;;AAE1C;AACA;AACA,iCAAiC,IAAI;AACrC;;AAEA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB,WAAW,8BAA8B;AACzC,WAAW,OAAO;AAClB,WAAW,mCAAmC;AAC9C,WAAW,6BAA6B;AACxC,WAAW,0CAA0C;AACrD,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,4BAA4B;AACvC,WAAW,OAAO;AAClB;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;;AAEA;AACA;AACA;AACA,oBAAoB,2BAA2B;AAC/C;;AAEA;AACA;AACA;;AAEA;AACA;AACA,qCAAqC,kBAAkB,uCAAuC,eAAe;AAC7G;AACA;;AAEA,gCAAgC,sDAAsD;AACtF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA,aAAa,0CAA0C;AACvD;AACA;AACA;AACA;;AAEA,aAAa,6CAA6C;AAC1D;AACA;AACA;AACA,2DAA2D,UAAU;AACrE;AACA;AACA,KAAK;AACL,oEAAoE,UAAU;AAC9E;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA,aAAa,uCAAuC;AACpD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,8BAA8B,UAAU;AACxC,KAAK;AACL,+DAA+D,UAAU;AACzE;AACA;AACA,OAAO;;AAEP;AACA;AACA;;AAEA,aAAa,4CAA4C;AACzD,4BAA4B,+BAA+B;AAC3D;AACA;AACA;;AAEA;AACA,0DAA0D,MAAM;AAChE;;AAEA;AACA;AACA;AACA,yBAAyB,MAAM,IAAI,aAAa;AAChD;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,eAAe,mBAAmB;AAClC;;AAEA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;AACA;;AAEA;AACA,mBAAmB;AACnB;;AAEA;AACA;;AAEA,aAAa,sCAAsC;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG,KAAK;AACR;AACA,mFAAmF,UAAU;AAC7F;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,+BAA+B,UAAU;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA,6BAA6B,OAAO,IAAI,UAAU;AAClD;AACA;AACA;AACA,OAAO;;AAEP;AACA,8BAA8B,6BAA6B;AAC3D;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW;;AAEX;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA,mDAAmD,UAAU;AAC7D;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;;AAEA;AACA;;AAEA,aAAa,qCAAqC;AAClD;AACA;AACA,wEAAwE,UAAU;AAClF;;AAEA;AACA;AACA;AACA,oDAAoD,UAAU;AAC9D;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;AACA,YAAY;AACZ;AACA,kBAAkB,yEAAyE;AAC3F;AACA;AACA;AACA,iBAAiB,4CAA4C;AAC7D;AACA,8DAA8D,MAAM;AACpE;;AAEA;AACA;AACA,6DAA6D,UAAU;AACvE;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT,yFAAyF,OAAO;AAChG;;AAEA;AACA;AACA;;AAEA;AACA;AACA,0EAA0E,SAAS;AACnF;AACA;;AAEA;;AAEA,2BAA2B,4CAA4C;;AAEvE,gBAAgB;AAChB,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA,aAAa,uCAAuC;AACpD,iBAAiB,2BAA2B;AAC5C;AACA,0DAA0D,MAAM;AAChE;;AAEA;AACA;AACA,yDAAyD,UAAU;AACnE;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL,qFAAqF,OAAO;AAC5F;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kDAAkD;AAC1E;;AAEA,aAAa,gDAAgD;AAC7D;AACA,4DAA4D,UAAU;AACtE;AACA;AACA,aAAa,SAAS,qCAAqC,sBAAsB;AACjF;AACA,KAAK;AACL;;AAEA;AACA,YAAY;AACZ;AACA,kBAAkB,0CAA0C;AAC5D;AACA;AACA;AACA;AACA;AACA,2BAA2B,2DAA2D;AACtF;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,wFAAwF,0BAA0B;AAClH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,YAAY;AACZ;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,YAAY;AACZ;AACA,iBAAiB,0CAA0C;AAC3D;AACA;AACA;AACA;AACA;AACA,2BAA2B,2DAA2D;AACtF;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,yFAAyF,0BAA0B;AACnH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,cAAc,OAAO;AACrB;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjhBA,mBAAmB,mBAAO,CAAC,2EAAqB;AAChD,iCAAiC,mBAAO,CAAC,6FAA8B;AACvE,sBAAsB,mBAAO,CAAC,qGAAkC;AAChE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtCA,aAAa,mBAAO,CAAC,kEAAkB;AACvC,gBAAgB,mBAAO,CAAC,wEAAqB;AAC7C,wBAAwB,mBAAO,CAAC,+FAAmB;AACnD,kCAAkC,mBAAO,CAAC,mHAA6B;AACvE;AACA,WAAW,iBAAiB;AAC5B,CAAC,GAAG,mBAAO,CAAC,8FAA0B;;AAEtC,OAAO,eAAe;AACtB,yEAAyE,YAAY,EAAE,KAAK;;AAE5F;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,iCAAiC;AAC9C,aAAa,gCAAgC;AAC7C,aAAa,2CAA2C;AACxD,aAAa,QAAQ;AACrB,aAAa,cAAc;AAC3B,aAAa,cAAc;AAC3B,cAAc,kBAAkB,2BAA2B;AAC3D,aAAa,wCAAwC;AACrD,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,wCAAwC;AACrD;AACA,eAAe,mBAAmB;AAClC;AACA;;AAEA;AACA,aAAa,8CAA8C;AAC3D;AACA,iBAAiB,2BAA2B;AAC5C;AACA;AACA;AACA;;AAEA;AACA,eAAe;AACf;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,aAAa,wCAAwC;AACrD;AACA,0BAA0B,mBAAmB;AAC7C,WAAW,kCAAkC;AAC7C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,mCAAmC;AAC3D,SAAS;AACT;AACA,KAAK;;AAEL,uBAAuB,mBAAmB;AAC1C;;AAEA;AACA;AACA;AACA;AACA,aAAa,8CAA8C;AAC3D;AACA,cAAc,2BAA2B;AACzC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA,WAAW,kCAAkC;AAC7C;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,oBAAoB;AAC5C,SAAS;AACT;AACA,KAAK;;AAEL,uBAAuB,mBAAmB;AAC1C;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA,eAAe,OAAO;AACtB,gBAAgB,eAAe;AAC/B;AACA,eAAe,OAAO;AACtB,gBAAgB,kBAAkB;AAClC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA;AACA,8BAA8B,aAAa;AAC3C;AACA;AACA;AACA,KAAK;AACL,sCAAsC,oBAAoB;AAC1D;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA,YAAY;AACZ;;AAEA,kCAAkC;AAClC,WAAW,kCAAkC;AAC7C,WAAW,4CAA4C;;AAEvD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,uBAAuB,oBAAoB;AAC3C;AACA,iBAAiB,oBAAoB,kBAAkB,sBAAsB;AAC7E;AACA;;AAEA;AACA,YAAY;AACZ;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,WAAW,UAAU;AACrB;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,8BAA8B,YAAY;AAC1C,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA,WAAW,6BAA6B;AACxC;AACA;AACA,KAAK;;AAEL,uDAAuD,oBAAoB;AAC3E;AACA;AACA;AACA;AACA,sBAAsB,SAAS;AAC/B,mBAAmB,YAAY,aAAa,YAAY;AACxD,SAAS;AACT;AACA;AACA;;AAEA,mCAAmC,oBAAoB;AACvD,0BAA0B,sBAAsB;AAChD;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,oBAAoB;AAC1C;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA,aAAa,wCAAwC;AACrD;AACA,gBAAgB,mBAAmB;AACnC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,yCAAyC,wBAAwB;AACjE;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;AC1YA,wBAAwB,mBAAO,CAAC,+FAAmB;AACnD,OAAO,eAAe;;AAEtB,+BAA+B,oBAAoB,kBAAkB,sBAAsB;AAC3F,2BAA2B,oBAAoB;AAC/C,eAAe,+CAA+C,GAAG;;AAEjE;AACA,uEAAuE;AACvE,iEAAiE;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gBAAgB;AAChB,OAAO;AACP;AACA,GAAG;AACH;;;;;;;;;;;;;;ACzBA,aAAa,mBAAO,CAAC,kEAAkB;;AAEvC;;;;;;;;;;;;;;ACFA,OAAO,eAAe,GAAG,mBAAO,CAAC,sBAAQ;AACzC,aAAa,mBAAO,CAAC,+DAAe;AACpC,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,yBAAyB,mBAAO,CAAC,6EAAsB;AACvD,OAAO,eAAe,GAAG,mBAAO,CAAC,uDAAW;AAC5C,gBAAgB,mBAAO,CAAC,iEAAW;;AAEnC;AACA,WAAW,0EAA0E;AACrF,CAAC,GAAG,mBAAO,CAAC,6FAAyB;;AAErC;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,6BAA6B;AAC1C,aAAa,0BAA0B;AACvC,aAAa,qCAAqC;AAClD,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,aAAa,mEAAmE;AAChF,aAAa,qEAAqE;AAClF,aAAa,OAAO;AACpB,aAAa,wBAAwB;AACrC,aAAa,mCAAmC;AAChD,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+CAA+C;AAC/C;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;;AAEL;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;;AAEA;AACA,WAAW,mBAAmB;;AAE9B;AACA,6DAA6D,mBAAmB;AAChF;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,gDAAgD,mCAAmC;AACnF,WAAW;AACX,SAAS;AACT,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;;AAEA;AACA;AACA;AACA;;AAEA,wCAAwC,2CAA2C;AACnF,0CAA0C,mCAAmC;AAC7E;AACA;AACA;;AAEA;AACA,WAAW,mBAAmB;AAC9B;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4CAA4C,4CAA4C;AACxF,SAAS;AACT;AACA,8CAA8C,mCAAmC;AACjF,SAAS;AACT;AACA;AACA;AACA;AACA,mBAAmB,wBAAwB;AAC3C;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA,yDAAyD,mBAAmB;AAC5E,OAAO;AACP,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,wCAAwC,+CAA+C;AACvF;AACA;;AAEA;AACA;;AAEA,oDAAoD;;AAEpD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,OAAO;;AAEP,0CAA0C,mCAAmC;AAC7E;;AAEA,WAAW,gCAAgC;AAC3C,2CAA2C,6CAA6C;;AAExF;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;;AAET;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa;AACb;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,wCAAwC,mCAAmC;AAC3E;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA,OAAO;AACP;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA,WAAW;;AAEX;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA,KAAK;AACL;AACA;;;;;;;;;;;;;;ACrkBA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,YAAY;AACZ;AACA;;;;;;;;;;;;;;ACnBA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,aAAa,uBAAuB,4BAA4B,0CAA0C;AAC1G;AACA;AACA;AACA,kBAAkB,yBAAyB;AAC3C,gBAAgB,wBAAwB,oBAAoB;AAC5D,OAAO;AACP;AACA;AACA;;AAEA;AACA,aAAa,uBAAuB,4BAA4B,0CAA0C;AAC1G;AACA;AACA,8BAA8B,oBAAoB;AAClD;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,uBAAuB,4BAA4B,0CAA0C;AAC1G;AACA;AACA,8BAA8B,oBAAoB;AAClD;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA,KAAK;AACL;;AAEA;AACA,eAAe,6CAA6C;AAC5D,gBAAgB,0CAA0C;AAC1D;AACA;AACA,+DAA+D,oBAAoB;AACnF;AACA;AACA,KAAK;AACL;;AAEA;AACA,eAAe,6CAA6C;AAC5D,gBAAgB,0CAA0C;AAC1D;AACA;AACA,+DAA+D,oBAAoB;AACnF;AACA;AACA,KAAK;AACL;;AAEA;AACA,eAAe,6CAA6C;AAC5D,gBAAgB,0CAA0C;AAC1D;AACA;AACA;AACA,aAAa,oBAAoB;AACjC;AACA;AACA,OAAO;AACP,gBAAgB,aAAa;AAC7B;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;AC1HA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACHD,gBAAgB,mBAAO,CAAC,4DAAiB;AACzC,OAAO,OAAO;;AAEd;AACA,kBAAkB,mBAAmB,KAAK;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,cAAc,mBAAmB;AACjC;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,0BAA0B,KAAK;AACjD,cAAc,YAAY;AAC1B;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,mBAAmB;AACrC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,WAAW;AAC7B;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,wBAAwB;AAC1C;AACA,oBAAoB,UAAU,iBAAiB,QAAQ;AACvD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,eAAe,KAAK;AACtC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,aAAa,KAAK;AACpC,cAAc,YAAY,KAAK,GAAG,KAAK,GAAG;AAC1C;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,4DAA4D,KAAK;AACnF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,QAAQ,KAAK;AAC/B;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,2BAA2B,KAAK;AAClD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,kBAAkB,KAAK;AACzC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,eAAe,aAAa;AAC5B;AACA;AACA;AACA;AACA,gBAAgB,QAAQ;AACxB,eAAe,QAAQ;;AAEvB,2CAA2C,YAAY;AACvD;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,iCAAiC,QAAQ;AACzC;;AAEA;AACA,oEAAoE,QAAQ;AAC5E,wBAAwB,SAAS,0DAA0D,WAAW;AACtG;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AChRA;AACA;AACA,WAAW,OAAO;AAClB,CAAC,GAAG,mBAAO,CAAC,8DAAW;;AAEvB,oCAAoC,mBAAO,CAAC,wFAA2B;AACvE,sBAAsB,mBAAO,CAAC,wEAAmB;AACjD,gBAAgB,mBAAO,CAAC,8DAAW;AACnC,uBAAuB,mBAAO,CAAC,gEAAY;AAC3C,uBAAuB,mBAAO,CAAC,gEAAY;AAC3C,oBAAoB,mBAAO,CAAC,0DAAS;AACrC,wBAAwB,mBAAO,CAAC,wFAA2B;AAC3D,6BAA6B,mBAAO,CAAC,oFAAyB;;AAE9D;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,cAAc;AAC3B,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,aAAa,gCAAgC;AAC7C,aAAa,kCAAkC;AAC/C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,yCAAyC,8BAA8B;AACvE;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG,KAAK;AACR;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,cAAc,2CAA2C;AACzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,aAAa;AAC1B;AACA;AACA;AACA;AACA,GAAG,KAAK;AACR;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,cAAc,2CAA2C;AACzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,SAAS,QAAQ,KAAK;AACtB;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,cAAc,2CAA2C;AACzD;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnMA,OAAO,eAAe,GAAG,mBAAO,CAAC,sBAAQ;AACzC,6BAA6B,mBAAO,CAAC,oEAAS;AAC9C,OAAO,eAAe,GAAG,mBAAO,CAAC,uDAAW;;AAE5C;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA,oDAAoD,mBAAmB;AACvE;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,yBAAyB;AACtC,eAAe,iEAAiE;AAChF;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjCA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;ACtBA,yCAAyC,UAAU,GAAG,KAAK;;;;;;;;;;;;;;ACA3D,OAAO,mBAAmB,GAAG,mBAAO,CAAC,4DAAS;;AAE9C,yBAAyB,+BAA+B;AACxD,iCAAiC,UAAU;AAC3C;AACA,mBAAmB,eAAe;AAClC,kBAAkB,OAAO,EAAE,YAAY;AACvC,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpBA,OAAO,SAAS;;AAEhB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA,uBAAuB,kCAAkC,KAAK;AAC9D;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnEA,qBAAqB,mBAAO,CAAC,8DAAU;AACvC,sBAAsB,mBAAO,CAAC,2EAAqB;AACnD,gBAAgB,mBAAO,CAAC,2EAAqB;AAC7C,OAAO,uDAAuD,GAAG,mBAAO,CAAC,uDAAW;AACpF,OAAO,mBAAmB,GAAG,mBAAO,CAAC,6DAAc;AACnD,eAAe,mBAAO,CAAC,iDAAQ;AAC/B,qBAAqB,mBAAO,CAAC,gFAAgB;AAC7C,OAAO,sCAAsC,GAAG,mBAAO,CAAC,kFAAoB;;AAE5E,sBAAsB,8BAA8B;AACpD,KAAK,QAAQ,QAAQ,OAAO,aAAa,WAAW;;AAEpD;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,6BAA6B;AAC1C,aAAa,qCAAqC;AAClD,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB;AACA,aAAa,qCAAqC;AAClD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,qBAAqB,UAAU,GAAG,UAAU;AAC5C;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA,6CAA6C;AAC7C;AACA,sBAAsB,0CAA0C;AAChE;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,sEAAsE,UAAU;AAChF,qBAAqB,UAAU,GAAG,UAAU;AAC5C;AACA,SAAS;;AAET,sCAAsC,iBAAiB;AACvD;AACA;;AAEA;AACA;;AAEA;AACA;AACA,qBAAqB,UAAU,GAAG,UAAU;AAC5C,SAAS;;AAET;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,OAAO;AACP;AACA;AACA,2DAA2D,UAAU;AACrE,uBAAuB,UAAU,GAAG,UAAU;AAC9C,WAAW;AACX;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA,gBAAgB,gDAAgD;AAChE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA,yBAAyB,UAAU,GAAG,UAAU;AAChD,aAAa;AACb;AACA,SAAS;AACT;;AAEA;AACA;;AAEA;AACA;AACA,OAAO;AACP;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,eAAe,cAAc;AAC7B;AACA,cAAc,oEAAoE;AAClF;;AAEA;AACA;AACA,aAAa,WAAW;AACxB;;AAEA,kDAAkD,mCAAmC;AACrF,aAAa,8BAA8B;AAC3C,+BAA+B,qBAAqB;AACpD;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA,yBAAyB;;AAEzB;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb,WAAW;AACX,SAAS;AACT;AACA;AACA,OAAO;AACP;;AAEA,WAAW,sCAAsC;;AAEjD;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,gCAAgC,mBAAmB;AACnD;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;AACA,kCAAkC,mBAAmB;AACrD;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA,gCAAgC,mBAAmB;AACnD;AACA;AACA;AACA,gDAAgD,qCAAqC;AACrF,OAAO;;AAEP;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,UAAU,GAAG,UAAU;AAC1C,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1bA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACXA,iCAAiC,mBAAO,CAAC,6FAA8B;AACvE;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACPA,OAAO,eAAe,GAAG,mBAAO,CAAC,sBAAQ;AACzC,sBAAsB,mBAAO,CAAC,yFAAiB;AAC/C,eAAe,mBAAO,CAAC,6FAA0B;AACjD,OAAO,4BAA4B,GAAG,mBAAO,CAAC,0DAAc;;AAE5D;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,gCAAgC;AAC7C,aAAa,wCAAwC;AACrD,aAAa,cAAc;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,cAAc;AACd;AACA;;AAEA;AACA;AACA;AACA,cAAc;AACd;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,eAAe,OAAO;AACtB,gBAAgB,uCAAuC;AACvD,gBAAgB,QAAQ;AACxB,gBAAgB,SAAS;AACzB,gBAAgB,OAAO;AACvB;AACA;AACA,aAAa,cAAc;AAC3B;AACA;AACA,WAAW,gBAAgB;AAC3B;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;;AAEL;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA,aAAa,cAAc;AAC3B;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA,+BAA+B,yBAAyB;AACxD;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,kBAAkB,+BAA+B;AACjD;AACA;AACA;;AAEA;AACA,+BAA+B,gBAAgB;AAC/C,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,MAAM;AACnB;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;;;;;;;;;;;;;;ACtTA,OAAO,uDAAuD,GAAG,mBAAO,CAAC,0DAAc;AACvF,eAAe,mBAAO,CAAC,6FAA0B;;AAEjD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,aAAa;AAC3B,cAAc,QAAQ;AACtB,cAAc,SAAS;AACvB,cAAc,SAAS;AACvB;AACA,aAAa,OAAO;AACpB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,SAAS;AACvB,cAAc,SAAS;AACvB;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,aAAa;AAC1B,aAAa,QAAQ;AACrB,aAAa,SAAS;AACtB,aAAa,WAAW;AACxB,aAAa,wCAAwC;AACrD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,8BAA8B;AACzC,2BAA2B,QAAQ,QAAQ,OAAO,aAAa,WAAW;AAC1E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,4DAA4D,YAAY;AACxE;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA,aAAa,gBAAgB;AAC7B;AACA;AACA;AACA,KAAK;;AAEL,WAAW,6EAA6E;;AAExF;AACA;AACA,mBAAmB,sCAAsC;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,uBAAuB,iBAAiB;AACxC;AACA;AACA;;AAEA;;AAEA;AACA,8CAA8C,QAAQ,MAAM,gBAAgB;AAC5E;AACA;AACA;;;;;;;;;;;;;;ACvKA;AACA,WAAW,OAAO;AAClB,WAAW,qCAAqC;AAChD,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,WAAW;AACtB,WAAW,uBAAuB;AAClC,WAAW,WAAW;AACtB,WAAW,qBAAqB;AAChC,WAAW,WAAW;AACtB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD,gCAAgC,6BAA6B;;AAE7D;AACA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;AC/BA;;AAEA;AACA,aAAa;AACb;AACA;AACA,cAAc,mBAAO,CAAC,gBAAK;AAC3B,cAAc,mBAAO,CAAC,gBAAK;;AAE3B,WAAW,6BAA6B;AACxC;AACA,mCAAmC,+BAA+B;AAClE,qBAAqB,aAAa;;AAElC;;AAEA;AACA;AACA;;;;;;;;;;;;;;AClBA;AACA;AACA,MAAM,gEAAgE;AACtE;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;AACA;;;;;;;;;;;;;;ACXA,oBAAoB,mBAAO,CAAC,8DAAa;AACzC,OAAO,2BAA2B,GAAG,mBAAO,CAAC,0DAAc;AAC3D,0BAA0B,mBAAO,CAAC,gGAAiC;AACnE,2BAA2B,mBAAO,CAAC,4GAA2B;AAC9D,eAAe,mBAAO,CAAC,sBAAQ;;AAE/B,eAAe,mBAAO,CAAC,gGAAqB;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yEAAyE;AACzE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB;;AAEA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,2CAA2C,SAAS;AACpD,kCAAkC,KAAK;AACvC;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,mBAAmB;AACnB;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA,mBAAmB;AACnB;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,aAAa;;AAEb;AACA;AACA;AACA;;AAEA,6DAA6D,4BAA4B;AACzF,WAAW;AACX;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;;AAEA;AACA;;AAEA;AACA;AACA,SAAS;AACT,OAAO;;AAEP;AACA;AACA;AACA,iBAAiB,OAAO;AACxB,iBAAiB,OAAO;AACxB,mBAAmB;AACnB;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA,iBAAiB,OAAO;AACxB,iBAAiB,OAAO;AACxB,iBAAiB,OAAO;AACxB;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,4BAA4B,MAAM,GAAG,UAAU,sBAAsB,SAAS;AAC9E;AACA;AACA;;AAEA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA,iBAAiB,YAAY;AAC7B;AACA,mBAAmB,OAAO;AAC1B,oBAAoB,OAAO;AAC3B,oBAAoB,SAAS;AAC7B,oBAAoB,OAAO;AAC3B;AACA;AACA;AACA;;AAEA,4BAA4B,oBAAoB;AAChD;;AAEA,+BAA+B,YAAY;AAC3C;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;;AAET;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA,2CAA2C,qDAAqD;AAChG;;AAEA,yBAAyB,oBAAoB;AAC7C;AACA;AACA,WAAW;AACX,SAAS;AACT,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA,OAAO;;AAEP;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA,iBAAiB,OAAO;AACxB,iBAAiB,oBAAoB;AACrC,mBAAmB;AACnB;AACA,mBAAmB,OAAO;AAC1B,oBAAoB,OAAO;AAC3B,oBAAoB,6BAA6B;AACjD;AACA,mBAAmB,OAAO;AAC1B,oBAAoB,OAAO;AAC3B,oBAAoB,OAAO;AAC3B;AACA,yBAAyB,0BAA0B;AACnD;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb,WAAW;AACX;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;;AAEf;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,eAAe;;AAEf;AACA;;AAEA;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;;AAEL;AACA;AACA;AACA;AACA,uBAAuB,oDAAoD;AAC3E,yBAAyB,4CAA4C;AACrE,mCAAmC,oCAAoC;AACvE,oBAAoB,oCAAoC;AACxD,eAAe,oCAAoC;AACnD,cAAc,oCAAoC;AAClD;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACtZA,OAAO,eAAe,GAAG,mBAAO,CAAC,sBAAQ;AACzC,OAAO,2BAA2B,GAAG,mBAAO,CAAC,0DAAc;AAC3D,eAAe,mBAAO,CAAC,gGAAqB;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8CAA8C;AACjE;;AAEA,kCAAkC,qCAAqC;AACvE;AACA,gFAAgF,OAAO;AACvF;;AAEA;AACA;;AAEA;AACA;AACA,uDAAuD,OAAO,cAAc,aAAa;AACzF;;AAEA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,eAAe,OAAO;AACtB,eAAe,OAAO;AACtB,eAAe,SAAS;AACxB,eAAe,aAAa;AAC5B;AACA;AACA;AACA;AACA;AACA,OAAO,IAAI;;AAEX,cAAc;AACd,KAAK;AACL;AACA;AACA;AACA;AACA,mDAAmD,aAAa,OAAO,MAAM;;AAE7E;AACA;AACA,6DAA6D,aAAa,OAAO,MAAM;AACvF;AACA;;AAEA,uCAAuC,gCAAgC;AACvE;AACA,KAAK;;AAEL;AACA;AACA,KAAK;AACL,GAAG;;AAEH;AACA;;;;;;;;;;;;;;AC9EA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,mBAAmB,kDAAkD;AACrE;AACA;AACA;;AAEA;AACA,mCAAmC,oCAAoC;AACvE;AACA,kCAAkC,qCAAqC;AACvE,GAAG,IAAI;AACP;;;;;;;;;;;;;;ACVA,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,OAAO,oBAAoB,GAAG,mBAAO,CAAC,2FAA6B;AACnE,OAAO,qBAAqB,GAAG,mBAAO,CAAC,kFAAiB;AACxD,oCAAoC,mBAAO,CAAC,yFAA4B;AACxE,yBAAyB,mBAAO,CAAC,6EAAc;AAC/C,8BAA8B,mBAAO,CAAC,iFAAmB;AACzD,OAAO,+CAA+C,GAAG,mBAAO,CAAC,6FAAyB;AAC1F,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;;AAExD,OAAO,eAAe;AACtB;AACA;AACA,iCAAiC,IAAI;AACrC;;AAEA,OAAO,sBAAsB;;AAE7B;AACA;AACA,WAAW,OAAO;AAClB,WAAW,8BAA8B;AACzC,WAAW,6BAA6B;AACxC,WAAW,yCAAyC;AACpD,WAAW,mCAAmC;AAC9C,WAAW,QAAQ;AACnB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,4BAA4B;AACvC;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA,oBAAoB;;AAEpB;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA,8CAA8C;AAC9C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH,SAAS,kBAAkB;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA,aAAa,qCAAqC;AAClD;AACA;AACA,wEAAwE,UAAU;AAClF;;AAEA;AACA;AACA;AACA,oDAAoD,UAAU;AAC9D;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA,eAAe,OAAO;AACtB,gBAAgB,SAAS;AACzB,gBAAgB,SAAS;AACzB,gBAAgB,SAAS;AACzB,gBAAgB,SAAS;AACzB,gBAAgB,SAAS;AACzB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,WAAW,yCAAyC;AACpD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,yBAAyB;AAC3C;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,kBAAkB,yBAAyB;AAC3C;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,kBAAkB,yBAAyB;AAC3C;AACA,4CAA4C,0BAA0B;AACtE,mDAAmD,0BAA0B;;AAE7E;AACA,iBAAiB,oBAAoB;AACrC,sBAAsB,oBAAoB;AAC1C;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA,eAAe,OAAO;AACtB;AACA;;AAEA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA,gBAAgB;AAChB;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrPA,mBAAmB,mBAAO,CAAC,2EAAqB;AAChD,sBAAsB,mBAAO,CAAC,qGAAkC;AAChE,iCAAiC,mBAAO,CAAC,6FAA8B;AACvE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA,2BAA2B,mBAAO,CAAC,2EAAgB;AACnD,OAAO,yCAAyC,GAAG,mBAAO,CAAC,uDAAW;AACtE,OAAO,oBAAoB,GAAG,mBAAO,CAAC,2FAA6B;;AAEnE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;;AAEA;AACA;AACA;AACA,yCAAyC;AACzC;AACA;AACA;AACA;AACA;;AAEA;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,MAAM;AACtB,+BAA+B,kCAAkC;AACjE;AACA,eAAe,OAAO;AACtB,gBAAgB,qBAAqB;AACrC,gBAAgB,OAAO;AACvB;AACA;AACA;AACA;AACA,gBAAgB,OAAO;AACvB,gBAAgB,kBAAkB;AAClC;AACA,aAAa;AACb,eAAe;AACf;AACA,4BAA4B,sDAAsD;AAClF,6BAA6B,QAAQ;AACrC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,gBAAgB,kBAAkB;AAClC;AACA;AACA,qCAAqC,SAAS,eAAe,MAAM;AACnE;AACA;;AAEA;AACA;AACA;AACA,sDAAsD,MAAM,KAAK;AACjE;AACA,YAAY;AACZ;AACA;AACA;;AAEA;AACA,+DAA+D,kBAAkB;AACjF,uCAAuC,qBAAqB;;AAE5D;AACA,qBAAqB,kBAAkB;AACvC,OAAO;AACP;AACA;;AAEA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,eAAe;AAC5B,eAAe;AACf;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,MAAM;AACtB,+BAA+B,kCAAkC;AACjE,gBAAgB,OAAO;AACvB;AACA;AACA;AACA,gBAAgB,OAAO;AACvB,gBAAgB,kBAAkB;AAClC;AACA,uBAAuB,8CAA8C;AACrE,0BAA0B;AAC1B;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnIA,gBAAgB,mBAAO,CAAC,sFAAW;AACnC,iCAAiC,mBAAO,CAAC,8FAAe;;AAExD;;;;;;;;;;;;;;ACHA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,iBAAiB,aAAa;AAC9B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;AClDA,oBAAoB,mBAAO,CAAC,8FAAe;;AAE3C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,WAAW,oCAAoC;AAC/C;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC9CA,OAAO,2BAA2B,GAAG,mBAAO,CAAC,6DAAiB;;AAE9D;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD,yBAAyB,mBAAO,CAAC,sBAAQ;AACzC;;AAEA;;AAEA;AACA;AACA;AACA,sBAAsB,KAAK,0DAA0D,UAAU;AAC/F;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;AC9BA,gBAAgB,mBAAO,CAAC,0FAAW;AACnC,iCAAiC,mBAAO,CAAC,uGAAwB;;AAEjE;;;;;;;;;;;;;;ACHA;AACA,aAAa,mBAAO,CAAC,qEAAqB;;AAE1C;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,iBAAiB,aAAa;AAC9B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACpDA,2BAA2B,mBAAO,CAAC,oFAAW;AAC9C,kCAAkC,mBAAO,CAAC,4FAAe;;AAEzD;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,gBAAgB,mBAAO,CAAC,qEAAkB;;AAE1C,mBAAmB,SAAS;AAC5B,kCAAkC,wBAAwB;AAC1D,kCAAkC,0BAA0B;AAC5D;;AAEA;AACA;;;;;;;;;;;;;;ACRA,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;AACxD,OAAO,gBAAgB,GAAG,mBAAO,CAAC,uEAAmB;AACrD,kCAAkC,mBAAO,CAAC,qGAA6B;AACvE,wBAAwB,mBAAO,CAAC,iFAAmB;AACnD,2BAA2B,mBAAO,CAAC,uFAAsB;;AAEzD,OAAO,OAAO;;AAEd;AACA,WAAW,OAAO;AAClB,WAAW,6BAA6B;AACxC,WAAW,8BAA8B;AACzC,WAAW,qDAAqD;AAChE,WAAW,kCAAkC;AAC7C,WAAW,2BAA2B;AACtC;AACA,mBAAmB,oDAAoD;AACvE,iBAAiB,4CAA4C;AAC7D,eAAe,yCAAyC;AACxD;;AAEA,gBAAgB,yCAAyC;AACzD;AACA;;AAEA;;AAEA,kBAAkB,kBAAkB;AACpC;;AAEA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA,SAAS,IAAI;;AAEb;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA,mDAAmD,SAAS;AAC5D;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,wBAAwB,sBAAsB;AAC9C,yBAAyB,kEAAkE;AAC3F;AACA;AACA;AACA;AACA,WAAW;;AAEX;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;;AAEA,sCAAsC,uBAAuB;AAC7D;;AAEA;AACA,WAAW;;AAEX;AACA,SAAS;AACT;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA,yCAAyC,QAAQ;AACjD;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,gCAAgC,6BAA6B;AAC7D;;AAEA;AACA,kEAAkE,UAAU;AAC5E;AACA;AACA,WAAW;AACX;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mDAAmD,UAAU,IAAI,wBAAwB;AACzF;AACA;AACA;;AAEA,wBAAwB,UAAU,IAAI,wBAAwB;AAC9D;AACA;AACA;AACA,KAAK;AACL;AACA;;;;;;;;;;;;;;ACpKA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AChEA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA,UAAU;AACV;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACRA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACXA;;AAEA;AACA,aAAa,OAAO;AACpB;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACXA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,SAAS,SAAS;AAClB;AACA;AACA,iBAAiB,OAAO;AACxB;AACA;AACA;AACA;;;;;;;;;;;;;;AC7QA,OAAO,qDAAqD,GAAG,mBAAO,CAAC,uDAAW;AAClF,aAAa,mBAAO,CAAC,+DAAe;;AAEpC;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mBAAmB,YAAY;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mBAAmB,YAAY;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mBAAmB,gBAAgB;AACnC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mBAAmB,YAAY;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/RA,aAAa,mBAAO,CAAC,+DAAe;;AAEpC;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,aAAa,MAAM;AACnB,aAAa,mCAAmC;AAChD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,MAAM;AACnB,aAAa,mCAAmC;AAChD,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClZA,OAAO,uBAAuB,GAAG,mBAAO,CAAC,uDAAW;AACpD,mBAAmB,mBAAO,CAAC,2EAAqB;;AAEhD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,iCAAiC,UAAU;AAC3C,CAAC;;AAED;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxlBA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACdA,OAAO,YAAY,GAAG,mBAAO,CAAC,kBAAM;AACpC,aAAa,mBAAO,CAAC,kBAAM;;AAE3B;AACA;;AAEA;AACA;AACA,aAAa,QAAQ;AACrB,eAAe;AACf;AACA;AACA;AACA,GAAG;;AAEH;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;ACtBA,OAAO,wBAAwB,GAAG,mBAAO,CAAC,6DAAiB;;AAE3D;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,sBAAsB,mBAAO,CAAC,+EAAQ;AACtC;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrCA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,0DAAc;;AAE1B,kBAAkB,mBAAO,CAAC,+EAAc;AACxC,kBAAkB,mBAAO,CAAC,+EAAc;;AAExC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+DAA+D,UAAU;AACzE;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,2DAA2D,eAAe,kBAAkB,KAAK;AACjG;AACA;;AAEA;AACA;AACA;AACA,wBAAwB,+BAA+B;AACvD;;;;;;;;;;;;;;ACpCA;AACA,KAAK,mBAAO,CAAC,qEAAM;AACnB,KAAK,mBAAO,CAAC,qEAAM;AACnB;;AAEA,mBAAmB,cAAc;;;;;;;;;;;;;;ACLjC;AACA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACJD,gBAAgB,mBAAO,CAAC,qEAAe;AACvC,cAAc,mBAAO,CAAC,iEAAa;AACnC,OAAO,6CAA6C,GAAG,mBAAO,CAAC,wFAAgB;;AAE/E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,6CAA6C;AAChE;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;ACvBA;AACA;AACA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACLD,gBAAgB,mBAAO,CAAC,qEAAe;AACvC,cAAc,mBAAO,CAAC,iEAAa;AACnC,OAAO,6CAA6C,GAAG,mBAAO,CAAC,wFAAgB;;AAE/E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,qEAAqE;AACxF;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;ACzBA,aAAa,mBAAO,CAAC,kEAAkB;AACvC,gBAAgB,mBAAO,CAAC,kEAAY;AACpC,uBAAuB,mBAAO,CAAC,kFAAoB;AACnD,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gGAAwB;AACpE,OAAO,6BAA6B,GAAG,mBAAO,CAAC,0DAAc;;AAE7D;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;AC1FA,gBAAgB,mBAAO,CAAC,kEAAY;AACpC,wBAAwB,mBAAO,CAAC,wEAAY;AAC5C,OAAO,QAAQ,GAAG,mBAAO,CAAC,gGAAwB;;AAElD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,MAAM,mCAAmC;AACzC,MAAM,mCAAmC;AACzC;AACA;AACA,mBAAmB,2CAA2C;AAC9D;AACA,mCAAmC,0BAA0B;AAC7D;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;;AAEH;AACA;;;;;;;;;;;;;;ACxCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,mBAAmB;AACpC;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpFA,eAAe,mBAAO,CAAC,kFAAU;AACjC;;AAEA;;;;;;;;;;;;;;ACHA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACHD,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,aAAa;AAChC;AACA;;;;;;;;;;;;;;ACXA,aAAa,mBAAO,CAAC,wEAAwB;AAC7C,sBAAsB,mBAAO,CAAC,qGAAyB;AACvD,uBAAuB,mBAAO,CAAC,sFAAyB;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4CAA4C;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,mBAAmB,aAAa,OAAO,uBAAuB,KAAK;;AAEnE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3DA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,eAAe,mBAAO,CAAC,2FAAiB;;AAExC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,2BAA2B;AAC3B,8BAA8B;AAC9B,eAAe;AACf,iBAAiB;AACjB,qBAAqB,GAAG;AACxB;AACA,mBAAmB,8DAA8D,EAAE;AACnF;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACjEA,gBAAgB,mBAAO,CAAC,qEAAe;AACvC,OAAO,6BAA6B,GAAG,mBAAO,CAAC,6DAAiB;AAChE,OAAO,0BAA0B,GAAG,mBAAO,CAAC,mGAA2B;AACvE,sBAAsB,mBAAO,CAAC,kGAAsB;AACpD,uBAAuB,mBAAO,CAAC,mFAAsB;;AAErD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,gEAAgE,eAAe,wBAAwB,OAAO;AAC9G;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,uDAAuD,8BAA8B;;AAErF;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,iBAAiB,YAAY;AAC7B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrHA,aAAa,mBAAO,CAAC,qEAAqB;AAC1C,gBAAgB,mBAAO,CAAC,qEAAe;AACvC,eAAe,mBAAO,CAAC,kFAAW;AAClC;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,mGAA2B;;AAEvC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5FA,gBAAgB,mBAAO,CAAC,iEAAW;;AAEnC,yBAAyB,oCAAoC,6BAA6B,EAAE;AAC5F;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACZA;AACA,OAAO,sDAAsD;AAC7D,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,sDAAsD;AACrF,GAAG;AACH,OAAO,sDAAsD;AAC7D,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,sDAAsD;AACrF,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8EAAe;;AAE3D;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,sDAAsD;AACzE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACtBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA,kBAAkB,mBAAO,CAAC,iGAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,sDAAsD;AACzE;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACnBA,OAAO,0BAA0B,GAAG,mBAAO,CAAC,mGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvBA;AACA,OAAO,qDAAqD;AAC5D,oBAAoB,mBAAO,CAAC,mGAAc;AAC1C,qBAAqB,mBAAO,CAAC,qGAAe;AAC5C,YAAY,mBAAmB,qDAAqD;AACpF,GAAG;AACH,OAAO,qDAAqD;AAC5D,oBAAoB,mBAAO,CAAC,mGAAc;AAC1C,qBAAqB,mBAAO,CAAC,qGAAe;AAC5C,YAAY,mBAAmB,qDAAqD;AACpF,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,6BAA6B,GAAG,mBAAO,CAAC,8EAAe;;AAE9D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,qDAAqD;AACxE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;AChCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA,WAAW,kBAAkB;AAC7B,qDAAqD,YAAY;AACjE,KAAK;AACL,cAAc,uBAAuB;;AAErC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClDA,kBAAkB,mBAAO,CAAC,oGAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,qDAAqD;AACxE;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACrBA,OAAO,0BAA0B,GAAG,mBAAO,CAAC,sGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA;AACA,OAAO,0BAA0B;AACjC,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,0BAA0B;AACzD,GAAG;AACH,OAAO,0BAA0B;AACjC,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,0BAA0B;AACzD,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB,WAAW,QAAQ;AACnB;AACA,mBAAmB,kCAAkC;AACrD;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,yBAAyB,4BAA4B;AACrD;AACA;AACA;AACA;AACA;;AAEA,8BAA8B,cAAc;AAC5C;AACA;;;;;;;;;;;;;;ACpCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,qDAAqD,YAAY;AACjE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3CA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB,WAAW,QAAQ;AACnB;AACA,mBAAmB,0BAA0B;AAC7C;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACxBA,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA;;AAEA;AACA;AACA,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACZD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1CA,kBAAkB,mBAAO,CAAC,6FAAe;;AAEzC;;AAEA,yBAAyB,gCAAgC;;;;;;;;;;;;;;ACJzD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,+FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChDA,kBAAkB,mBAAO,CAAC,6FAAe;;AAEzC;;AAEA,yBAAyB,gCAAgC;;;;;;;;;;;;;;ACJzD,OAAO,0BAA0B,GAAG,mBAAO,CAAC,+FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA;AACA,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACtCD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,qEAAqE,YAAY;AACjF;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACzCD,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1BA;AACA,OAAO,yCAAyC;AAChD,oBAAoB,mBAAO,CAAC,iGAAc;AAC1C,qBAAqB,mBAAO,CAAC,mGAAe;AAC5C,YAAY,mBAAmB,yCAAyC;AACxE,GAAG;AACH,OAAO,yCAAyC;AAChD,oBAAoB,mBAAO,CAAC,iGAAc;AAC1C,qBAAqB,mBAAO,CAAC,mGAAe;AAC5C,YAAY,mBAAmB,yCAAyC;AACxE,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,8EAAe;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,wDAAwD;AAC3E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,iCAAiC;AACjE;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;ACnCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,oDAAoD,YAAY;AAChE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzCA,kBAAkB,mBAAO,CAAC,kGAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,yCAAyC;AAC5D,2BAA2B,yCAAyC,IAAI,gBAAgB;;;;;;;;;;;;;;ACdxF,OAAO,0BAA0B,GAAG,mBAAO,CAAC,oGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA;AACA,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,gCAAgC;AACvC,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,gCAAgC;AAC/D,GAAG;AACH,OAAO,gCAAgC;AACvC,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,gCAAgC;AAC/D,GAAG;AACH,OAAO,gCAAgC;AACvC,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,gCAAgC;AAC/D,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AC1BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,yBAAyB;AAC5C;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,kCAAkC,sBAAsB;AACxD;AACA;;AAEA,8BAA8B,cAAc;AAC5C;AACA;;;;;;;;;;;;;;AChDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,OAAO,iDAAiD,GAAG,mBAAO,CAAC,gEAAoB;;AAEvF;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,oDAAoD,YAAY;AAChE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,+CAA+C;AAClE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,kCAAkC,sBAAsB;AACxD;AACA;;AAEA,8BAA8B,cAAc;AAC5C;AACA;;;;;;;;;;;;;;ACpDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,gGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7BA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,gCAAgC;AACnD,2BAA2B,gCAAgC,IAAI,gBAAgB;;;;;;;;;;;;;;ACnB/E,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,gGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/BA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,gCAAgC;AACnD,2BAA2B,gCAAgC,IAAI,gBAAgB;;;;;;;;;;;;;;ACnB/E,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA;AACA,OAAO,UAAU;AACjB,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY,mBAAmB,UAAU;AACzC,GAAG;AACH,OAAO,UAAU;AACjB,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY,mBAAmB,UAAU;AACzC,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,UAAU;AAC7B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACtCD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,iEAAiE,YAAY;AAC7E;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,wDAAwD,YAAY;;AAEpE;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1EA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,UAAU;AAC7B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACzCD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,8FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3DA;AACA;AACA,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;AACA;;;;;;;;;;;;;;ACrBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtCA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;;AAEA,iEAAiE,gBAAgB;;;;;;;;;;;;;;ACNjF,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1BA;AACA,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,8FAAc;AAC1C,qBAAqB,mBAAO,CAAC,gGAAe;AAC5C,YAAY,mBAAmB,kBAAkB,uBAAuB,SAAS;AACjF,GAAG;AACH,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,8FAAc;AAC1C,qBAAqB,mBAAO,CAAC,gGAAe;AAC5C,YAAY,mBAAmB,kBAAkB,uBAAuB,SAAS;AACjF,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,wBAAwB,GAAG,mBAAO,CAAC,8EAAe;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,yBAAyB;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB,oBAAoB;AACzC;AACA,6BAA6B,oBAAoB;AACjD;AACA,aAAa;AACb;AACA,SAAS;AACT;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AC7BD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iCAAiC,GAAG,mBAAO,CAAC,gEAAoB;AACvE,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA,WAAW,aAAa;AACxB,gDAAgD,YAAY;AAC5D,KAAK;AACL,cAAc,uBAAuB;;AAErC;AACA;AACA,YAAY,QAAQ;AACpB,YAAY,gCAAgC;AAC5C,YAAY,uBAAuB;;AAEnC;AACA;AACA,6CAA6C,uBAAuB;AACpE;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA,mBAAmB,SAAS;AAC5B;AACA;AACA,CAAC;;;;;;;;;;;;;;AChED,kBAAkB,mBAAO,CAAC,+FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,kBAAkB;AACrC,2BAA2B,kBAAkB,IAAI,gBAAgB;;;;;;;;;;;;;;ACZjE,mBAAmB,mBAAO,CAAC,iGAAgB;;AAE3C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,SAAS;AAC5B,SAAS,0BAA0B,eAAe,SAAS;;AAE3D;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjCA;AACA,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA,mBAAmB,yBAAyB;AAC5C;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACfD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,oDAAoD,YAAY;AAChE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpCA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC,2BAA2B,kBAAkB,IAAI,gBAAgB;;;;;;;;;;;;;;ACTjE,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,gGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnCA;AACA,OAAO,yEAAyE;AAChF,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C;AACA,wBAAwB,yEAAyE;AACjG;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,yEAAyE;AAC5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AC1BD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACpCD,OAAO,QAAQ,GAAG,mBAAO,CAAC,gGAAgB;AAC1C,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxDA;AACA,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH,OAAO,6BAA6B;AACpC,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,6BAA6B;AAC5D,GAAG;AACH,OAAO,6BAA6B;AACpC,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,6BAA6B;AAC5D,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACrBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8EAAe;;AAE3D;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB;AACA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,yBAAyB,+BAA+B;AACxD;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,qBAAqB,mBAAO,CAAC,kFAAuB;AACpD,4BAA4B,mBAAO,CAAC,gGAA8B;;AAElE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,qDAAqD,YAAY;AACjE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjGA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8EAAe;;AAE3D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB;AACA;AACA,mBAAmB,qCAAqC;AACxD;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,yBAAyB,+BAA+B;AACxD;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC9BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,mGAAgB;AACnD,OAAO,iBAAiB,GAAG,mBAAO,CAAC,kFAAuB;;AAE1D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvEA,kBAAkB,mBAAO,CAAC,iGAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB;AACA;AACA,mBAAmB,6BAA6B;AAChD,2BAA2B,6BAA6B,IAAI,gBAAgB;;;;;;;;;;;;;;AChB5E,OAAO,0BAA0B,GAAG,mBAAO,CAAC,mGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtCA;AACA,OAAO,WAAW;AAClB,oBAAoB,mBAAO,CAAC,+FAAc;AAC1C,qBAAqB,mBAAO,CAAC,iGAAe;AAC5C,YAAY,mBAAmB,WAAW;AAC1C,GAAG;AACH,OAAO,WAAW;AAClB,oBAAoB,mBAAO,CAAC,+FAAc;AAC1C,qBAAqB,mBAAO,CAAC,iGAAe;AAC5C,YAAY,mBAAmB,WAAW;AAC1C,GAAG;AACH,OAAO,WAAW;AAClB,oBAAoB,mBAAO,CAAC,+FAAc;AAC1C,qBAAqB,mBAAO,CAAC,iGAAe;AAC5C,YAAY,mBAAmB,WAAW;AAC1C,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACrBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,yBAAyB,GAAG,mBAAO,CAAC,8EAAe;;AAE1D;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB;AACA,mBAAmB,WAAW;AAC9B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AClBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,+CAA+C,YAAY;AAC3D;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzDA,kBAAkB,mBAAO,CAAC,gGAAe;;AAEzC;AACA;AACA;AACA;;AAEA,mBAAmB,WAAW,8BAA8B,WAAW,IAAI,gBAAgB;;;;;;;;;;;;;;ACP3F,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,kGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnDA,kBAAkB,mBAAO,CAAC,gGAAe;;AAEzC;AACA;AACA;AACA;;AAEA,mBAAmB,WAAW,8BAA8B,WAAW,IAAI,gBAAgB;;;;;;;;;;;;;;ACP3F,OAAO,0BAA0B,GAAG,mBAAO,CAAC,kGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnCA;AACA,OAAO,gEAAgE;AACvE,oBAAoB,mBAAO,CAAC,uFAAc;AAC1C,qBAAqB,mBAAO,CAAC,yFAAe;AAC5C;AACA,wBAAwB,gEAAgE;AACxF;AACA;AACA,GAAG;AACH,OAAO,gEAAgE;AACvE,oBAAoB,mBAAO,CAAC,uFAAc;AAC1C,qBAAqB,mBAAO,CAAC,yFAAe;AAC5C;AACA,wBAAwB,gEAAgE;AACxF;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,8EAAe;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,gEAAgE;AACnF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACtBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA,kBAAkB,mBAAO,CAAC,wFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,gEAAgE;AACnF,2BAA2B,gEAAgE;AAC3F;AACA,GAAG;;;;;;;;;;;;;;ACbH,OAAO,0BAA0B,GAAG,mBAAO,CAAC,0FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxBA,wBAAwB,mBAAO,CAAC,mFAAsB;;AAEtD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,OAAO,wDAAwD;AAC/D,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,2CAA2C;AACnE;AACA;AACA;AACA,GAAG;AACH,OAAO,wDAAwD;AAC/D,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,2CAA2C;AACnE;AACA;AACA;AACA,GAAG;AACH,OAAO,wDAAwD;AAC/D,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,2CAA2C;AACnE;AACA;AACA;AACA,GAAG;AACH,OAAO,kEAAkE;AACzE,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,qDAAqD;AAC7E;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,qEAAqE;AAC7F;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,qEAAqE;AAC7F;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,qEAAqE;AAC7F;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,wFAAe;AAC3C,qBAAqB,mBAAO,CAAC,0FAAgB;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,wFAAe;AAC3C,qBAAqB,mBAAO,CAAC,0FAAgB;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AC1PA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;;AAEjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,MAAM;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,2CAA2C;AAC9D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,mCAAmC;AAC7D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gEAAoB;AAChE,OAAO,2CAA2C,GAAG,mBAAO,CAAC,oEAAgB;AAC7E,gBAAgB,mBAAO,CAAC,8EAA2B;AACnD,0BAA0B,mBAAO,CAAC,8FAA6B;;AAE/D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,OAAO,uCAAuC;AAC9C;AACA;;AAEA;AACA,mDAAmD,wBAAwB;AAC3E;AACA;AACA,wCAAwC,cAAc,mBAAmB;AACzE,GAAG;;AAEH;AACA;AACA,WAAW,8BAA8B;AACzC;AACA,yEAAyE,mBAAmB;AAC5F;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClEA,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC,mBAAmB,2CAA2C;AAC9D,kCAAkC,2CAA2C,IAAI,gBAAgB;AACjG;;;;;;;;;;;;;;ACJA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,0BAA0B,mBAAO,CAAC,8FAA6B;;AAE/D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3CA,wBAAwB,mBAAO,CAAC,sFAAyB;AACzD,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACtDA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,oBAAoB;AACpD;AACA;;AAEA,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnFA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,qGAAsB;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpEA,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC,mBAAmB,2CAA2C;AAC9D,kCAAkC,2CAA2C,IAAI,gBAAgB;AACjG;;;;;;;;;;;;;;ACJA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;;AAEjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA;AACA,WAAW,MAAM;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,qDAAqD;AACxE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,mCAAmC;AAC7D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7DA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,0BAA0B,mBAAO,CAAC,8FAA6B;AAC/D,2BAA2B,mBAAO,CAAC,sGAAiC;AACpE,OAAO,aAAa,GAAG,mBAAO,CAAC,4FAAyB;;AAExD;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;;;;;;;;;;;;;;AC7CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,mCAAmC;AAC7D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AClDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,iGAAkB;;AAEjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,wDAAwD;AAClF;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,qGAAsB;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxDA,wBAAwB,mBAAO,CAAC,sFAAyB;AACzD,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACrCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,oBAAoB;AACpD;AACA;;AAEA,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,wDAAwD;AAClF;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxEA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,qGAAsB;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC9DA,wBAAwB,mBAAO,CAAC,sFAAyB;AACzD,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACrDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,qGAAsB;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClEA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,oBAAoB;AACpD;AACA;;AAEA,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AChFA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzBA,0BAA0B,mBAAO,CAAC,uFAAwB;;AAE1D;AACA,OAAO,UAAU;AACjB,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,UAAU;AACzC,GAAG;AACH,OAAO,qDAAqD;AAC5D,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,2CAA2C;AAC1E,GAAG;AACH,OAAO,qDAAqD;AAC5D,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,2CAA2C;AAC1E,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,8EAAe;;AAE5D;AACA;AACA;AACA;;AAEA,mBAAmB,UAAU;AAC7B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACfD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,8EAAe;;AAE5D;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kCAAkC;AACrD;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/CA,kBAAkB,mBAAO,CAAC,iGAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kCAAkC;AACrD,2BAA2B,kCAAkC,IAAI,gBAAgB;;;;;;;;;;;;;;ACTjF,OAAO,0BAA0B,GAAG,mBAAO,CAAC,mGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7BA;AACA,OAAO,uCAAuC;AAC9C,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,uCAAuC;AAC/D;AACA;AACA,GAAG;AACH,OAAO,uCAAuC;AAC9C,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,uCAAuC;AAC/D;AACA;AACA,GAAG;AACH,OAAO,uCAAuC;AAC9C,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,uCAAuC;AAC/D;AACA;AACA,GAAG;AACH,OAAO,wDAAwD;AAC/D,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,wDAAwD;AAChF;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,uCAAuC;AAC1D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACpBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,UAAU;AACV;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,uCAAuC;AAC1D,2BAA2B,uCAAuC,IAAI,gBAAgB;;;;;;;;;;;;;;ACVtF,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6FAAgB;;AAEnD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA,UAAU;AACV;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvBA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,uCAAuC;AAC1D,2BAA2B,uCAAuC,IAAI,gBAAgB;;;;;;;;;;;;;;ACVtF,OAAO,0BAA0B,GAAG,mBAAO,CAAC,6FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,wDAAwD;AAC3E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACzBD,OAAO,gBAAgB,GAAG,mBAAO,CAAC,6FAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACVA,gBAAgB,mBAAO,CAAC,0EAAW;AACnC,OAAO,2DAA2D,GAAG,mBAAO,CAAC,0DAAc;;AAE3F;AACA,aAAa,uBAAuB,4DAA4D;AAChG;;AAEA;AACA,aAAa,OAAO;AACpB,cAAc,SAAS;AACvB,cAAc,EAAE,kBAAkB,aAAa;AAC/C;;AAEA;AACA,aAAa,6DAA6D;AAC1E;;AAEA,WAAW,mBAAmB;AAC9B;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA,WAAW;AACX;AACA;AACA,WAAW,mBAAO,CAAC,gFAAW;AAC9B,SAAS,mBAAO,CAAC,4EAAS;AAC1B,eAAe,mBAAO,CAAC,wFAAe;AACtC,YAAY,mBAAO,CAAC,kFAAY;AAChC;AACA;AACA;AACA;AACA,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC,eAAe,mBAAO,CAAC,wFAAe;AACtC,oBAAoB,mBAAO,CAAC,gGAAmB;AAC/C,aAAa,mBAAO,CAAC,oFAAa;AAClC,aAAa,mBAAO,CAAC,oFAAa;AAClC,cAAc,mBAAO,CAAC,sFAAc;AACpC,aAAa,mBAAO,CAAC,oFAAa;AAClC,kBAAkB,mBAAO,CAAC,8FAAkB;AAC5C,cAAc,mBAAO,CAAC,sFAAc;AACpC,iBAAiB,mBAAO,CAAC,4FAAiB;AAC1C,eAAe,mBAAO,CAAC,wFAAe;AACtC,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC,iBAAiB,mBAAO,CAAC,4FAAiB;AAC1C,kBAAkB,mBAAO,CAAC,8FAAkB;AAC5C;AACA,sBAAsB,mBAAO,CAAC,sGAAsB;AACpD,mBAAmB,mBAAO,CAAC,gGAAmB;AAC9C,UAAU,mBAAO,CAAC,8EAAU;AAC5B;AACA,mBAAmB,mBAAO,CAAC,gGAAmB;AAC9C,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC,cAAc,mBAAO,CAAC,sFAAc;AACpC,cAAc,mBAAO,CAAC,sFAAc;AACpC,mBAAmB,mBAAO,CAAC,gGAAmB;AAC9C,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC;AACA;AACA,oBAAoB,mBAAO,CAAC,kGAAoB;AAChD,oBAAoB,mBAAO,CAAC,kGAAoB;AAChD;AACA;AACA;AACA;AACA,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC;;AAEA;AACA;AACA;;AAEA;AACA,WAAW,qCAAqC;AAChD,aAAa;AACb;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA,8BAA8B,gCAAgC;AAC9D;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrGA;AACA,OAAO,6CAA6C;AACpD,oBAAoB,mBAAO,CAAC,+FAAc;AAC1C,qBAAqB,mBAAO,CAAC,iGAAe;AAC5C,YAAY,mBAAmB,sCAAsC;AACrE,GAAG;AACH,OAAO,6CAA6C;AACpD,oBAAoB,mBAAO,CAAC,+FAAc;AAC1C,qBAAqB,mBAAO,CAAC,iGAAe;AAC5C,YAAY,mBAAmB,sCAAsC;AACrE,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,yBAAyB,GAAG,mBAAO,CAAC,8EAAe;;AAE1D;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,sCAAsC;AACzD;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpCA,kBAAkB,mBAAO,CAAC,gGAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,sCAAsC;AACzD,2BAA2B,sCAAsC,IAAI,gBAAgB;;;;;;;;;;;;;;ACTrF,OAAO,0BAA0B,GAAG,mBAAO,CAAC,kGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1BA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yBAAyB,mCAAmC;AAC5D;AACA;AACA;;AAEA;;AAEA;AACA,OAAO,kEAAkE;AACzE,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,yCAAyC;AAC/E;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,mCAAmC;AACzE;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,mCAAmC;AACzE;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,mCAAmC;AACzE;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,mCAAmC;AACzE;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,mCAAmC;AACzE;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtIA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kEAAkE;AACrF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,+BAA+B,mCAAmC;AAClE;AACA;;;;;;;;;;;;;;AC9BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,+BAA+B,mCAAmC;AAClE;AACA;;;;;;;;;;;;;;ACvCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,6FAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjBA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;AChCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzCA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;AChCA,OAAO,0BAA0B,GAAG,mBAAO,CAAC,6FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/BA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACnCA,OAAO,SAAS,GAAG,mBAAO,CAAC,6FAAgB;AAC3C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gEAAoB;AAChE,OAAO,2CAA2C,GAAG,mBAAO,CAAC,oEAAgB;;AAE7E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,OAAO,sCAAsC;AAC7C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,+BAA+B,mCAAmC;AAClE;AACA;;;;;;;;;;;;;;AC7CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gEAAoB;AAChE;AACA;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,oEAAgB;;AAE5B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,sCAAsC;AAC7C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClEA;AACA,OAAO,oBAAoB;AAC3B,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C;AACA,wBAAwB,oBAAoB;AAC5C;AACA;AACA,GAAG;AACH,OAAO,oBAAoB;AAC3B,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C;AACA,wBAAwB,oBAAoB;AAC5C;AACA;AACA,GAAG;AACH,OAAO,oBAAoB;AAC3B,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C;AACA,wBAAwB,oBAAoB;AAC5C;AACA;AACA,GAAG;AACH,OAAO,qCAAqC;AAC5C,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C;AACA,wBAAwB,qBAAqB,4BAA4B,GAAG;AAC5E;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,oBAAoB;AACvC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,UAAU;AACV;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA,kBAAkB,mBAAO,CAAC,4FAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,oBAAoB;AACvC,2BAA2B,oBAAoB,IAAI,gBAAgB;;;;;;;;;;;;;;ACTnE,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,8FAAgB;;AAEnD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA,UAAU;AACV;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvBA,kBAAkB,mBAAO,CAAC,4FAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,oBAAoB;AACvC,2BAA2B,oBAAoB,IAAI,gBAAgB;;;;;;;;;;;;;;ACTnE,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,mBAAmB;AACtC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,uBAAuB,mCAAmC;AAC1D;AACA;;;;;;;;;;;;;;AC5BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;AAC5F,OAAO,iBAAiB,GAAG,mBAAO,CAAC,8FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA,UAAU;AACV;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA;AACA;AACA,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACrBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvCA,kBAAkB,mBAAO,CAAC,4FAAe;;AAEzC;AACA;AACA;;AAEA,mDAAmD,gBAAgB;;;;;;;;;;;;;;ACNnE,mBAAmB,mBAAO,CAAC,8FAAgB;;AAE3C,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7BA,kBAAkB,mBAAO,CAAC,4FAAe;;AAEzC;AACA;AACA;;AAEA,mDAAmD,gBAAgB;;;;;;;;;;;;;;ACNnE,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1BA,wBAAwB,mBAAO,CAAC,mFAAsB;;AAEtD;AACA;;AAEA;AACA,OAAO,iCAAiC;AACxC,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,oBAAoB;AACnD,GAAG;AACH,OAAO,iCAAiC;AACxC,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,oBAAoB;AACnD,GAAG;AACH,OAAO,kFAAkF;AACzF,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,oCAAoC;AACnE,GAAG;AACH,OAAO,kFAAkF;AACzF,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,oCAAoC;AACnE,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AC/BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA,gDAAgD,8BAA8B;AAC9E;AACA;AACA;AACA;AACA,mBAAmB,oBAAoB;AACvC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,+CAA+C;AACzE;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;;AAED;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,oBAAoB;AACvC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,4BAA4B;AACtD;AACA;;;;;;;;;;;;;;AC3BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,oCAAoC;AACvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,4BAA4B;AACtD;AACA;;;;;;;;;;;;;;AC/BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnDA,kBAAkB,mBAAO,CAAC,6FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,oCAAoC;AACvD,2BAA2B,oCAAoC,IAAI,gBAAgB;;;;;;;;;;;;;;ACbnF,OAAO,0BAA0B,GAAG,mBAAO,CAAC,+FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7BA;AACA,OAAO,SAAS;AAChB,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,SAAS;AACxC,GAAG;AACH,OAAO,SAAS;AAChB,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,SAAS;AACxC,GAAG;AACH,OAAO,SAAS;AAChB,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,SAAS;AACxC,GAAG;AACH,OAAO,SAAS;AAChB,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,SAAS;AACxC,GAAG;AACH,OAAO,iCAAiC;AACxC,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,iCAAiC;AAChE,GAAG;AACH,OAAO,iCAAiC;AACxC,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,iCAAiC;AAChE,GAAG;AACH,OAAO,iCAAiC;AACxC,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,iCAAiC;AAChE,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACzCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,mBAAmB,GAAG,mBAAO,CAAC,8EAAe;;AAEpD;AACA;AACA;AACA;;AAEA,mBAAmB,SAAS;AAC5B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACfD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,iBAAiB;AAC5B;AACA;;AAEA;AACA;AACA,GAAG;;AAEH;AACA;AACA,WAAW,qBAAqB;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1EA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,mBAAmB,GAAG,mBAAO,CAAC,8EAAe;;AAEpD;AACA;AACA;AACA;;AAEA,mBAAmB,SAAS;AAC5B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACfD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,4FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzDA,kBAAkB,mBAAO,CAAC,0FAAe;;AAEzC;AACA;AACA;AACA;;AAEA,mBAAmB,SAAS,8BAA8B,SAAS,IAAI,gBAAgB;;;;;;;;;;;;;;ACPvF,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,4FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3DA,kBAAkB,mBAAO,CAAC,0FAAe;;AAEzC;AACA;AACA;AACA;;AAEA,mBAAmB,SAAS,8BAA8B,SAAS,IAAI,gBAAgB;;;;;;;;;;;;;;ACPvF,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,4FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7DA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,mBAAmB,GAAG,mBAAO,CAAC,8EAAe;;AAEpD;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,wCAAwC;AAC3D;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,OAAO,mCAAmC,GAAG,mBAAO,CAAC,4FAAgB;;AAErE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA,kBAAkB,mBAAO,CAAC,0FAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,wCAAwC;AAC3D,2BAA2B,iCAAiC,IAAI,gBAAgB;;;;;;;;;;;;;;ACThF,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,4FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/DA,kBAAkB,mBAAO,CAAC,0FAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,wCAAwC;AAC3D,2BAA2B,iCAAiC,IAAI,gBAAgB;;;;;;;;;;;;;;ACThF,OAAO,0BAA0B,GAAG,mBAAO,CAAC,4FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA;AACA;;AAEA;AACA,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,+CAA+C;AACtD,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,+CAA+C;AAC9E,GAAG;AACH,OAAO,+EAA+E;AACtF,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,+EAA+E;AACtF,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,+EAA+E;AACtF,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,+CAA+C;AACtD,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AC1EA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,qCAAqC;AAC/D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,+CAA+C;AAClE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,6DAA6D;AACvF;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,gGAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACdA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8DAA8D;AACjF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,qCAAqC;AAC/D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,gGAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACdA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8DAA8D;AACjF,2BAA2B,8DAA8D;AACzF;AACA,GAAG;;;;;;;;;;;;;;ACnBH,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,gGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;;;;;;;;;;;;;AClCA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8DAA8D;AACjF,2BAA2B,8DAA8D;AACzF;AACA,GAAG;;;;;;;;;;;;;;ACnBH,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,+CAA+C;AAClE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,qCAAqC;AAC/D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,gGAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACdA;AACA,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AC1BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,YAAY;AACtC;AACA;;;;;;;;;;;;;;AC3BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjDA,kBAAkB,mBAAO,CAAC,6FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC,2BAA2B,kBAAkB,IAAI,gBAAgB;;;;;;;;;;;;;;ACZjE,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,YAAY;AACtC;AACA;;;;;;;;;;;;;;AC3BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,+FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA,kBAAkB,mBAAO,CAAC,6FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC,2BAA2B,kBAAkB,IAAI,gBAAgB;;;;;;;;;;;;;;ACZjE,OAAO,0BAA0B,GAAG,mBAAO,CAAC,+FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/BA;AACA,OAAO,2BAA2B;AAClC,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C,YAAY,mBAAmB,2BAA2B;AAC1D,GAAG;AACH,OAAO,2BAA2B;AAClC,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C,YAAY,mBAAmB,2BAA2B;AAC1D,GAAG;AACH,OAAO,wCAAwC;AAC/C,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C,YAAY,mBAAmB,wCAAwC;AACvE,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACrGA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,kBAAkB,GAAG,mBAAO,CAAC,8EAAe;AACnD,mBAAmB,mBAAO,CAAC,oFAAqB;;AAEhD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM,mCAAmC;AACzC,MAAM,mCAAmC;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,gBAAgB,QAAQ;AACxB;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,QAAQ;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB;AACrB;AACA,mBAAmB,2BAA2B;AAC9C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,2BAA2B,sBAAsB;AACjD,iCAAiC,uCAAuC;AACxE;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrFA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA,WAAW,YAAY;AACvB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChDA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;;AAEA,mBAAmB,2BAA2B;AAC9C,kCAAkC,2BAA2B,IAAI,gBAAgB;AACjF;;;;;;;;;;;;;;ACPA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,2FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,kBAAkB,GAAG,mBAAO,CAAC,8EAAe;AACnD,mBAAmB,mBAAO,CAAC,oFAAqB;AAChD,OAAO,qBAAqB,GAAG,mBAAO,CAAC,sGAA8B;;AAErE;AACA;;AAEA,mBAAmB,qDAAqD;AACxE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;;AAEA,iBAAiB,oBAAoB;AACrC;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,iDAAiD,sBAAsB;AACvE,iCAAiC,oDAAoD;;AAErF;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,eAAe,iDAAiD;AAChE,GAAG;;AAEH;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjEA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,2FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvCA,aAAa,mBAAO,CAAC,wEAAwB;AAC7C,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,kBAAkB,GAAG,mBAAO,CAAC,8EAAe;AACnD,OAAO,QAAQ,GAAG,mBAAO,CAAC,sGAA8B;AACxD,eAAe,mBAAO,CAAC,0GAAgC;AACvD,OAAO,cAAc,GAAG,mBAAO,CAAC,4FAAyB;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kCAAkC,OAAO;AACzC,gBAAgB,QAAQ;AACxB,mBAAmB,QAAQ;AAC3B,+CAA+C;AAC/C,qBAAqB;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,2BAA2B,sDAAsD;AACjF;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;;AAEA;AACA;AACA,8BAA8B,sDAAsD;AACpF;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtHA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA,WAAW,YAAY;AACvB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvDA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;AClCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,2FAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjBA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;AClCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,2FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1CA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACrCA,OAAO,0BAA0B,GAAG,mBAAO,CAAC,2FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/BA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACrCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,2FAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClBA;AACA,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,iGAAc;AAC1C,qBAAqB,mBAAO,CAAC,mGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,iGAAc;AAC1C,qBAAqB,mBAAO,CAAC,mGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,8EAAe;;AAE5D;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AClBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C;AACA;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,oEAAgB;;AAE5B,OAAO,uBAAuB,GAAG,mBAAO,CAAC,gEAAoB;AAC7D;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1DA,kBAAkB,mBAAO,CAAC,kGAAe;;AAEzC;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA,mBAAmB,YAAY,8BAA8B,YAAY,IAAI,gBAAgB;;;;;;;;;;;;;;ACV7F,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,oGAAgB;AACnD,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;;AAE9D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpCA;AACA,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,8FAAc;AAC1C,qBAAqB,mBAAO,CAAC,gGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,8FAAc;AAC1C,qBAAqB,mBAAO,CAAC,gGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,wBAAwB,GAAG,mBAAO,CAAC,8EAAe;;AAEzD;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA,kBAAkB,mBAAO,CAAC,+FAAe;;AAEzC,mBAAmB,YAAY,OAAO,eAAe,YAAY,kBAAkB;;;;;;;;;;;;;;ACFnF,OAAO,mCAAmC,GAAG,mBAAO,CAAC,iGAAgB;;AAErE;AACA;AACA;AACA;;;;;;;;;;;;;;ACLA;AACA,OAAO,mDAAmD;AAC1D,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,mDAAmD;AAC3E;AACA;AACA,GAAG;AACH,OAAO,mDAAmD;AAC1D,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,mDAAmD;AAC3E;AACA;AACA,GAAG;AACH,OAAO,mDAAmD;AAC1D,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,mDAAmD;AAC3E;AACA;AACA,GAAG;AACH,OAAO,oEAAoE;AAC3E,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,oEAAoE;AAC5F;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,mDAAmD;AACtE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,6BAA6B;AAC7D;AACA;;;;;;;;;;;;;;AC5BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,mDAAmD;AACtE,2BAA2B,mDAAmD,IAAI,gBAAgB;;;;;;;;;;;;;;ACblG,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,mDAAmD;AACtE,2BAA2B,mDAAmD,IAAI,gBAAgB;;;;;;;;;;;;;;ACblG,OAAO,0BAA0B,GAAG,mBAAO,CAAC,6FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,6BAA6B;AAC7D;AACA;;;;;;;;;;;;;;ACvCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,6FAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACXA;AACA,OAAO,8DAA8D;AACrE,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C;AACA,wBAAwB,8DAA8D;AACtF;AACA;AACA,GAAG;AACH,OAAO,8DAA8D;AACrE,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C;AACA,wBAAwB,8DAA8D;AACtF;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8EAAe;;AAE3D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8DAA8D;AACjF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,8BAA8B;AACxD;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA,WAAW,aAAa;AACxB,gDAAgD,YAAY;AAC5D,KAAK;AACL,cAAc,uBAAuB;;AAErC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClDA,kBAAkB,mBAAO,CAAC,iGAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8DAA8D;AACjF,2BAA2B,8DAA8D;AACzF;AACA,GAAG;;;;;;;;;;;;;;ACnBH,OAAO,0BAA0B,GAAG,mBAAO,CAAC,mGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6BAA6B,gEAAgE;AAC7F,UAAU,sBAAsB;AAChC;AACA,kEAAkE,2DAA2D;AAC7H,0DAA0D,2CAA2C;AACrG,kGAAkG,oDAAoD;AACtJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qCAAqC,QAAQ;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA,yBAAyB,mBAAO,CAAC,mFAAoB;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACRA;AACA,WAAW,mBAAO,CAAC,6EAAW;AAC9B,YAAY,mBAAO,CAAC,+EAAY;AAChC;;;;;;;;;;;;;;ACHA,gBAAgB,mBAAO,CAAC,qEAAe;;AAEvC;;AAEA,mBAAmB,yEAAyE;AAC5F;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACVD;AACA;AACA;AACA;;;;;;;;;;;;;;ACHA;AACA,WAAW,mBAAO,CAAC,kFAAW;AAC9B,YAAY,mBAAO,CAAC,oFAAY;AAChC;;;;;;;;;;;;;;ACHA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gBAAgB,mBAAO,CAAC,qEAAe;;AAEvC;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,cAAc,OAAO,EAAE,EAAE,GAAG,cAAc;AAC1C;AACA;;AAEA;AACA;;AAEA,yBAAyB,+BAA+B;AACxD,6DAA6D,sBAAsB;AACnF;AACA;AACA,aAAa,UAAU,EAAE,IAAI;AAC7B;;AAEA,wBAAwB,QAAQ,GAAG,UAAU,cAAc,uBAAuB,EAAE,IAAI,EAAE,UAAU,EAAE,UAAU;;AAEhH;AACA;AACA;AACA,KAAK;AACL;AACA;;;;;;;;;;;;;;AC7DA;AACA;AACA;AACA;;;;;;;;;;;;;;ACHA;AACA,WAAW,mBAAO,CAAC,4EAAW;AAC9B,YAAY,mBAAO,CAAC,8EAAY;AAChC;;;;;;;;;;;;;;ACHA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gBAAgB,mBAAO,CAAC,qEAAe;;AAEvC;;AAEA,mBAAmB,mDAAmD;AACtE;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AC3BD;AACA;AACA;AACA;;;;;;;;;;;;;;ACHA,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C,mBAAmB,eAAe;AAClC;AACA,CAAC;;;;;;;;;;;;;;ACJD,+IAAoD;;;;;;;;;;;;;;ACApD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;;AAEA,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C,mBAAmB,qBAAqB;AACxC;AACA,CAAC;;;;;;;;;;;;;;ACrBD,qCAAqC,2BAA2B;;AAEhE,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,gCAAgC,+BAA+B,KAAK;;AAEpE,YAAY;AACZ,GAAG;AACH;;;;;;;;;;;;;;ACtBA;AACA;AACA,aAAa,mBAAO,CAAC,sGAAwB;AAC7C,cAAc,mBAAO,CAAC,wGAAyB;AAC/C,GAAG;AACH;AACA,aAAa,mBAAO,CAAC,sGAAwB;AAC7C,cAAc,mBAAO,CAAC,wGAAyB;AAC/C,GAAG;AACH;;;;;;;;;;;;;;ACTA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;ACdA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,OAAO,2DAA2D,GAAG,mBAAO,CAAC,uDAAW;;AAExF,mBAAmB,aAAoB;AACvC,mCAAmC,mBAAO,CAAC,0EAAiB,IAAI,mBAAO,CAAC,gEAAY;;AAEpF;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,SAAS,4CAA4C;;AAErD;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX,0DAA0D,wBAAwB;AAClF;AACA,SAAS;AACT;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA,aAAa,4GAA4G;AACzH;;AAEA;AACA,WAAW,mCAAmC;AAC9C,aAAa;AACb;AACA,2BAA2B;AAC3B;AACA,oCAAoC;AACpC;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;AC1EA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACbA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW,aAAa;AACxB,WAAW,oBAAoB;AAC/B,aAAa;AACb;AACA;AACA;AACA;AACA,gBAAgB,gBAAgB;AAChC;;AAEA;AACA,mEAAmE;AACnE;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,6BAA6B,iDAAiD;;AAE9E,0DAA0D,gBAAgB;AAC1E;AACA;AACA;AACA,eAAe,UAAU;AACzB;AACA,OAAO;AACP;AACA,eAAe,SAAS;AACxB;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;AC1DA,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;;AAExD;AACA;AACA;AACA;;AAEA,sBAAsB,yBAAyB,KAAK;AACpD;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;;AAEA;;;;;;;;;;;;;;AC9DA;AACA;AACA;AACA,WAAW,gBAAgB;AAC3B,aAAa;AACb;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;ACXA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACTA,OAAO,SAAS,GAAG,mBAAO,CAAC,kBAAM;AACjC,OAAO,qBAAqB,GAAG,mBAAO,CAAC,uDAAW;;AAElD;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA,eAAe,8BAA8B,KAAK;AAClD;AACA,kEAAkE,eAAe;AACjF;;AAEA;AACA;AACA;AACA;AACA;AACA,8BAA8B,eAAe,KAAK,YAAY;AAC9D;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC9DA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,EAAE;AACf,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,0BAA0B;AACvC,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe,MAAM;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe,MAAM;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,qBAAqB;AAClC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,qBAAqB;AAClC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,qBAAqB;AAClC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe,MAAM;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,qBAAqB;AAClC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,OAAO;AACtB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe,OAAO;AACtB;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe,OAAO;AACtB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA;;AAEA;;;;;;;;;;;;;;ACtVA;AACA;AACA,WAAW,+BAA+B;AAC1C;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjBA;AACA,WAAW,IAAI;AACf;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,+BAA+B,OAAO;AACtC;AACA;AACA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACxBA;AACA;AACA;AACA,GAAG;;;;;;;;;;;;;;ACHH,OAAO,OAAO;AACd;AACA,yCAAyC,gCAAgC,KAAK;;;;;;;;;;;;;;ACF9E,cAAc,mBAAO,CAAC,0DAAS;AAC/B,OAAO,iBAAiB,GAAG,mBAAO,CAAC,uDAAW;;AAE9C;AACA;AACA,GAAG,iFAAiF;AACpF;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;;;;;;;;;;;;;;AC5CA;;AAEA,oCAAoC,SAAS,GAAG,KAAK,EAAE,uBAAuB;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ACFnD;AAC0B;AACrD;AACA;AACA;AACA;AACA;AACA,IAAI,4DAAqB;AACzB;AACA,GAAG;AACH,IAAI,4DAAqB;AACzB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iCAAiC,gBAAgB;AACjD,UAAU,+DAAW;AACrB;AACA;AACA;AACoE;;;;;;;;;;;;;;;;;;;;AC5CpE;AACA;AACsB","file":"251.js","sourcesContent":["const Kafka = require('./src')\nconst PartitionAssigners = require('./src/consumer/assigners')\nconst AssignerProtocol = require('./src/consumer/assignerProtocol')\nconst Partitioners = require('./src/producer/partitioners')\nconst Compression = require('./src/protocol/message/compression')\nconst ResourceTypes = require('./src/protocol/resourceTypes')\nconst ConfigResourceTypes = require('./src/protocol/configResourceTypes')\nconst ConfigSource = require('./src/protocol/configSource')\nconst AclResourceTypes = require('./src/protocol/aclResourceTypes')\nconst AclOperationTypes = require('./src/protocol/aclOperationTypes')\nconst AclPermissionTypes = require('./src/protocol/aclPermissionTypes')\nconst ResourcePatternTypes = require('./src/protocol/resourcePatternTypes')\nconst Errors = require('./src/errors')\nconst { LEVELS } = require('./src/loggers')\n\nmodule.exports = {\n  Kafka,\n  PartitionAssigners,\n  AssignerProtocol,\n  Partitioners,\n  logLevel: LEVELS,\n  CompressionTypes: Compression.Types,\n  CompressionCodecs: Compression.Codecs,\n  /**\n   * @deprecated\n   * @see https://github.com/tulios/kafkajs/issues/649\n   *\n   * Use ConfigResourceTypes or AclResourceTypes instead.\n   */\n  ResourceTypes,\n  ConfigResourceTypes,\n  AclResourceTypes,\n  AclOperationTypes,\n  AclPermissionTypes,\n  ResourcePatternTypes,\n  ConfigSource,\n  ...Errors,\n}\n","const createRetry = require('../retry')\nconst flatten = require('../utils/flatten')\nconst waitFor = require('../utils/waitFor')\nconst groupBy = require('../utils/groupBy')\nconst createConsumer = require('../consumer')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { LEVELS } = require('../loggers')\nconst {\n  KafkaJSNonRetriableError,\n  KafkaJSDeleteGroupsError,\n  KafkaJSBrokerNotFound,\n  KafkaJSDeleteTopicRecordsError,\n  KafkaJSAggregateError,\n} = require('../errors')\nconst { staleMetadata } = require('../protocol/error')\nconst CONFIG_RESOURCE_TYPES = require('../protocol/configResourceTypes')\nconst ACL_RESOURCE_TYPES = require('../protocol/aclResourceTypes')\nconst ACL_OPERATION_TYPES = require('../protocol/aclOperationTypes')\nconst ACL_PERMISSION_TYPES = require('../protocol/aclPermissionTypes')\nconst RESOURCE_PATTERN_TYPES = require('../protocol/resourcePatternTypes')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\n\nconst { CONNECT, DISCONNECT } = events\n\nconst NO_CONTROLLER_ID = -1\n\nconst { values, keys, entries } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `admin.events.${key}`)\n  .join(', ')\n\nconst retryOnLeaderNotAvailable = (fn, opts = {}) => {\n  const callback = async () => {\n    try {\n      return await fn()\n    } catch (e) {\n      if (e.type !== 'LEADER_NOT_AVAILABLE') {\n        throw e\n      }\n      return false\n    }\n  }\n\n  return waitFor(callback, opts)\n}\n\nconst isConsumerGroupRunning = description => ['Empty', 'Dead'].includes(description.state)\nconst findTopicPartitions = async (cluster, topic) => {\n  await cluster.addTargetTopic(topic)\n  await cluster.refreshMetadataIfNecessary()\n\n  return cluster\n    .findTopicPartitionMetadata(topic)\n    .map(({ partitionId }) => partitionId)\n    .sort()\n}\nconst indexByPartition = array =>\n  array.reduce(\n    (obj, { partition, ...props }) => Object.assign(obj, { [partition]: { ...props } }),\n    {}\n  )\n\n/**\n *\n * @param {Object} params\n * @param {import(\"../../types\").Logger} params.logger\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {import('../../types').RetryOptions} params.retry\n * @param {import(\"../../types\").Cluster} params.cluster\n *\n * @returns {import(\"../../types\").Admin}\n */\nmodule.exports = ({\n  logger: rootLogger,\n  instrumentationEmitter: rootInstrumentationEmitter,\n  retry,\n  cluster,\n}) => {\n  const logger = rootLogger.namespace('Admin')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n\n  /**\n   * @returns {Promise}\n   */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const disconnect = async () => {\n    await cluster.disconnect()\n    instrumentationEmitter.emit(DISCONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const listTopics = async () => {\n    const { topicMetadata } = await cluster.metadata()\n    const topics = topicMetadata.map(t => t.topic)\n    return topics\n  }\n\n  /**\n   * @param {Object} request\n   * @param {array} request.topics\n   * @param {boolean} [request.validateOnly=false]\n   * @param {number} [request.timeout=5000]\n   * @param {boolean} [request.waitForLeaders=true]\n   * @return {Promise}\n   */\n  const createTopics = async ({ topics, validateOnly, timeout, waitForLeaders = true }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topics.map(({ topic }) => topic))\n    if (topicNames.size < topics.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createTopics({ topics, validateOnly, timeout })\n\n        if (waitForLeaders) {\n          const topicNamesArray = Array.from(topicNames.values())\n          await retryOnLeaderNotAvailable(async () => await broker.metadata(topicNamesArray), {\n            delay: 100,\n            maxWait: timeout,\n            timeoutMessage: 'Timed out while waiting for topic leaders',\n          })\n        }\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e instanceof KafkaJSAggregateError) {\n          if (e.errors.every(error => error.type === 'TOPIC_ALREADY_EXISTS')) {\n            return false\n          }\n        }\n\n        bail(e)\n      }\n    })\n  }\n  /**\n   * @param {array} topicPartitions\n   * @param {boolean} [validateOnly=false]\n   * @param {number} [timeout=5000]\n   * @return {Promise<void>}\n   */\n  const createPartitions = async ({ topicPartitions, validateOnly, timeout }) => {\n    if (!topicPartitions || !Array.isArray(topicPartitions)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic partitions array ${topicPartitions}`)\n    }\n    if (topicPartitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Empty topic partitions array`)\n    }\n\n    if (topicPartitions.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topicPartitions.map(({ topic }) => topic))\n    if (topicNames.size < topicPartitions.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createPartitions({ topicPartitions, validateOnly, timeout })\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string[]} topics\n   * @param {number} [timeout=5000]\n   * @return {Promise}\n   */\n  const deleteTopics = async ({ topics, timeout }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(topic => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError('Invalid topics array, the names must be a valid string')\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.deleteTopics({ topics, timeout })\n\n        // Remove deleted topics\n        for (const topic of topics) {\n          cluster.targetTopics.delete(topic)\n        }\n\n        await cluster.refreshMetadata()\n      } catch (e) {\n        if (['NOT_CONTROLLER', 'UNKNOWN_TOPIC_OR_PARTITION'].includes(e.type)) {\n          logger.warn('Could not delete topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e.type === 'REQUEST_TIMED_OUT') {\n          logger.error(\n            'Could not delete topics, check if \"delete.topic.enable\" is set to \"true\" (the default value is \"false\") or increase the timeout',\n            {\n              error: e.message,\n              retryCount,\n              retryTime,\n            }\n          )\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   */\n\n  const fetchTopicOffsets = async topic => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const low = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: true,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const { partitions: highPartitions } = high.pop()\n        const { partitions: lowPartitions } = low.pop()\n        return highPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset,\n          high: offset,\n          low: lowPartitions.find(({ partition: lowPartition }) => lowPartition === partition)\n            .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} [timestamp]\n   */\n\n  const fetchTopicOffsetsByTimestamp = async (topic, timestamp) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const partitions = metadata.map(p => ({ partition: p.partitionId }))\n\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions,\n          },\n        ])\n        const { partitions: highPartitions } = high.pop()\n\n        const offsets = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromTimestamp: timestamp,\n            partitions,\n          },\n        ])\n        const { partitions: lowPartitions } = offsets.pop()\n\n        return lowPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset:\n            parseInt(offset, 10) >= 0\n              ? offset\n              : highPartitions.find(({ partition: highPartition }) => highPartition === partition)\n                  .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Fetch offsets for a topic or multiple topics\n   *\n   * Note: set either topic or topics but not both.\n   *\n   * @param {string} groupId\n   * @param {string} topic - deprecated, use the `topics` parameter. Topic to fetch offsets for.\n   * @param {string[]} topics - list of topics to fetch offsets for, defaults to `[]` which fetches all topics for `groupId`.\n   * @param {boolean} [resolveOffsets=false]\n   * @return {Promise}\n   */\n  const fetchOffsets = async ({ groupId, topic, topics, resolveOffsets = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic && !topics) {\n      topics = []\n    }\n\n    if (!topic && !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Expected topic or topics array to be set`)\n    }\n\n    if (topic && topics) {\n      throw new KafkaJSNonRetriableError(`Either topic or topics must be set, not both`)\n    }\n\n    if (topic) {\n      topics = [topic]\n    }\n\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const topicsToFetch = await Promise.all(\n      topics.map(async topic => {\n        const partitions = await findTopicPartitions(cluster, topic)\n        const partitionsToFetch = partitions.map(partition => ({ partition }))\n        return { topic, partitions: partitionsToFetch }\n      })\n    )\n    let { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: topicsToFetch,\n    })\n\n    if (resolveOffsets) {\n      consumerOffsets = await Promise.all(\n        consumerOffsets.map(async ({ topic, partitions }) => {\n          const indexedOffsets = indexByPartition(await fetchTopicOffsets(topic))\n          const recalculatedPartitions = partitions.map(({ offset, partition, ...props }) => {\n            let resolvedOffset = offset\n            if (Number(offset) === EARLIEST_OFFSET) {\n              resolvedOffset = indexedOffsets[partition].low\n            }\n            if (Number(offset) === LATEST_OFFSET) {\n              resolvedOffset = indexedOffsets[partition].high\n            }\n            return {\n              partition,\n              offset: resolvedOffset,\n              ...props,\n            }\n          })\n\n          await setOffsets({ groupId, topic, partitions: recalculatedPartitions })\n\n          return {\n            topic,\n            partitions: recalculatedPartitions,\n          }\n        })\n      )\n    }\n\n    const result = consumerOffsets.map(({ topic, partitions }) => {\n      const completePartitions = partitions.map(({ partition, offset, metadata }) => ({\n        partition,\n        offset,\n        metadata: metadata || null,\n      }))\n\n      return { topic, partitions: completePartitions }\n    })\n\n    if (topic) {\n      return result.pop().partitions\n    } else {\n      return result\n    }\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {boolean} [earliest=false]\n   * @return {Promise}\n   */\n  const resetOffsets = async ({ groupId, topic, earliest = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const partitions = await findTopicPartitions(cluster, topic)\n    const partitionsToSeek = partitions.map(partition => ({\n      partition,\n      offset: cluster.defaultOffset({ fromBeginning: earliest }),\n    }))\n\n    return setOffsets({ groupId, topic, partitions: partitionsToSeek })\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const setOffsets = async ({ groupId, topic, partitions }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const consumer = createConsumer({\n      logger: rootLogger.namespace('Admin', LEVELS.NOTHING),\n      cluster,\n      groupId,\n    })\n\n    await consumer.subscribe({ topic, fromBeginning: true })\n    const description = await consumer.describeGroup()\n\n    if (!isConsumerGroupRunning(description)) {\n      throw new KafkaJSNonRetriableError(\n        `The consumer group must have no running instances, current state: ${description.state}`\n      )\n    }\n\n    return new Promise((resolve, reject) => {\n      consumer.on(consumer.events.FETCH, async () =>\n        consumer\n          .stop()\n          .then(resolve)\n          .catch(reject)\n      )\n\n      consumer\n        .run({\n          eachBatchAutoResolve: false,\n          eachBatch: async () => true,\n        })\n        .catch(reject)\n\n      // This consumer doesn't need to consume any data\n      consumer.pause([{ topic }])\n\n      for (const seekData of partitions) {\n        consumer.seek({ topic, ...seekData })\n      }\n    })\n  }\n\n  const isBrokerConfig = type =>\n    [CONFIG_RESOURCE_TYPES.BROKER, CONFIG_RESOURCE_TYPES.BROKER_LOGGER].includes(type)\n\n  /**\n   * Broker configs can only be returned by the target broker\n   *\n   * @see\n   * https://github.com/apache/kafka/blob/821c1ac6641845aeca96a43bc2b946ecec5cba4f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L3783\n   * https://github.com/apache/kafka/blob/821c1ac6641845aeca96a43bc2b946ecec5cba4f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L2027\n   *\n   * @param {Broker} defaultBroker. Broker used in case the configuration is not a broker config\n   */\n  const groupResourcesByBroker = ({ resources, defaultBroker }) =>\n    groupBy(resources, async ({ type, name: nodeId }) => {\n      return isBrokerConfig(type)\n        ? await cluster.findBroker({ nodeId: String(nodeId) })\n        : defaultBroker\n    })\n\n  /**\n   * @param {Array<ResourceConfigQuery>} resources\n   * @param {boolean} [includeSynonyms=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfigQuery\n   * @property {ConfigResourceType} type\n   * @property {string} name\n   * @property {Array<String>} [configNames=[]]\n   */\n  const describeConfigs = async ({ resources, includeSynonyms }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(\n      r => !Array.isArray(r.configNames) && r.configNames != null\n    )\n\n    if (invalidConfigs) {\n      const { configNames } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configNames ${configNames}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const controller = await cluster.findControllerBroker()\n        const resourcerByBroker = await groupResourcesByBroker({\n          resources,\n          defaultBroker: controller,\n        })\n\n        const describeConfigsAction = async broker => {\n          const targetBroker = broker || controller\n          return targetBroker.describeConfigs({\n            resources: resourcerByBroker.get(targetBroker),\n            includeSynonyms,\n          })\n        }\n\n        const brokers = Array.from(resourcerByBroker.keys())\n        const responses = await Promise.all(brokers.map(describeConfigsAction))\n        const responseResources = responses.reduce(\n          (result, { resources }) => [...result, ...resources],\n          []\n        )\n\n        return { resources: responseResources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ResourceConfig>} resources\n   * @param {boolean} [validateOnly=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfig\n   * @property {ConfigResourceType} type\n   * @property {string} name\n   * @property {Array<ResourceConfigEntry>} configEntries\n   *\n   * @typedef {Object} ResourceConfigEntry\n   * @property {string} name\n   * @property {string} value\n   */\n  const alterConfigs = async ({ resources, validateOnly }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(r => !Array.isArray(r.configEntries))\n\n    if (invalidConfigs) {\n      const { configEntries } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configEntries ${configEntries}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const invalidConfigValue = resources.find(r =>\n      r.configEntries.some(e => typeof e.name !== 'string' || typeof e.value !== 'string')\n    )\n\n    if (invalidConfigValue) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource config value: ${JSON.stringify(invalidConfigValue)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const controller = await cluster.findControllerBroker()\n        const resourcerByBroker = await groupResourcesByBroker({\n          resources,\n          defaultBroker: controller,\n        })\n\n        const alterConfigsAction = async broker => {\n          const targetBroker = broker || controller\n          return targetBroker.alterConfigs({\n            resources: resourcerByBroker.get(targetBroker),\n            validateOnly: !!validateOnly,\n          })\n        }\n\n        const brokers = Array.from(resourcerByBroker.keys())\n        const responses = await Promise.all(brokers.map(alterConfigsAction))\n        const responseResources = responses.reduce(\n          (result, { resources }) => [...result, ...resources],\n          []\n        )\n\n        return { resources: responseResources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not alter configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @deprecated - This method was replaced by `fetchTopicMetadata`. This implementation\n   * is limited by the topics in the target group, so it can't fetch all topics when\n   * necessary.\n   *\n   * Fetch metadata for provided topics.\n   *\n   * If no topics are provided fetch metadata for all topics of which we are aware.\n   * @see https://kafka.apache.org/protocol#The_Messages_Metadata\n   *\n   * @param {Object} [options]\n   * @param {string[]} [options.topics]\n   * @return {Promise<TopicsMetadata>}\n   *\n   * @typedef {Object} TopicsMetadata\n   * @property {Array<TopicMetadata>} topics\n   *\n   * @typedef {Object} TopicMetadata\n   * @property {String} name\n   * @property {Array<PartitionMetadata>} partitions\n   *\n   * @typedef {Object} PartitionMetadata\n   * @property {number} partitionErrorCode Response error code\n   * @property {number} partitionId Topic partition id\n   * @property {number} leader  The id of the broker acting as leader for this partition.\n   * @property {Array<number>} replicas The set of all nodes that host this partition.\n   * @property {Array<number>} isr The set of nodes that are in sync with the leader for this partition.\n   */\n  const getTopicMetadata = async options => {\n    const { topics } = options || {}\n\n    if (topics) {\n      await Promise.all(\n        topics.map(async topic => {\n          if (!topic) {\n            throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n          }\n\n          try {\n            await cluster.addTargetTopic(topic)\n          } catch (e) {\n            e.message = `Failed to add target topic ${topic}: ${e.message}`\n            throw e\n          }\n        })\n      )\n    }\n\n    await cluster.refreshMetadataIfNecessary()\n    const targetTopics = topics || [...cluster.targetTopics]\n\n    return {\n      topics: await Promise.all(\n        targetTopics.map(async topic => ({\n          name: topic,\n          partitions: cluster.findTopicPartitionMetadata(topic),\n        }))\n      ),\n    }\n  }\n\n  /**\n   * Fetch metadata for provided topics.\n   *\n   * If no topics are provided fetch metadata for all topics.\n   * @see https://kafka.apache.org/protocol#The_Messages_Metadata\n   *\n   * @param {Object} [options]\n   * @param {string[]} [options.topics]\n   * @return {Promise<TopicsMetadata>}\n   *\n   * @typedef {Object} TopicsMetadata\n   * @property {Array<TopicMetadata>} topics\n   *\n   * @typedef {Object} TopicMetadata\n   * @property {String} name\n   * @property {Array<PartitionMetadata>} partitions\n   *\n   * @typedef {Object} PartitionMetadata\n   * @property {number} partitionErrorCode Response error code\n   * @property {number} partitionId Topic partition id\n   * @property {number} leader  The id of the broker acting as leader for this partition.\n   * @property {Array<number>} replicas The set of all nodes that host this partition.\n   * @property {Array<number>} isr The set of nodes that are in sync with the leader for this partition.\n   */\n  const fetchTopicMetadata = async ({ topics = [] } = {}) => {\n    if (topics) {\n      topics.forEach(topic => {\n        if (!topic || typeof topic !== 'string') {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n      })\n    }\n\n    const metadata = await cluster.metadata({ topics })\n\n    return {\n      topics: metadata.topicMetadata.map(topicMetadata => ({\n        name: topicMetadata.topic,\n        partitions: topicMetadata.partitionMetadata,\n      })),\n    }\n  }\n\n  /**\n   * Describe cluster\n   *\n   * @return {Promise<ClusterMetadata>}\n   *\n   * @typedef {Object} ClusterMetadata\n   * @property {Array<Broker>} brokers\n   * @property {Number} controller Current controller id. Returns null if unknown.\n   * @property {String} clusterId\n   *\n   * @typedef {Object} Broker\n   * @property {Number} nodeId\n   * @property {String} host\n   * @property {Number} port\n   */\n  const describeCluster = async () => {\n    const { brokers: nodes, clusterId, controllerId } = await cluster.metadata({ topics: [] })\n    const brokers = nodes.map(({ nodeId, host, port }) => ({\n      nodeId,\n      host,\n      port,\n    }))\n    const controller =\n      controllerId == null || controllerId === NO_CONTROLLER_ID ? null : controllerId\n\n    return {\n      brokers,\n      controller,\n      clusterId,\n    }\n  }\n\n  /**\n   * List groups in a broker\n   *\n   * @return {Promise<ListGroups>}\n   *\n   * @typedef {Object} ListGroups\n   * @property {Array<ListGroup>} groups\n   *\n   * @typedef {Object} ListGroup\n   * @property {string} groupId\n   * @property {string} protocolType\n   */\n  const listGroups = async () => {\n    await cluster.refreshMetadata()\n    let groups = []\n    for (var nodeId in cluster.brokerPool.brokers) {\n      const broker = await cluster.findBroker({ nodeId })\n      const response = await broker.listGroups()\n      groups = groups.concat(response.groups)\n    }\n\n    return { groups }\n  }\n\n  /**\n   * Describe groups by group ids\n   * @param {Array<string>} groupIds\n   *\n   * @typedef {Object} GroupDescriptions\n   * @property {Array<GroupDescription>} groups\n   *\n   * @return {Promise<GroupDescriptions>}\n   */\n  const describeGroups = async groupIds => {\n    const coordinatorsForGroup = await Promise.all(\n      groupIds.map(async groupId => {\n        const coordinator = await cluster.findGroupCoordinator({ groupId })\n        return {\n          coordinator,\n          groupId,\n        }\n      })\n    )\n\n    const groupsByCoordinator = Object.values(\n      coordinatorsForGroup.reduce((coordinators, { coordinator, groupId }) => {\n        const group = coordinators[coordinator.nodeId]\n\n        if (group) {\n          coordinators[coordinator.nodeId] = {\n            ...group,\n            groupIds: [...group.groupIds, groupId],\n          }\n        } else {\n          coordinators[coordinator.nodeId] = { coordinator, groupIds: [groupId] }\n        }\n        return coordinators\n      }, {})\n    )\n\n    const responses = await Promise.all(\n      groupsByCoordinator.map(async ({ coordinator, groupIds }) => {\n        const retrier = createRetry(retry)\n        const { groups } = await retrier(() => coordinator.describeGroups({ groupIds }))\n        return groups\n      })\n    )\n\n    const groups = [].concat.apply([], responses)\n\n    return { groups }\n  }\n\n  /**\n   * Delete groups in a broker\n   *\n   * @param {string[]} [groupIds]\n   * @return {Promise<DeleteGroups>}\n   *\n   * @typedef {Array} DeleteGroups\n   * @property {string} groupId\n   * @property {number} errorCode\n   */\n  const deleteGroups = async groupIds => {\n    if (!groupIds || !Array.isArray(groupIds)) {\n      throw new KafkaJSNonRetriableError(`Invalid groupIds array ${groupIds}`)\n    }\n\n    const invalidGroupId = groupIds.some(g => typeof g !== 'string')\n\n    if (invalidGroupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId name: ${JSON.stringify(invalidGroupId)}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    let results = []\n\n    let clonedGroupIds = groupIds.slice()\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        if (clonedGroupIds.length === 0) return []\n\n        await cluster.refreshMetadata()\n\n        const brokersPerGroups = {}\n        const brokersPerNode = {}\n        for (const groupId of clonedGroupIds) {\n          const broker = await cluster.findGroupCoordinator({ groupId })\n          if (brokersPerGroups[broker.nodeId] === undefined) brokersPerGroups[broker.nodeId] = []\n          brokersPerGroups[broker.nodeId].push(groupId)\n          brokersPerNode[broker.nodeId] = broker\n        }\n\n        const res = await Promise.all(\n          Object.keys(brokersPerNode).map(\n            async nodeId => await brokersPerNode[nodeId].deleteGroups(brokersPerGroups[nodeId])\n          )\n        )\n\n        const errors = flatten(\n          res.map(({ results }) =>\n            results.map(({ groupId, errorCode, error }) => {\n              return { groupId, errorCode, error }\n            })\n          )\n        ).filter(({ errorCode }) => errorCode !== 0)\n\n        clonedGroupIds = errors.map(({ groupId }) => groupId)\n\n        if (errors.length > 0) throw new KafkaJSDeleteGroupsError('Error in DeleteGroups', errors)\n\n        results = flatten(res.map(({ results }) => results))\n\n        return results\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER' || e.type === 'COORDINATOR_NOT_AVAILABLE') {\n          logger.warn('Could not delete groups', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Delete topic records up to the selected partition offsets\n   *\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const deleteTopicRecords = async ({ topic, partitions }) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic \"${topic}\"`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const partitionsByBroker = cluster.findLeaderForPartitions(\n      topic,\n      partitions.map(p => p.partition)\n    )\n\n    const partitionsFound = flatten(values(partitionsByBroker))\n    const topicOffsets = await fetchTopicOffsets(topic)\n\n    const leaderNotFoundErrors = []\n    partitions.forEach(({ partition, offset }) => {\n      // throw if no leader found for partition\n      if (!partitionsFound.includes(partition)) {\n        leaderNotFoundErrors.push({\n          partition,\n          offset,\n          error: new KafkaJSBrokerNotFound('Could not find the leader for the partition', {\n            retriable: false,\n          }),\n        })\n        return\n      }\n      const { low } = topicOffsets.find(p => p.partition === partition) || {\n        high: undefined,\n        low: undefined,\n      }\n      // warn in case of offset below low watermark\n      if (parseInt(offset) < parseInt(low) && parseInt(offset) !== -1) {\n        logger.warn(\n          'The requested offset is before the earliest offset maintained on the partition - no records will be deleted from this partition',\n          {\n            topic,\n            partition,\n            offset,\n          }\n        )\n      }\n    })\n\n    if (leaderNotFoundErrors.length > 0) {\n      throw new KafkaJSDeleteTopicRecordsError({ topic, partitions: leaderNotFoundErrors })\n    }\n\n    const seekEntriesByBroker = entries(partitionsByBroker).reduce(\n      (obj, [nodeId, nodePartitions]) => {\n        obj[nodeId] = {\n          topic,\n          partitions: partitions.filter(p => nodePartitions.includes(p.partition)),\n        }\n        return obj\n      },\n      {}\n    )\n\n    const retrier = createRetry(retry)\n    return retrier(async bail => {\n      try {\n        const partitionErrors = []\n\n        const brokerRequests = entries(seekEntriesByBroker).map(\n          ([nodeId, { topic, partitions }]) => async () => {\n            const broker = await cluster.findBroker({ nodeId })\n            await broker.deleteRecords({ topics: [{ topic, partitions }] })\n            // remove successful entry so it's ignored on retry\n            delete seekEntriesByBroker[nodeId]\n          }\n        )\n\n        await Promise.all(\n          brokerRequests.map(request =>\n            request().catch(e => {\n              if (e.name === 'KafkaJSDeleteTopicRecordsError') {\n                e.partitions.forEach(({ partition, offset, error }) => {\n                  partitionErrors.push({\n                    partition,\n                    offset,\n                    error,\n                  })\n                })\n              } else {\n                // then it's an unknown error, not from the broker response\n                throw e\n              }\n            })\n          )\n        )\n\n        if (partitionErrors.length > 0) {\n          throw new KafkaJSDeleteTopicRecordsError({\n            topic,\n            partitions: partitionErrors,\n          })\n        }\n      } catch (e) {\n        if (\n          e.retriable &&\n          e.partitions.some(\n            ({ error }) => staleMetadata(error) || error.name === 'KafkaJSMetadataNotLoaded'\n          )\n        ) {\n          await cluster.refreshMetadata()\n        }\n        throw e\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ACLEntry>} acl\n   * @return {Promise<void>}\n   *\n   * @typedef {Object} ACLEntry\n   */\n  const createAcls = async ({ acl }) => {\n    if (!acl || !Array.isArray(acl)) {\n      throw new KafkaJSNonRetriableError(`Invalid ACL array ${acl}`)\n    }\n    if (acl.length === 0) {\n      throw new KafkaJSNonRetriableError('Empty ACL array')\n    }\n\n    // Validate principal\n    if (acl.some(({ principal }) => typeof principal !== 'string')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL array, the principals have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (acl.some(({ host }) => typeof host !== 'string')) {\n      throw new KafkaJSNonRetriableError('Invalid ACL array, the hosts have to be a valid string')\n    }\n\n    // Validate resourceName\n    if (acl.some(({ resourceName }) => typeof resourceName !== 'string')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL array, the resourceNames have to be a valid string'\n      )\n    }\n\n    let invalidType\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    invalidType = acl.find(i => !validOperationTypes.includes(i.operation))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourcePatternTypes\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    invalidType = acl.find(i => !validResourcePatternTypes.includes(i.resourcePatternType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(\n          invalidType\n        )}`\n      )\n    }\n\n    // Validate permissionTypes\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    invalidType = acl.find(i => !validPermissionTypes.includes(i.permissionType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourceTypes\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    invalidType = acl.find(i => !validResourceTypes.includes(i.resourceType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createAcls({ acl })\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {ACLResourceTypes} resourceType The type of resource\n   * @param {string} resourceName The name of the resource\n   * @param {ACLResourcePatternTypes} resourcePatternType The resource pattern type filter\n   * @param {string} principal The principal name\n   * @param {string} host The hostname\n   * @param {ACLOperationTypes} operation The type of operation\n   * @param {ACLPermissionTypes} permissionType The type of permission\n   * @return {Promise<void>}\n   *\n   * @typedef {number} ACLResourceTypes\n   * @typedef {number} ACLResourcePatternTypes\n   * @typedef {number} ACLOperationTypes\n   * @typedef {number} ACLPermissionTypes\n   */\n  const describeAcls = async ({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) => {\n    // Validate principal\n    if (typeof principal !== 'string' && typeof principal !== 'undefined') {\n      throw new KafkaJSNonRetriableError(\n        'Invalid principal, the principal have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (typeof host !== 'string' && typeof host !== 'undefined') {\n      throw new KafkaJSNonRetriableError('Invalid host, the host have to be a valid string')\n    }\n\n    // Validate resourceName\n    if (typeof resourceName !== 'string' && typeof resourceName !== 'undefined') {\n      throw new KafkaJSNonRetriableError(\n        'Invalid resourceName, the resourceName have to be a valid string'\n      )\n    }\n\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    if (!validOperationTypes.includes(operation)) {\n      throw new KafkaJSNonRetriableError(`Invalid operation type ${operation}`)\n    }\n\n    // Validate resourcePatternType\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    if (!validResourcePatternTypes.includes(resourcePatternType)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern filter type ${resourcePatternType}`\n      )\n    }\n\n    // Validate permissionType\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    if (!validPermissionTypes.includes(permissionType)) {\n      throw new KafkaJSNonRetriableError(`Invalid permission type ${permissionType}`)\n    }\n\n    // Validate resourceType\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    if (!validResourceTypes.includes(resourceType)) {\n      throw new KafkaJSNonRetriableError(`Invalid resource type ${resourceType}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const { resources } = await broker.describeAcls({\n          resourceType,\n          resourceName,\n          resourcePatternType,\n          principal,\n          host,\n          operation,\n          permissionType,\n        })\n        return { resources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ACLFilter>} filters\n   * @return {Promise<void>}\n   *\n   * @typedef {Object} ACLFilter\n   */\n  const deleteAcls = async ({ filters }) => {\n    if (!filters || !Array.isArray(filters)) {\n      throw new KafkaJSNonRetriableError(`Invalid ACL Filter array ${filters}`)\n    }\n\n    if (filters.length === 0) {\n      throw new KafkaJSNonRetriableError('Empty ACL Filter array')\n    }\n\n    // Validate principal\n    if (\n      filters.some(\n        ({ principal }) => typeof principal !== 'string' && typeof principal !== 'undefined'\n      )\n    ) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the principals have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (filters.some(({ host }) => typeof host !== 'string' && typeof host !== 'undefined')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the hosts have to be a valid string'\n      )\n    }\n\n    // Validate resourceName\n    if (\n      filters.some(\n        ({ resourceName }) =>\n          typeof resourceName !== 'string' && typeof resourceName !== 'undefined'\n      )\n    ) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the resourceNames have to be a valid string'\n      )\n    }\n\n    let invalidType\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    invalidType = filters.find(i => !validOperationTypes.includes(i.operation))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourcePatternTypes\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    invalidType = filters.find(i => !validResourcePatternTypes.includes(i.resourcePatternType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(\n          invalidType\n        )}`\n      )\n    }\n\n    // Validate permissionTypes\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    invalidType = filters.find(i => !validPermissionTypes.includes(i.permissionType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourceTypes\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    invalidType = filters.find(i => !validResourceTypes.includes(i.resourceType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const { filterResponses } = await broker.deleteAcls({ filters })\n        return { filterResponses }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not delete ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /** @type {import(\"../../types\").Admin[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    listTopics,\n    createTopics,\n    deleteTopics,\n    createPartitions,\n    getTopicMetadata,\n    fetchTopicMetadata,\n    describeCluster,\n    events,\n    fetchOffsets,\n    fetchTopicOffsets,\n    fetchTopicOffsetsByTimestamp,\n    setOffsets,\n    resetOffsets,\n    describeConfigs,\n    alterConfigs,\n    on,\n    logger: getLogger,\n    listGroups,\n    describeGroups,\n    deleteGroups,\n    describeAcls,\n    deleteAcls,\n    createAcls,\n    deleteTopicRecords,\n  }\n}\n","const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst adminType = InstrumentationEventType('admin')\n\nconst events = {\n  CONNECT: adminType('connect'),\n  DISCONNECT: adminType('disconnect'),\n  REQUEST: adminType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: adminType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: adminType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n","const Long = require('../utils/long')\nconst Lock = require('../utils/lock')\nconst { Types: Compression } = require('../protocol/message/compression')\nconst { requests, lookup } = require('../protocol/requests')\nconst { KafkaJSNonRetriableError } = require('../errors')\nconst apiKeys = require('../protocol/requests/apiKeys')\nconst SASLAuthenticator = require('./saslAuthenticator')\nconst shuffle = require('../utils/shuffle')\nconst { ApiVersions: apiVersionsApiKey } = require('../protocol/requests/apiKeys')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst PRIVATE = {\n  SHOULD_REAUTHENTICATE: Symbol('private:Broker:shouldReauthenticate'),\n  SEND_REQUEST: Symbol('private:Broker:sendRequest'),\n  AUTHENTICATE: Symbol('private:Broker:authenticate'),\n}\n\n/** @type {import(\"../protocol/requests\").Lookup} */\nconst notInitializedLookup = () => {\n  throw new Error('Broker not connected')\n}\n\n/**\n * @param request - request from protocol\n * @returns {boolean}\n */\nconst isAuthenticatedRequest = request => {\n  return request.apiKey !== apiVersionsApiKey\n}\n\n/**\n * Each node in a Kafka cluster is called broker. This class contains\n * the high-level operations a node can perform.\n *\n * @type {import(\"../../types\").Broker}\n */\nmodule.exports = class Broker {\n  /**\n   * @param {Object} options\n   * @param {import(\"../network/connection\")} options.connection\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {number} [options.nodeId]\n   * @param {import(\"../../types\").ApiVersions} [options.versions=null] The object with all available versions and APIs\n   *                                 supported by this cluster. The output of broker#apiVersions\n   * @param {number} [options.authenticationTimeout=1000]\n   * @param {number} [options.reauthenticationThreshold=10000]\n   * @param {boolean} [options.allowAutoTopicCreation=true] If this and the broker config 'auto.create.topics.enable'\n   *                                                are true, topics that don't exist will be created when\n   *                                                fetching metadata.\n   * @param {boolean} [options.supportAuthenticationProtocol=null] If the server supports the SASLAuthenticate protocol\n   */\n  constructor({\n    connection,\n    logger,\n    nodeId = null,\n    versions = null,\n    authenticationTimeout = 1000,\n    reauthenticationThreshold = 10000,\n    allowAutoTopicCreation = true,\n    supportAuthenticationProtocol = null,\n  }) {\n    this.connection = connection\n    this.nodeId = nodeId\n    this.rootLogger = logger\n    this.logger = logger.namespace('Broker')\n    this.versions = versions\n    this.authenticationTimeout = authenticationTimeout\n    this.reauthenticationThreshold = reauthenticationThreshold\n    this.allowAutoTopicCreation = allowAutoTopicCreation\n    this.supportAuthenticationProtocol = supportAuthenticationProtocol\n\n    this.authenticatedAt = null\n    this.sessionLifetime = Long.ZERO\n\n    // The lock timeout has twice the connectionTimeout because the same timeout is used\n    // for the first apiVersions call\n    const lockTimeout = 2 * this.connection.connectionTimeout + this.authenticationTimeout\n    this.brokerAddress = `${this.connection.host}:${this.connection.port}`\n\n    this.lock = new Lock({\n      timeout: lockTimeout,\n      description: `connect to broker ${this.brokerAddress}`,\n    })\n\n    this.lookupRequest = notInitializedLookup\n\n    /**\n     * @private\n     * @returns {Promise}\n     */\n    this[PRIVATE.AUTHENTICATE] = sharedPromiseTo(async () => {\n      if (this.connection.sasl && !this.isAuthenticated()) {\n        const authenticator = new SASLAuthenticator(\n          this.connection,\n          this.rootLogger,\n          this.versions,\n          this.supportAuthenticationProtocol\n        )\n\n        await authenticator.authenticate()\n        this.authenticatedAt = process.hrtime()\n        this.sessionLifetime = Long.fromValue(authenticator.sessionLifetime)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {boolean}\n   */\n  isAuthenticated() {\n    return this.authenticatedAt != null && !this[PRIVATE.SHOULD_REAUTHENTICATE]()\n  }\n\n  /**\n   * @public\n   * @returns {boolean}\n   */\n  isConnected() {\n    const { connected, sasl } = this.connection\n    return sasl ? connected && this.isAuthenticated() : connected\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async connect() {\n    try {\n      await this.lock.acquire()\n      if (this.isConnected()) {\n        return\n      }\n\n      this.authenticatedAt = null\n      await this.connection.connect()\n\n      if (!this.versions) {\n        this.versions = await this.apiVersions()\n      }\n\n      this.lookupRequest = lookup(this.versions)\n\n      if (this.supportAuthenticationProtocol === null) {\n        try {\n          this.lookupRequest(apiKeys.SaslAuthenticate, requests.SaslAuthenticate)\n          this.supportAuthenticationProtocol = true\n        } catch (_) {\n          this.supportAuthenticationProtocol = false\n        }\n\n        this.logger.debug(`Verified support for SaslAuthenticate`, {\n          broker: this.brokerAddress,\n          supportAuthenticationProtocol: this.supportAuthenticationProtocol,\n        })\n      }\n\n      await this[PRIVATE.AUTHENTICATE]()\n    } finally {\n      await this.lock.release()\n    }\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.authenticatedAt = null\n    await this.connection.disconnect()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").ApiVersions>}\n   */\n  async apiVersions() {\n    let response\n    const availableVersions = requests.ApiVersions.versions\n      .map(Number)\n      .sort()\n      .reverse()\n\n    // Find the best version implemented by the server\n    for (const candidateVersion of availableVersions) {\n      try {\n        const apiVersions = requests.ApiVersions.protocol({ version: candidateVersion })\n        response = await this[PRIVATE.SEND_REQUEST]({\n          ...apiVersions(),\n          requestTimeout: this.connection.connectionTimeout,\n        })\n        break\n      } catch (e) {\n        if (e.type !== 'UNSUPPORTED_VERSION') {\n          throw e\n        }\n      }\n    }\n\n    if (!response) {\n      throw new KafkaJSNonRetriableError('API Versions not supported')\n    }\n\n    return response.apiVersions.reduce(\n      (obj, version) =>\n        Object.assign(obj, {\n          [version.apiKey]: {\n            minVersion: version.minVersion,\n            maxVersion: version.maxVersion,\n          },\n        }),\n      {}\n    )\n  }\n\n  /**\n   * @public\n   * @type {import(\"../../types\").Broker['metadata']}\n   * @param {string[]} [topics=[]] An array of topics to fetch metadata for.\n   *                            If no topics are specified fetch metadata for all topics\n   */\n  async metadata(topics = []) {\n    const metadata = this.lookupRequest(apiKeys.Metadata, requests.Metadata)\n    const shuffledTopics = shuffle(topics)\n    return await this[PRIVATE.SEND_REQUEST](\n      metadata({ topics: shuffledTopics, allowAutoTopicCreation: this.allowAutoTopicCreation })\n    )\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {Array} request.topicData An array of messages per topic and per partition, example:\n   *                          [\n   *                            {\n   *                              topic: 'test-topic-1',\n   *                              partitions: [\n   *                                {\n   *                                  partition: 0,\n   *                                  firstSequence: 0,\n   *                                  messages: [\n   *                                    { key: '1', value: 'A' },\n   *                                    { key: '2', value: 'B' },\n   *                                  ]\n   *                                },\n   *                                {\n   *                                  partition: 1,\n   *                                  firstSequence: 0,\n   *                                  messages: [\n   *                                    { key: '3', value: 'C' },\n   *                                  ]\n   *                                }\n   *                              ]\n   *                            },\n   *                            {\n   *                              topic: 'test-topic-2',\n   *                              partitions: [\n   *                                {\n   *                                  partition: 4,\n   *                                  firstSequence: 0,\n   *                                  messages: [\n   *                                    { key: '32', value: 'E' },\n   *                                  ]\n   *                                },\n   *                              ]\n   *                            },\n   *                          ]\n   * @param {number} [request.acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   * @param {number} [request.timeout=30000] The time to await a response in ms\n   * @param {string} [request.transactionalId=null]\n   * @param {number} [request.producerId=-1] Broker assigned producerId\n   * @param {number} [request.producerEpoch=0] Broker assigned producerEpoch\n   * @param {import(\"../../types\").CompressionTypes} [request.compression=CompressionTypes.None] Compression codec\n   * @returns {Promise}\n   */\n  async produce({\n    topicData,\n    transactionalId,\n    producerId,\n    producerEpoch,\n    acks = -1,\n    timeout = 30000,\n    compression = Compression.None,\n  }) {\n    const produce = this.lookupRequest(apiKeys.Produce, requests.Produce)\n    return await this[PRIVATE.SEND_REQUEST](\n      produce({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {number} [request.replicaId=-1] Broker id of the follower. For normal consumers, use -1\n   * @param {number} [request.isolationLevel=1] This setting controls the visibility of transactional records. Default READ_COMMITTED.\n   * @param {number} [request.maxWaitTime=5000] Maximum time in ms to wait for the response\n   * @param {number} [request.minBytes=1] Minimum bytes to accumulate in the response\n   * @param {number} [request.maxBytes=10485760] Maximum bytes to accumulate in the response. Note that this is\n   *                                   not an absolute maximum, if the first message in the first non-empty\n   *                                   partition of the fetch is larger than this value, the message will still\n   *                                   be returned to ensure that progress can be made. Default 10MB.\n   * @param {Array} request.topics Topics to fetch\n   *                        [\n   *                          {\n   *                            topic: 'topic-name',\n   *                            partitions: [\n   *                              {\n   *                                partition: 0,\n   *                                fetchOffset: '4124',\n   *                                maxBytes: 2048\n   *                              }\n   *                            ]\n   *                          }\n   *                        ]\n   * @param {string} [request.rackId=''] A rack identifier for this client. This can be any string value which indicates where this\n   *                           client is physically located. It corresponds with the broker config `broker.rack`.\n   * @returns {Promise}\n   */\n  async fetch({\n    replicaId,\n    isolationLevel,\n    maxWaitTime = 5000,\n    minBytes = 1,\n    maxBytes = 10485760,\n    topics,\n    rackId = '',\n  }) {\n    // TODO: validate topics not null/empty\n    const fetch = this.lookupRequest(apiKeys.Fetch, requests.Fetch)\n\n    // Shuffle topic-partitions to ensure fair response allocation across partitions (KIP-74)\n    const flattenedTopicPartitions = topics.reduce((topicPartitions, { topic, partitions }) => {\n      partitions.forEach(partition => {\n        topicPartitions.push({ topic, partition })\n      })\n      return topicPartitions\n    }, [])\n\n    const shuffledTopicPartitions = shuffle(flattenedTopicPartitions)\n\n    // Consecutive partitions for the same topic can be combined into a single `topic` entry\n    const consolidatedTopicPartitions = shuffledTopicPartitions.reduce(\n      (topicPartitions, { topic, partition }) => {\n        const last = topicPartitions[topicPartitions.length - 1]\n\n        if (last != null && last.topic === topic) {\n          topicPartitions[topicPartitions.length - 1].partitions.push(partition)\n        } else {\n          topicPartitions.push({ topic, partitions: [partition] })\n        }\n\n        return topicPartitions\n      },\n      []\n    )\n\n    return await this[PRIVATE.SEND_REQUEST](\n      fetch({\n        replicaId,\n        isolationLevel,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics: consolidatedTopicPartitions,\n        rackId,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The group id\n   * @param {number} request.groupGenerationId The generation of the group\n   * @param {string} request.memberId The member id assigned by the group coordinator\n   * @returns {Promise}\n   */\n  async heartbeat({ groupId, groupGenerationId, memberId }) {\n    const heartbeat = this.lookupRequest(apiKeys.Heartbeat, requests.Heartbeat)\n    return await this[PRIVATE.SEND_REQUEST](heartbeat({ groupId, groupGenerationId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The unique group id\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} request.coordinatorType The type of coordinator to find\n   * @returns {Promise}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType }) {\n    // TODO: validate groupId, mandatory\n    const findCoordinator = this.lookupRequest(apiKeys.GroupCoordinator, requests.GroupCoordinator)\n    return await this[PRIVATE.SEND_REQUEST](findCoordinator({ groupId, coordinatorType }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The unique group id\n   * @param {number} request.sessionTimeout The coordinator considers the consumer dead if it receives\n   *                                no heartbeat after this timeout in ms\n   * @param {number} request.rebalanceTimeout The maximum time that the coordinator will wait for each member\n   *                                  to rejoin when rebalancing the group\n   * @param {string} [request.memberId=\"\"] The assigned consumer id or an empty string for a new consumer\n   * @param {string} [request.protocolType=\"consumer\"] Unique name for class of protocols implemented by group\n   * @param {Array} request.groupProtocols List of protocols that the member supports (assignment strategy)\n   *                                [{ name: 'AssignerName', metadata: '{\"version\": 1, \"topics\": []}' }]\n   * @returns {Promise}\n   */\n  async joinGroup({\n    groupId,\n    sessionTimeout,\n    rebalanceTimeout,\n    memberId = '',\n    protocolType = 'consumer',\n    groupProtocols,\n  }) {\n    const joinGroup = this.lookupRequest(apiKeys.JoinGroup, requests.JoinGroup)\n    const makeRequest = (assignedMemberId = memberId) =>\n      this[PRIVATE.SEND_REQUEST](\n        joinGroup({\n          groupId,\n          sessionTimeout,\n          rebalanceTimeout,\n          memberId: assignedMemberId,\n          protocolType,\n          groupProtocols,\n        })\n      )\n\n    try {\n      return await makeRequest()\n    } catch (error) {\n      if (error.name === 'KafkaJSMemberIdRequired') {\n        return makeRequest(error.memberId)\n      }\n\n      throw error\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {string} request.memberId\n   * @returns {Promise}\n   */\n  async leaveGroup({ groupId, memberId }) {\n    const leaveGroup = this.lookupRequest(apiKeys.LeaveGroup, requests.LeaveGroup)\n    return await this[PRIVATE.SEND_REQUEST](leaveGroup({ groupId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {number} request.generationId\n   * @param {string} request.memberId\n   * @param {object} request.groupAssignment\n   * @returns {Promise}\n   */\n  async syncGroup({ groupId, generationId, memberId, groupAssignment }) {\n    const syncGroup = this.lookupRequest(apiKeys.SyncGroup, requests.SyncGroup)\n    return await this[PRIVATE.SEND_REQUEST](\n      syncGroup({\n        groupId,\n        generationId,\n        memberId,\n        groupAssignment,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {number} request.replicaId=-1 Broker id of the follower. For normal consumers, use -1\n   * @param {number} request.isolationLevel=1 This setting controls the visibility of transactional records (default READ_COMMITTED, Kafka >0.11 only)\n   * @param {TopicPartitionOffset[]} request.topics e.g:\n   *\n   * @typedef {Object} TopicPartitionOffset\n   * @property {string} topic\n   * @property {PartitionOffset[]} partitions\n   *\n   * @typedef {Object} PartitionOffset\n   * @property {number} partition\n   * @property {number} [timestamp=-1]\n   *\n   *\n   * @returns {Promise}\n   */\n  async listOffsets({ replicaId, isolationLevel, topics }) {\n    const listOffsets = this.lookupRequest(apiKeys.ListOffsets, requests.ListOffsets)\n    const result = await this[PRIVATE.SEND_REQUEST](\n      listOffsets({ replicaId, isolationLevel, topics })\n    )\n\n    // ListOffsets >= v1 will return a single `offset` rather than an array of `offsets` (ListOffsets V0).\n    // Normalize to just return `offset`.\n    for (const response of result.responses) {\n      response.partitions = response.partitions.map(({ offsets, ...partitionData }) => {\n        return offsets ? { ...partitionData, offset: offsets.pop() } : partitionData\n      })\n    }\n\n    return result\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {number} request.groupGenerationId\n   * @param {string} request.memberId\n   * @param {number} [request.retentionTime=-1] -1 signals to the broker that its default configuration\n   *                                    should be used.\n   * @param {object} request.topics Topics to commit offsets, e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0, offset: '11' }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetCommit({ groupId, groupGenerationId, memberId, retentionTime, topics }) {\n    const offsetCommit = this.lookupRequest(apiKeys.OffsetCommit, requests.OffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      offsetCommit({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {object} request.topics - If the topic array is null fetch offsets for all topics. e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0 }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetFetch({ groupId, topics }) {\n    const offsetFetch = this.lookupRequest(apiKeys.OffsetFetch, requests.OffsetFetch)\n    return await this[PRIVATE.SEND_REQUEST](offsetFetch({ groupId, topics }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.groupIds\n   * @returns {Promise}\n   */\n  async describeGroups({ groupIds }) {\n    const describeGroups = this.lookupRequest(apiKeys.DescribeGroups, requests.DescribeGroups)\n    return await this[PRIVATE.SEND_REQUEST](describeGroups({ groupIds }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.topics e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     numPartitions: 1,\n   *                     replicationFactor: 1\n   *                   }\n   *                 ]\n   * @param {boolean} [request.validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise}\n   */\n  async createTopics({ topics, validateOnly = false, timeout = 5000 }) {\n    const createTopics = this.lookupRequest(apiKeys.CreateTopics, requests.CreateTopics)\n    return await this[PRIVATE.SEND_REQUEST](createTopics({ topics, validateOnly, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.topicPartitions e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     count: 3,\n   *                     assignments: []\n   *                   }\n   *                 ]\n   * @param {boolean} [request.validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise<void>}\n   */\n  async createPartitions({ topicPartitions, validateOnly = false, timeout = 5000 }) {\n    const createPartitions = this.lookupRequest(apiKeys.CreatePartitions, requests.CreatePartitions)\n    return await this[PRIVATE.SEND_REQUEST](\n      createPartitions({ topicPartitions, validateOnly, timeout })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string[]} request.topics An array of topics to be deleted\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely deleted on the\n   *                                controller node. Values <= 0 will trigger topic deletion and return\n   *                                immediately\n   * @returns {Promise}\n   */\n  async deleteTopics({ topics, timeout = 5000 }) {\n    const deleteTopics = this.lookupRequest(apiKeys.DeleteTopics, requests.DeleteTopics)\n    return await this[PRIVATE.SEND_REQUEST](deleteTopics({ topics, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").ResourceConfigQuery[]} request.resources\n   *                                 [{\n   *                                   type: RESOURCE_TYPES.TOPIC,\n   *                                   name: 'topic-name',\n   *                                   configNames: ['compression.type', 'retention.ms']\n   *                                 }]\n   * @param {boolean} [request.includeSynonyms=false]\n   * @returns {Promise}\n   */\n  async describeConfigs({ resources, includeSynonyms = false }) {\n    const describeConfigs = this.lookupRequest(apiKeys.DescribeConfigs, requests.DescribeConfigs)\n    return await this[PRIVATE.SEND_REQUEST](describeConfigs({ resources, includeSynonyms }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").IResourceConfig[]} request.resources\n   *                                 [{\n   *                                  type: RESOURCE_TYPES.TOPIC,\n   *                                  name: 'topic-name',\n   *                                  configEntries: [\n   *                                    {\n   *                                      name: 'cleanup.policy',\n   *                                      value: 'compact'\n   *                                    }\n   *                                  ]\n   *                                 }]\n   * @param {boolean} [request.validateOnly=false]\n   * @returns {Promise}\n   */\n  async alterConfigs({ resources, validateOnly = false }) {\n    const alterConfigs = this.lookupRequest(apiKeys.AlterConfigs, requests.AlterConfigs)\n    return await this[PRIVATE.SEND_REQUEST](alterConfigs({ resources, validateOnly }))\n  }\n\n  /**\n   * Send an `InitProducerId` request to fetch a PID and bump the producer epoch.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {number} request.transactionTimeout The time in ms to wait for before aborting idle transactions\n   * @param {number} [request.transactionalId] The transactional id or null if the producer is not transactional\n   * @returns {Promise}\n   */\n  async initProducerId({ transactionalId, transactionTimeout }) {\n    const initProducerId = this.lookupRequest(apiKeys.InitProducerId, requests.InitProducerId)\n    return await this[PRIVATE.SEND_REQUEST](initProducerId({ transactionalId, transactionTimeout }))\n  }\n\n  /**\n   * Send an `AddPartitionsToTxn` request to mark a TopicPartition as participating in the transaction.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {object[]} request.topics e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [ 0, 1]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics }) {\n    const addPartitionsToTxn = this.lookupRequest(\n      apiKeys.AddPartitionsToTxn,\n      requests.AddPartitionsToTxn\n    )\n    return await this[PRIVATE.SEND_REQUEST](\n      addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `AddOffsetsToTxn` request.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {string} request.groupId The unique group identifier (for the consumer group)\n   * @returns {Promise}\n   */\n  async addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId }) {\n    const addOffsetsToTxn = this.lookupRequest(apiKeys.AddOffsetsToTxn, requests.AddOffsetsToTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId })\n    )\n  }\n\n  /**\n   * Send a `TxnOffsetCommit` request to persist the offsets in the `__consumer_offsets` topics.\n   *\n   * Request should be made to the consumer coordinator.\n   * @public\n   * @param {object} request\n   * @param {OffsetCommitTopic[]} request.topics\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {string} request.groupId The unique group identifier (for the consumer group)\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {OffsetCommitTopic[]} request.topics\n   *\n   * @typedef {Object} OffsetCommitTopic\n   * @property {string} topic\n   * @property {OffsetCommitTopicPartition[]} partitions\n   *\n   * @typedef {Object} OffsetCommitTopicPartition\n   * @property {number} partition\n   * @property {number} offset\n   * @property {string} [metadata]\n   *\n   * @returns {Promise}\n   */\n  async txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics }) {\n    const txnOffsetCommit = this.lookupRequest(apiKeys.TxnOffsetCommit, requests.TxnOffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `EndTxn` request to indicate transaction should be committed or aborted.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {boolean} request.transactionResult The result of the transaction (false = ABORT, true = COMMIT)\n   * @returns {Promise}\n   */\n  async endTxn({ transactionalId, producerId, producerEpoch, transactionResult }) {\n    const endTxn = this.lookupRequest(apiKeys.EndTxn, requests.EndTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      endTxn({ transactionalId, producerId, producerEpoch, transactionResult })\n    )\n  }\n\n  /**\n   * Send request for list of groups\n   * @public\n   * @returns {Promise}\n   */\n  async listGroups() {\n    const listGroups = this.lookupRequest(apiKeys.ListGroups, requests.ListGroups)\n    return await this[PRIVATE.SEND_REQUEST](listGroups())\n  }\n\n  /**\n   * Send request to delete groups\n   * @param {string[]} groupIds\n   * @public\n   * @returns {Promise}\n   */\n  async deleteGroups(groupIds) {\n    const deleteGroups = this.lookupRequest(apiKeys.DeleteGroups, requests.DeleteGroups)\n    return await this[PRIVATE.SEND_REQUEST](deleteGroups(groupIds))\n  }\n\n  /**\n   * Send request to delete records\n   * @public\n   * @param {object} request\n   * @param {TopicPartitionRecords[]} request.topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset 2 },\n   *                                { partition: 1, offset 4 },\n   *                              ],\n   *                            }\n   *                          ]\n   * @returns {Promise<Array>} example:\n   *                          {\n   *                            throttleTime: 0\n   *                           [\n   *                              {\n   *                                topic: 'my-topic-name',\n   *                                partitions: [\n   *                                 { partition: 0, lowWatermark: '2n', errorCode: 0 },\n   *                                 { partition: 1, lowWatermark: '4n', errorCode: 0 },\n   *                               ],\n   *                             },\n   *                           ]\n   *                          }\n   *\n   * @typedef {object} TopicPartitionRecords\n   * @property {string} topic\n   * @property {PartitionRecord[]} partitions\n   *\n   * @typedef {object} PartitionRecord\n   * @property {number} partition\n   * @property {number} offset\n   */\n  async deleteRecords({ topics }) {\n    const deleteRecords = this.lookupRequest(apiKeys.DeleteRecords, requests.DeleteRecords)\n    return await this[PRIVATE.SEND_REQUEST](deleteRecords({ topics }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").AclEntry[]} request.acl e.g:\n   *                 [\n   *                   {\n   *                     resourceType: AclResourceTypes.TOPIC,\n   *                     resourceName: 'topic-name',\n   *                     resourcePatternType: ResourcePatternTypes.LITERAL,\n   *                     principal: 'User:bob',\n   *                     host: '*',\n   *                     operation: AclOperationTypes.ALL,\n   *                     permissionType: AclPermissionTypes.DENY,\n   *                   }\n   *                 ]\n   * @returns {Promise<void>}\n   */\n  async createAcls({ acl }) {\n    const createAcls = this.lookupRequest(apiKeys.CreateAcls, requests.CreateAcls)\n    return await this[PRIVATE.SEND_REQUEST](createAcls({ creations: acl }))\n  }\n\n  /**\n   * @public\n   * @param {import(\"../../types\").AclEntry} aclEntry\n   * @returns {Promise<void>}\n   */\n  async describeAcls({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) {\n    const describeAcls = this.lookupRequest(apiKeys.DescribeAcls, requests.DescribeAcls)\n    return await this[PRIVATE.SEND_REQUEST](\n      describeAcls({\n        resourceType,\n        resourceName,\n        resourcePatternType,\n        principal,\n        host,\n        operation,\n        permissionType,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").AclEntry[]} request.filters\n   * @returns {Promise<void>}\n   */\n  async deleteAcls({ filters }) {\n    const deleteAcls = this.lookupRequest(apiKeys.DeleteAcls, requests.DeleteAcls)\n    return await this[PRIVATE.SEND_REQUEST](deleteAcls({ filters }))\n  }\n\n  /***\n   * @private\n   */\n  [PRIVATE.SHOULD_REAUTHENTICATE]() {\n    if (this.sessionLifetime.equals(Long.ZERO)) {\n      return false\n    }\n\n    if (this.authenticatedAt == null) {\n      return true\n    }\n\n    const [secondsSince, remainingNanosSince] = process.hrtime(this.authenticatedAt)\n    const millisSince = Long.fromValue(secondsSince)\n      .multiply(1000)\n      .add(Long.fromValue(remainingNanosSince).divide(1000000))\n\n    const reauthenticateAt = millisSince.add(this.reauthenticationThreshold)\n    return reauthenticateAt.greaterThanOrEqual(this.sessionLifetime)\n  }\n\n  /**\n   * @private\n   */\n  async [PRIVATE.SEND_REQUEST](protocolRequest) {\n    if (!this.isAuthenticated() && isAuthenticatedRequest(protocolRequest.request)) {\n      await this[PRIVATE.AUTHENTICATE]()\n    }\n    try {\n      return await this.connection.send(protocolRequest)\n    } catch (e) {\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        await this.disconnect()\n      }\n\n      throw e\n    }\n  }\n}\n","const awsIam = require('../../protocol/sasl/awsIam')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nmodule.exports = class AWSIAMAuthenticator {\n  constructor(connection, logger, saslAuthenticate) {\n    this.connection = connection\n    this.logger = logger.namespace('SASLAWSIAMAuthenticator')\n    this.saslAuthenticate = saslAuthenticate\n  }\n\n  async authenticate() {\n    const { sasl } = this.connection\n    if (!sasl.authorizationIdentity) {\n      throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing authorizationIdentity')\n    }\n    if (!sasl.accessKeyId) {\n      throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing accessKeyId')\n    }\n    if (!sasl.secretAccessKey) {\n      throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing secretAccessKey')\n    }\n    if (!sasl.sessionToken) {\n      sasl.sessionToken = ''\n    }\n\n    const request = awsIam.request(sasl)\n    const response = awsIam.response\n    const { host, port } = this.connection\n    const broker = `${host}:${port}`\n\n    try {\n      this.logger.debug('Authenticate with SASL AWS-IAM', { broker })\n      await this.saslAuthenticate({ request, response })\n      this.logger.debug('SASL AWS-IAM authentication successful', { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(\n        `SASL AWS-IAM authentication failed: ${e.message}`\n      )\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n}\n","const { requests, lookup } = require('../../protocol/requests')\nconst apiKeys = require('../../protocol/requests/apiKeys')\nconst PlainAuthenticator = require('./plain')\nconst SCRAM256Authenticator = require('./scram256')\nconst SCRAM512Authenticator = require('./scram512')\nconst AWSIAMAuthenticator = require('./awsIam')\nconst OAuthBearerAuthenticator = require('./oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst AUTHENTICATORS = {\n  PLAIN: PlainAuthenticator,\n  'SCRAM-SHA-256': SCRAM256Authenticator,\n  'SCRAM-SHA-512': SCRAM512Authenticator,\n  AWS: AWSIAMAuthenticator,\n  OAUTHBEARER: OAuthBearerAuthenticator,\n}\n\nconst SUPPORTED_MECHANISMS = Object.keys(AUTHENTICATORS)\nconst UNLIMITED_SESSION_LIFETIME = '0'\n\nmodule.exports = class SASLAuthenticator {\n  constructor(connection, logger, versions, supportAuthenticationProtocol) {\n    this.connection = connection\n    this.logger = logger\n    this.sessionLifetime = UNLIMITED_SESSION_LIFETIME\n\n    const lookupRequest = lookup(versions)\n    this.saslHandshake = lookupRequest(apiKeys.SaslHandshake, requests.SaslHandshake)\n    this.protocolAuthentication = supportAuthenticationProtocol\n      ? lookupRequest(apiKeys.SaslAuthenticate, requests.SaslAuthenticate)\n      : null\n  }\n\n  async authenticate() {\n    const mechanism = this.connection.sasl.mechanism.toUpperCase()\n    if (!SUPPORTED_MECHANISMS.includes(mechanism)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `SASL ${mechanism} mechanism is not supported by the client`\n      )\n    }\n\n    const handshake = await this.connection.send(this.saslHandshake({ mechanism }))\n    if (!handshake.enabledMechanisms.includes(mechanism)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `SASL ${mechanism} mechanism is not supported by the server`\n      )\n    }\n\n    const saslAuthenticate = async ({ request, response, authExpectResponse }) => {\n      if (this.protocolAuthentication) {\n        const { buffer: requestAuthBytes } = await request.encode()\n        const authResponse = await this.connection.send(\n          this.protocolAuthentication({ authBytes: requestAuthBytes })\n        )\n\n        // `0` is a string because `sessionLifetimeMs` is an int64 encoded as string.\n        // This is not present in SaslAuthenticateV0, so we default to `\"0\"`\n        this.sessionLifetime = authResponse.sessionLifetimeMs || UNLIMITED_SESSION_LIFETIME\n\n        if (!authExpectResponse) {\n          return\n        }\n\n        const { authBytes: responseAuthBytes } = authResponse\n        const payloadDecoded = await response.decode(responseAuthBytes)\n        return response.parse(payloadDecoded)\n      }\n\n      return this.connection.authenticate({ request, response, authExpectResponse })\n    }\n\n    const Authenticator = AUTHENTICATORS[mechanism]\n    await new Authenticator(this.connection, this.logger, saslAuthenticate).authenticate()\n  }\n}\n","/**\n * The sasl object must include a property named oauthBearerProvider, an\n * async function that is used to return the OAuth bearer token.\n *\n * The OAuth bearer token must be an object with properties value and\n * (optionally) extensions, that will be sent during the SASL/OAUTHBEARER\n * request.\n *\n * The implementation of the oauthBearerProvider must take care that tokens are\n * reused and refreshed when appropriate.\n */\n\nconst oauthBearer = require('../../protocol/sasl/oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nmodule.exports = class OAuthBearerAuthenticator {\n  constructor(connection, logger, saslAuthenticate) {\n    this.connection = connection\n    this.logger = logger.namespace('SASLOAuthBearerAuthenticator')\n    this.saslAuthenticate = saslAuthenticate\n  }\n\n  async authenticate() {\n    const { sasl } = this.connection\n    if (sasl.oauthBearerProvider == null) {\n      throw new KafkaJSSASLAuthenticationError(\n        'SASL OAUTHBEARER: Missing OAuth bearer token provider'\n      )\n    }\n\n    const { oauthBearerProvider } = sasl\n\n    const oauthBearerToken = await oauthBearerProvider()\n\n    if (oauthBearerToken.value == null) {\n      throw new KafkaJSSASLAuthenticationError('SASL OAUTHBEARER: Invalid OAuth bearer token')\n    }\n\n    const request = await oauthBearer.request(sasl, oauthBearerToken)\n    const response = oauthBearer.response\n    const { host, port } = this.connection\n    const broker = `${host}:${port}`\n\n    try {\n      this.logger.debug('Authenticate with SASL OAUTHBEARER', { broker })\n      await this.saslAuthenticate({ request, response })\n      this.logger.debug('SASL OAUTHBEARER authentication successful', { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(\n        `SASL OAUTHBEARER authentication failed: ${e.message}`\n      )\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n}\n","const plain = require('../../protocol/sasl/plain')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nmodule.exports = class PlainAuthenticator {\n  constructor(connection, logger, saslAuthenticate) {\n    this.connection = connection\n    this.logger = logger.namespace('SASLPlainAuthenticator')\n    this.saslAuthenticate = saslAuthenticate\n  }\n\n  async authenticate() {\n    const { sasl } = this.connection\n    if (sasl.username == null || sasl.password == null) {\n      throw new KafkaJSSASLAuthenticationError('SASL Plain: Invalid username or password')\n    }\n\n    const request = plain.request(sasl)\n    const response = plain.response\n    const { host, port } = this.connection\n    const broker = `${host}:${port}`\n\n    try {\n      this.logger.debug('Authenticate with SASL PLAIN', { broker })\n      await this.saslAuthenticate({ request, response })\n      this.logger.debug('SASL PLAIN authentication successful', { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(\n        `SASL PLAIN authentication failed: ${e.message}`\n      )\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n}\n","const crypto = require('crypto')\nconst scram = require('../../protocol/sasl/scram')\nconst { KafkaJSSASLAuthenticationError, KafkaJSNonRetriableError } = require('../../errors')\n\nconst GS2_HEADER = 'n,,'\n\nconst EQUAL_SIGN_REGEX = /=/g\nconst COMMA_SIGN_REGEX = /,/g\n\nconst URLSAFE_BASE64_PLUS_REGEX = /\\+/g\nconst URLSAFE_BASE64_SLASH_REGEX = /\\//g\nconst URLSAFE_BASE64_TRAILING_EQUAL_REGEX = /=+$/\n\nconst HMAC_CLIENT_KEY = 'Client Key'\nconst HMAC_SERVER_KEY = 'Server Key'\n\nconst DIGESTS = {\n  SHA256: {\n    length: 32,\n    type: 'sha256',\n    minIterations: 4096,\n  },\n  SHA512: {\n    length: 64,\n    type: 'sha512',\n    minIterations: 4096,\n  },\n}\n\nconst encode64 = str => Buffer.from(str).toString('base64')\n\nclass SCRAM {\n  /**\n   * From https://tools.ietf.org/html/rfc5802#section-5.1\n   *\n   * The characters ',' or '=' in usernames are sent as '=2C' and\n   * '=3D' respectively.  If the server receives a username that\n   * contains '=' not followed by either '2C' or '3D', then the\n   * server MUST fail the authentication.\n   *\n   * @returns {String}\n   */\n  static sanitizeString(str) {\n    return str.replace(EQUAL_SIGN_REGEX, '=3D').replace(COMMA_SIGN_REGEX, '=2C')\n  }\n\n  /**\n   * In cryptography, a nonce is an arbitrary number that can be used just once.\n   * It is similar in spirit to a nonce * word, hence the name. It is often a random or pseudo-random\n   * number issued in an authentication protocol to * ensure that old communications cannot be reused\n   * in replay attacks.\n   *\n   * @returns {String}\n   */\n  static nonce() {\n    return crypto\n      .randomBytes(16)\n      .toString('base64')\n      .replace(URLSAFE_BASE64_PLUS_REGEX, '-') // make it url safe\n      .replace(URLSAFE_BASE64_SLASH_REGEX, '_')\n      .replace(URLSAFE_BASE64_TRAILING_EQUAL_REGEX, '')\n      .toString('ascii')\n  }\n\n  /**\n   * Hi() is, essentially, PBKDF2 [RFC2898] with HMAC() as the\n   * pseudorandom function (PRF) and with dkLen == output length of\n   * HMAC() == output length of H()\n   *\n   * @returns {Promise<Buffer>}\n   */\n  static hi(password, salt, iterations, digestDefinition) {\n    return new Promise((resolve, reject) => {\n      crypto.pbkdf2(\n        password,\n        salt,\n        iterations,\n        digestDefinition.length,\n        digestDefinition.type,\n        (err, derivedKey) => (err ? reject(err) : resolve(derivedKey))\n      )\n    })\n  }\n\n  /**\n   * Apply the exclusive-or operation to combine the octet string\n   * on the left of this operator with the octet string on the right of\n   * this operator.  The length of the output and each of the two\n   * inputs will be the same for this use\n   *\n   * @returns {Buffer}\n   */\n  static xor(left, right) {\n    const bufferA = Buffer.from(left)\n    const bufferB = Buffer.from(right)\n    const length = Buffer.byteLength(bufferA)\n\n    if (length !== Buffer.byteLength(bufferB)) {\n      throw new KafkaJSNonRetriableError('Buffers must be of the same length')\n    }\n\n    const result = []\n    for (let i = 0; i < length; i++) {\n      result.push(bufferA[i] ^ bufferB[i])\n    }\n\n    return Buffer.from(result)\n  }\n\n  /**\n   * @param {Connection} connection\n   * @param {Logger} logger\n   * @param {Function} saslAuthenticate\n   * @param {DigestDefinition} digestDefinition\n   */\n  constructor(connection, logger, saslAuthenticate, digestDefinition) {\n    this.connection = connection\n    this.logger = logger\n    this.saslAuthenticate = saslAuthenticate\n    this.digestDefinition = digestDefinition\n\n    const digestType = digestDefinition.type.toUpperCase()\n    this.PREFIX = `SASL SCRAM ${digestType} authentication`\n\n    this.currentNonce = SCRAM.nonce()\n  }\n\n  async authenticate() {\n    const { PREFIX } = this\n    const { host, port, sasl } = this.connection\n    const broker = `${host}:${port}`\n\n    if (sasl.username == null || sasl.password == null) {\n      throw new KafkaJSSASLAuthenticationError(`${this.PREFIX}: Invalid username or password`)\n    }\n\n    try {\n      this.logger.debug('Exchanging first client message', { broker })\n      const clientMessageResponse = await this.sendClientFirstMessage()\n\n      this.logger.debug('Sending final message', { broker })\n      const finalResponse = await this.sendClientFinalMessage(clientMessageResponse)\n\n      if (finalResponse.e) {\n        throw new Error(finalResponse.e)\n      }\n\n      const serverKey = await this.serverKey(clientMessageResponse)\n      const serverSignature = this.serverSignature(serverKey, clientMessageResponse)\n\n      if (finalResponse.v !== serverSignature) {\n        throw new Error('Invalid server signature in server final message')\n      }\n\n      this.logger.debug(`${PREFIX} successful`, { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(`${PREFIX} failed: ${e.message}`)\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFirstMessage() {\n    const clientFirstMessage = `${GS2_HEADER}${this.firstMessageBare()}`\n    const request = scram.firstMessage.request({ clientFirstMessage })\n    const response = scram.firstMessage.response\n\n    return this.saslAuthenticate({\n      authExpectResponse: true,\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFinalMessage(clientMessageResponse) {\n    const { PREFIX } = this\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    const { minIterations } = this.digestDefinition\n\n    if (!clientMessageResponse.r.startsWith(this.currentNonce)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Invalid server nonce, it does not start with the client nonce`\n      )\n    }\n\n    if (iterations < minIterations) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Requested iterations ${iterations} is less than the minimum ${minIterations}`\n      )\n    }\n\n    const finalMessageWithoutProof = this.finalMessageWithoutProof(clientMessageResponse)\n    const clientProof = await this.clientProof(clientMessageResponse)\n    const finalMessage = `${finalMessageWithoutProof},p=${clientProof}`\n    const request = scram.finalMessage.request({ finalMessage })\n    const response = scram.finalMessage.response\n\n    return this.saslAuthenticate({\n      authExpectResponse: true,\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async clientProof(clientMessageResponse) {\n    const clientKey = await this.clientKey(clientMessageResponse)\n    const storedKey = this.H(clientKey)\n    const clientSignature = this.clientSignature(storedKey, clientMessageResponse)\n    return encode64(SCRAM.xor(clientKey, clientSignature))\n  }\n\n  /**\n   * @private\n   */\n  async clientKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_CLIENT_KEY)\n  }\n\n  /**\n   * @private\n   */\n  async serverKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_SERVER_KEY)\n  }\n\n  /**\n   * @private\n   */\n  clientSignature(storedKey, clientMessageResponse) {\n    return this.HMAC(storedKey, this.authMessage(clientMessageResponse))\n  }\n\n  /**\n   * @private\n   */\n  serverSignature(serverKey, clientMessageResponse) {\n    return encode64(this.HMAC(serverKey, this.authMessage(clientMessageResponse)))\n  }\n\n  /**\n   * @private\n   */\n  authMessage(clientMessageResponse) {\n    return [\n      this.firstMessageBare(),\n      clientMessageResponse.original,\n      this.finalMessageWithoutProof(clientMessageResponse),\n    ].join(',')\n  }\n\n  /**\n   * @private\n   */\n  async saltPassword(clientMessageResponse) {\n    const salt = Buffer.from(clientMessageResponse.s, 'base64')\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    return SCRAM.hi(this.encodedPassword(), salt, iterations, this.digestDefinition)\n  }\n\n  /**\n   * @private\n   */\n  firstMessageBare() {\n    return `n=${this.encodedUsername()},r=${this.currentNonce}`\n  }\n\n  /**\n   * @private\n   */\n  finalMessageWithoutProof(clientMessageResponse) {\n    const rnonce = clientMessageResponse.r\n    return `c=${encode64(GS2_HEADER)},r=${rnonce}`\n  }\n\n  /**\n   * @private\n   */\n  encodedUsername() {\n    const { username } = this.connection.sasl\n    return SCRAM.sanitizeString(username).toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  encodedPassword() {\n    const { password } = this.connection.sasl\n    return password.toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  H(data) {\n    return crypto\n      .createHash(this.digestDefinition.type)\n      .update(data)\n      .digest()\n  }\n\n  /**\n   * @private\n   */\n  HMAC(key, data) {\n    return crypto\n      .createHmac(this.digestDefinition.type, key)\n      .update(data)\n      .digest()\n  }\n}\n\nmodule.exports = {\n  DIGESTS,\n  SCRAM,\n}\n","const { SCRAM, DIGESTS } = require('./scram')\n\nmodule.exports = class SCRAM256Authenticator extends SCRAM {\n  constructor(connection, logger, saslAuthenticate) {\n    super(connection, logger.namespace('SCRAM256Authenticator'), saslAuthenticate, DIGESTS.SHA256)\n  }\n}\n","const { SCRAM, DIGESTS } = require('./scram')\n\nmodule.exports = class SCRAM512Authenticator extends SCRAM {\n  constructor(connection, logger, saslAuthenticate) {\n    super(connection, logger.namespace('SCRAM512Authenticator'), saslAuthenticate, DIGESTS.SHA512)\n  }\n}\n","const Broker = require('../broker')\nconst createRetry = require('../retry')\nconst shuffle = require('../utils/shuffle')\nconst arrayDiff = require('../utils/arrayDiff')\nconst { KafkaJSBrokerNotFound, KafkaJSProtocolError } = require('../errors')\n\nconst { keys, assign, values } = Object\nconst hasBrokerBeenReplaced = (broker, { host, port, rack }) =>\n  broker.connection.host !== host ||\n  broker.connection.port !== port ||\n  broker.connection.rack !== rack\n\nmodule.exports = class BrokerPool {\n  /**\n   * @param {object} options\n   * @param {import(\"./connectionBuilder\").ConnectionBuilder} options.connectionBuilder\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.allowAutoTopicCreation]\n   * @param {number} [options.authenticationTimeout]\n   * @param {number} [options.reauthenticationThreshold]\n   * @param {number} [options.metadataMaxAge]\n   */\n  constructor({\n    connectionBuilder,\n    logger,\n    retry,\n    allowAutoTopicCreation,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    metadataMaxAge,\n  }) {\n    this.rootLogger = logger\n    this.connectionBuilder = connectionBuilder\n    this.metadataMaxAge = metadataMaxAge || 0\n    this.logger = logger.namespace('BrokerPool')\n    this.retrier = createRetry(assign({}, retry))\n\n    this.createBroker = options =>\n      new Broker({\n        allowAutoTopicCreation,\n        authenticationTimeout,\n        reauthenticationThreshold,\n        ...options,\n      })\n\n    this.brokers = {}\n    /** @type {Broker | undefined} */\n    this.seedBroker = undefined\n    /** @type {import(\"../../types\").BrokerMetadata | null} */\n    this.metadata = null\n    this.metadataExpireAt = null\n    this.versions = null\n    this.supportAuthenticationProtocol = null\n  }\n\n  /**\n   * @public\n   * @returns {Boolean}\n   */\n  hasConnectedBrokers() {\n    const brokers = values(this.brokers)\n    return (\n      !!brokers.find(broker => broker.isConnected()) ||\n      (this.seedBroker ? this.seedBroker.isConnected() : false)\n    )\n  }\n\n  async createSeedBroker() {\n    if (this.seedBroker) {\n      await this.seedBroker.disconnect()\n    }\n\n    this.seedBroker = this.createBroker({\n      connection: await this.connectionBuilder.build(),\n      logger: this.rootLogger,\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async connect() {\n    if (this.hasConnectedBrokers()) {\n      return\n    }\n\n    if (!this.seedBroker) {\n      await this.createSeedBroker()\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.seedBroker.connect()\n        this.versions = this.seedBroker.versions\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          // Connection builder will always rotate the seed broker\n          await this.createSeedBroker()\n          this.logger.error(\n            `Failed to connect to seed broker, trying another broker from the list: ${e.message}`,\n            { retryCount, retryTime }\n          )\n        } else {\n          this.logger.error(e.message, { retryCount, retryTime })\n        }\n\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.seedBroker && (await this.seedBroker.disconnect())\n    await Promise.all(values(this.brokers).map(broker => broker.disconnect()))\n\n    this.brokers = {}\n    this.metadata = null\n    this.versions = null\n    this.supportAuthenticationProtocol = null\n  }\n\n  /**\n   * @public\n   * @param {Object} destination\n   * @param {string} destination.host\n   * @param {number} destination.port\n   */\n  removeBroker({ host, port }) {\n    const removedBroker = values(this.brokers).find(\n      broker => broker.connection.host === host && broker.connection.port === port\n    )\n\n    if (removedBroker) {\n      delete this.brokers[removedBroker.nodeId]\n      this.metadataExpireAt = null\n\n      if (this.seedBroker.nodeId === removedBroker.nodeId) {\n        this.seedBroker = shuffle(values(this.brokers))[0]\n      }\n    }\n  }\n\n  /**\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadata(topics) {\n    const broker = await this.findConnectedBroker()\n    const { host: seedHost, port: seedPort } = this.seedBroker.connection\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        this.metadata = await broker.metadata(topics)\n        this.metadataExpireAt = Date.now() + this.metadataMaxAge\n\n        const replacedBrokers = []\n\n        this.brokers = await this.metadata.brokers.reduce(\n          async (resultPromise, { nodeId, host, port, rack }) => {\n            const result = await resultPromise\n\n            if (result[nodeId]) {\n              if (!hasBrokerBeenReplaced(result[nodeId], { host, port, rack })) {\n                return result\n              }\n\n              replacedBrokers.push(result[nodeId])\n            }\n\n            if (host === seedHost && port === seedPort) {\n              this.seedBroker.nodeId = nodeId\n              this.seedBroker.connection.rack = rack\n              return assign(result, {\n                [nodeId]: this.seedBroker,\n              })\n            }\n\n            return assign(result, {\n              [nodeId]: this.createBroker({\n                logger: this.rootLogger,\n                versions: this.versions,\n                supportAuthenticationProtocol: this.supportAuthenticationProtocol,\n                connection: await this.connectionBuilder.build({ host, port, rack }),\n                nodeId,\n              }),\n            })\n          },\n          this.brokers\n        )\n\n        const freshBrokerIds = this.metadata.brokers.map(({ nodeId }) => `${nodeId}`).sort()\n        const currentBrokerIds = keys(this.brokers).sort()\n        const unusedBrokerIds = arrayDiff(currentBrokerIds, freshBrokerIds)\n\n        const brokerDisconnects = unusedBrokerIds.map(nodeId => {\n          const broker = this.brokers[nodeId]\n          return broker.disconnect().then(() => {\n            delete this.brokers[nodeId]\n          })\n        })\n\n        const replacedBrokersDisconnects = replacedBrokers.map(broker => broker.disconnect())\n        await Promise.all([...brokerDisconnects, ...replacedBrokersDisconnects])\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Only refreshes metadata if the data is stale according to the `metadataMaxAge` param or does not contain information about the provided topics\n   *\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadataIfNecessary(topics) {\n    const shouldRefresh =\n      this.metadata == null ||\n      this.metadataExpireAt == null ||\n      Date.now() > this.metadataExpireAt ||\n      !topics.every(topic =>\n        this.metadata.topicMetadata.some(topicMetadata => topicMetadata.topic === topic)\n      )\n\n    if (shouldRefresh) {\n      return this.refreshMetadata(topics)\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} options\n   * @param {string} options.nodeId\n   * @returns {Promise<Broker>}\n   */\n  async findBroker({ nodeId }) {\n    const broker = this.brokers[nodeId]\n\n    if (!broker) {\n      throw new KafkaJSBrokerNotFound(`Broker ${nodeId} not found in the cached metadata`)\n    }\n\n    await this.connectBroker(broker)\n    return broker\n  }\n\n  /**\n   * @public\n   * @param {(params: { nodeId: string, broker: Broker }) => Promise<T>} callback\n   * @returns {Promise<T>}\n   * @template T\n   */\n  async withBroker(callback) {\n    const brokers = shuffle(keys(this.brokers))\n    if (brokers.length === 0) {\n      throw new KafkaJSBrokerNotFound('No brokers in the broker pool')\n    }\n\n    for (const nodeId of brokers) {\n      const broker = await this.findBroker({ nodeId })\n      try {\n        return await callback({ nodeId, broker })\n      } catch (e) {}\n    }\n\n    return null\n  }\n\n  /**\n   * @public\n   * @returns {Promise<Broker>}\n   */\n  async findConnectedBroker() {\n    const nodeIds = shuffle(keys(this.brokers))\n    const connectedBrokerId = nodeIds.find(nodeId => this.brokers[nodeId].isConnected())\n\n    if (connectedBrokerId) {\n      return await this.findBroker({ nodeId: connectedBrokerId })\n    }\n\n    // Cycle through the nodes until one connects\n    for (const nodeId of nodeIds) {\n      try {\n        return await this.findBroker({ nodeId })\n      } catch (e) {}\n    }\n\n    // Failed to connect to all known brokers, metadata might be old\n    await this.connect()\n    return this.seedBroker\n  }\n\n  /**\n   * @private\n   * @param {Broker} broker\n   * @returns {Promise<null>}\n   */\n  async connectBroker(broker) {\n    if (broker.isConnected()) {\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await broker.connect()\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          await broker.disconnect()\n        }\n\n        // To avoid reconnecting to an unavailable host, we bail on connection errors\n        // and refresh metadata on a higher level before reconnecting\n        if (e.name === 'KafkaJSConnectionError') {\n          return bail(e)\n        }\n\n        if (e.type === 'ILLEGAL_SASL_STATE') {\n          // Rebuild the connection since it can't recover from illegal SASL state\n          broker.connection = await this.connectionBuilder.build({\n            host: broker.connection.host,\n            port: broker.connection.port,\n            rack: broker.connection.rack,\n          })\n\n          this.logger.error(`Failed to connect to broker, reconnecting`, { retryCount, retryTime })\n          throw new KafkaJSProtocolError(e, { retriable: true })\n        }\n\n        if (e.retriable) throw e\n        this.logger.error(e, { retryCount, retryTime, stack: e.stack })\n        bail(e)\n      }\n    })\n  }\n}\n","const Connection = require('../network/connection')\nconst { KafkaJSConnectionError, KafkaJSNonRetriableError } = require('../errors')\n\n/**\n * @typedef {Object} ConnectionBuilder\n * @property {(destination?: { host?: string, port?: number, rack?: string }) => Promise<Connection>} build\n */\n\n/**\n * @param {Object} options\n * @param {import(\"../../types\").ISocketFactory} [options.socketFactory]\n * @param {string[]|(() => string[])} options.brokers\n * @param {Object} [options.ssl]\n * @param {Object} [options.sasl]\n * @param {string} options.clientId\n * @param {number} options.requestTimeout\n * @param {boolean} [options.enforceRequestTimeout]\n * @param {number} [options.connectionTimeout]\n * @param {number} [options.maxInFlightRequests]\n * @param {import(\"../../types\").RetryOptions} [options.retry]\n * @param {import(\"../../types\").Logger} options.logger\n * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter]\n * @returns {ConnectionBuilder}\n */\nmodule.exports = ({\n  socketFactory,\n  brokers,\n  ssl,\n  sasl,\n  clientId,\n  requestTimeout,\n  enforceRequestTimeout,\n  connectionTimeout,\n  maxInFlightRequests,\n  logger,\n  instrumentationEmitter = null,\n}) => {\n  let index = 0\n\n  const isValidBroker = broker => {\n    return broker && typeof broker === 'string' && broker.length > 0\n  }\n\n  const validateBrokers = brokers => {\n    if (!brokers) {\n      throw new KafkaJSNonRetriableError(`Failed to connect: brokers should not be null`)\n    }\n\n    if (Array.isArray(brokers)) {\n      if (!brokers.length) {\n        throw new KafkaJSNonRetriableError(`Failed to connect: brokers array is empty`)\n      }\n\n      brokers.forEach((broker, index) => {\n        if (!isValidBroker(broker)) {\n          throw new KafkaJSNonRetriableError(\n            `Failed to connect: broker at index ${index} is invalid \"${typeof broker}\"`\n          )\n        }\n      })\n    }\n  }\n\n  const getBrokers = async () => {\n    let list\n\n    if (typeof brokers === 'function') {\n      try {\n        list = await brokers()\n      } catch (e) {\n        const wrappedError = new KafkaJSConnectionError(\n          `Failed to connect: \"config.brokers\" threw: ${e.message}`\n        )\n        wrappedError.stack = `${wrappedError.name}\\n  Caused by: ${e.stack}`\n        throw wrappedError\n      }\n    } else {\n      list = brokers\n    }\n\n    validateBrokers(list)\n\n    return list\n  }\n\n  return {\n    build: async ({ host, port, rack } = {}) => {\n      if (!host) {\n        const list = await getBrokers()\n\n        const randomBroker = list[index++ % list.length]\n\n        host = randomBroker.split(':')[0]\n        port = Number(randomBroker.split(':')[1])\n      }\n\n      return new Connection({\n        host,\n        port,\n        rack,\n        sasl,\n        ssl,\n        clientId,\n        socketFactory,\n        connectionTimeout,\n        requestTimeout,\n        enforceRequestTimeout,\n        maxInFlightRequests,\n        instrumentationEmitter,\n        logger,\n      })\n    },\n  }\n}\n","const BrokerPool = require('./brokerPool')\nconst Lock = require('../utils/lock')\nconst createRetry = require('../retry')\nconst connectionBuilder = require('./connectionBuilder')\nconst flatten = require('../utils/flatten')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst {\n  KafkaJSError,\n  KafkaJSBrokerNotFound,\n  KafkaJSMetadataNotLoaded,\n  KafkaJSTopicMetadataNotLoaded,\n  KafkaJSGroupCoordinatorNotFound,\n} = require('../errors')\nconst COORDINATOR_TYPES = require('../protocol/coordinatorTypes')\n\nconst { keys } = Object\n\nconst mergeTopics = (obj, { topic, partitions }) => ({\n  ...obj,\n  [topic]: [...(obj[topic] || []), ...partitions],\n})\n\nmodule.exports = class Cluster {\n  /**\n   * @param {Object} options\n   * @param {Array<string>} options.brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n   * @param {Object} options.ssl\n   * @param {Object} options.sasl\n   * @param {string} options.clientId\n   * @param {number} options.connectionTimeout - in milliseconds\n   * @param {number} options.authenticationTimeout - in milliseconds\n   * @param {number} options.reauthenticationThreshold - in milliseconds\n   * @param {number} [options.requestTimeout=30000] - in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {number} options.metadataMaxAge - in milliseconds\n   * @param {boolean} options.allowAutoTopicCreation\n   * @param {number} options.maxInFlightRequests\n   * @param {number} options.isolationLevel\n   * @param {import(\"../../types\").RetryOptions} options.retry\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n   * @param {Map} [options.offsets]\n   * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    logger: rootLogger,\n    socketFactory,\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout = 30000,\n    enforceRequestTimeout,\n    metadataMaxAge,\n    retry,\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    isolationLevel,\n    instrumentationEmitter = null,\n    offsets = new Map(),\n  }) {\n    this.rootLogger = rootLogger\n    this.logger = rootLogger.namespace('Cluster')\n    this.retrier = createRetry(retry)\n    this.connectionBuilder = connectionBuilder({\n      logger: rootLogger,\n      instrumentationEmitter,\n      socketFactory,\n      brokers,\n      ssl,\n      sasl,\n      clientId,\n      connectionTimeout,\n      requestTimeout,\n      enforceRequestTimeout,\n      maxInFlightRequests,\n    })\n\n    this.targetTopics = new Set()\n    this.mutatingTargetTopics = new Lock({\n      description: `updating target topics`,\n      timeout: requestTimeout,\n    })\n    this.isolationLevel = isolationLevel\n    this.brokerPool = new BrokerPool({\n      connectionBuilder: this.connectionBuilder,\n      logger: this.rootLogger,\n      retry,\n      allowAutoTopicCreation,\n      authenticationTimeout,\n      reauthenticationThreshold,\n      metadataMaxAge,\n    })\n    this.committedOffsetsByGroup = offsets\n  }\n\n  isConnected() {\n    return this.brokerPool.hasConnectedBrokers()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async connect() {\n    await this.brokerPool.connect()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async disconnect() {\n    await this.brokerPool.disconnect()\n  }\n\n  /**\n   * @public\n   * @param {object} destination\n   * @param {String} destination.host\n   * @param {Number} destination.port\n   */\n  removeBroker({ host, port }) {\n    this.brokerPool.removeBroker({ host, port })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async refreshMetadata() {\n    await this.brokerPool.refreshMetadata(Array.from(this.targetTopics))\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async refreshMetadataIfNecessary() {\n    await this.brokerPool.refreshMetadataIfNecessary(Array.from(this.targetTopics))\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").BrokerMetadata>}\n   */\n  async metadata({ topics = [] } = {}) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.brokerPool.refreshMetadataIfNecessary(topics)\n        return this.brokerPool.withBroker(async ({ broker }) => broker.metadata(topics))\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @return {Promise}\n   */\n  async addTargetTopic(topic) {\n    return this.addMultipleTargetTopics([topic])\n  }\n\n  /**\n   * @public\n   * @param {string[]} topics\n   * @return {Promise}\n   */\n  async addMultipleTargetTopics(topics) {\n    await this.mutatingTargetTopics.acquire()\n\n    try {\n      const previousSize = this.targetTopics.size\n      const previousTopics = new Set(this.targetTopics)\n      for (const topic of topics) {\n        this.targetTopics.add(topic)\n      }\n\n      const hasChanged = previousSize !== this.targetTopics.size || !this.brokerPool.metadata\n\n      if (hasChanged) {\n        try {\n          await this.refreshMetadata()\n        } catch (e) {\n          if (e.type === 'INVALID_TOPIC_EXCEPTION' || e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n            this.targetTopics = previousTopics\n          }\n\n          throw e\n        }\n      }\n    } finally {\n      await this.mutatingTargetTopics.release()\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} options\n   * @param {string} options.nodeId\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findBroker({ nodeId }) {\n    try {\n      return await this.brokerPool.findBroker({ nodeId })\n    } catch (e) {\n      // The client probably has stale metadata\n      if (\n        e.name === 'KafkaJSBrokerNotFound' ||\n        e.name === 'KafkaJSLockTimeout' ||\n        e.name === 'KafkaJSConnectionError'\n      ) {\n        await this.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findControllerBroker() {\n    const { metadata } = this.brokerPool\n\n    if (!metadata || metadata.controllerId == null) {\n      throw new KafkaJSMetadataNotLoaded('Topic metadata not loaded')\n    }\n\n    const broker = await this.findBroker({ nodeId: metadata.controllerId })\n\n    if (!broker) {\n      throw new KafkaJSBrokerNotFound(\n        `Controller broker with id ${metadata.controllerId} not found in the cached metadata`\n      )\n    }\n\n    return broker\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @returns {import(\"../../types\").PartitionMetadata[]} Example:\n   *                   [{\n   *                     isr: [2],\n   *                     leader: 2,\n   *                     partitionErrorCode: 0,\n   *                     partitionId: 0,\n   *                     replicas: [2],\n   *                   }]\n   */\n  findTopicPartitionMetadata(topic) {\n    const { metadata } = this.brokerPool\n    if (!metadata || !metadata.topicMetadata) {\n      throw new KafkaJSTopicMetadataNotLoaded('Topic metadata not loaded', { topic })\n    }\n\n    const topicMetadata = metadata.topicMetadata.find(t => t.topic === topic)\n    return topicMetadata ? topicMetadata.partitionMetadata : []\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @param {(number|string)[]} partitions\n   * @returns {Object} Object with leader and partitions. For partitions 0 and 5\n   *                   the result could be:\n   *                     { '0': [0], '2': [5] }\n   *\n   *                   where the key is the nodeId.\n   */\n  findLeaderForPartitions(topic, partitions) {\n    const partitionMetadata = this.findTopicPartitionMetadata(topic)\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader === null || metadata.leader === undefined) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      const { leader } = metadata\n      const current = result[leader] || []\n      return { ...result, [leader]: [...current, partitionId] }\n    }, {})\n  }\n\n  /**\n   * @public\n   * @param {object} params\n   * @param {string} params.groupId\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} [params.coordinatorType=0]\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        const { coordinator } = await this.findGroupCoordinatorMetadata({\n          groupId,\n          coordinatorType,\n        })\n        return await this.findBroker({ nodeId: coordinator.nodeId })\n      } catch (e) {\n        // A new broker can join the cluster before we have the chance\n        // to refresh metadata\n        if (e.name === 'KafkaJSBrokerNotFound' || e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n          this.logger.debug(`${e.message}, refreshing metadata and trying again...`, {\n            groupId,\n            retryCount,\n            retryTime,\n          })\n\n          await this.refreshMetadata()\n          throw e\n        }\n\n        if (e.code === 'ECONNREFUSED') {\n          // During maintenance the current coordinator can go down; findBroker will\n          // refresh metadata and re-throw the error. findGroupCoordinator has to re-throw\n          // the error to go through the retry cycle.\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} params\n   * @param {string} params.groupId\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} [params.coordinatorType=0]\n   * @returns {Promise<Object>}\n   */\n  async findGroupCoordinatorMetadata({ groupId, coordinatorType }) {\n    const brokerMetadata = await this.brokerPool.withBroker(async ({ nodeId, broker }) => {\n      return await this.retrier(async (bail, retryCount, retryTime) => {\n        try {\n          const brokerMetadata = await broker.findGroupCoordinator({ groupId, coordinatorType })\n          this.logger.debug('Found group coordinator', {\n            broker: brokerMetadata.host,\n            nodeId: brokerMetadata.coordinator.nodeId,\n          })\n          return brokerMetadata\n        } catch (e) {\n          this.logger.debug('Tried to find group coordinator', {\n            nodeId,\n            error: e,\n          })\n\n          if (e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n            this.logger.debug('Group coordinator not available, retrying...', {\n              nodeId,\n              retryCount,\n              retryTime,\n            })\n\n            throw e\n          }\n\n          bail(e)\n        }\n      })\n    })\n\n    if (brokerMetadata) {\n      return brokerMetadata\n    }\n\n    throw new KafkaJSGroupCoordinatorNotFound('Failed to find group coordinator')\n  }\n\n  /**\n   * @param {object} topicConfiguration\n   * @returns {number}\n   */\n  defaultOffset({ fromBeginning }) {\n    return fromBeginning ? EARLIEST_OFFSET : LATEST_OFFSET\n  }\n\n  /**\n   * @public\n   * @param {Array<Object>} topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [{ partition: 0 }],\n   *                              fromBeginning: false\n   *                            }\n   *                          ]\n   * @returns {Promise<import(\"../../types\").TopicOffsets[]>} example:\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset: '1' },\n   *                                { partition: 1, offset: '2' },\n   *                                { partition: 2, offset: '1' },\n   *                              ],\n   *                            },\n   *                          ]\n   */\n  async fetchTopicsOffset(topics) {\n    const partitionsPerBroker = {}\n    const topicConfigurations = {}\n\n    const addDefaultOffset = topic => partition => {\n      const { timestamp } = topicConfigurations[topic]\n      return { ...partition, timestamp }\n    }\n\n    // Index all topics and partitions per leader (nodeId)\n    for (const topicData of topics) {\n      const { topic, partitions, fromBeginning, fromTimestamp } = topicData\n      const partitionsPerLeader = this.findLeaderForPartitions(\n        topic,\n        partitions.map(p => p.partition)\n      )\n      const timestamp =\n        fromTimestamp != null ? fromTimestamp : this.defaultOffset({ fromBeginning })\n\n      topicConfigurations[topic] = { timestamp }\n\n      keys(partitionsPerLeader).forEach(nodeId => {\n        partitionsPerBroker[nodeId] = partitionsPerBroker[nodeId] || {}\n        partitionsPerBroker[nodeId][topic] = partitions.filter(p =>\n          partitionsPerLeader[nodeId].includes(p.partition)\n        )\n      })\n    }\n\n    // Create a list of requests to fetch the offset of all partitions\n    const requests = keys(partitionsPerBroker).map(async nodeId => {\n      const broker = await this.findBroker({ nodeId })\n      const partitions = partitionsPerBroker[nodeId]\n\n      const { responses: topicOffsets } = await broker.listOffsets({\n        isolationLevel: this.isolationLevel,\n        topics: keys(partitions).map(topic => ({\n          topic,\n          partitions: partitions[topic].map(addDefaultOffset(topic)),\n        })),\n      })\n\n      return topicOffsets\n    })\n\n    // Execute all requests, merge and normalize the responses\n    const responses = await Promise.all(requests)\n    const partitionsPerTopic = flatten(responses).reduce(mergeTopics, {})\n\n    return keys(partitionsPerTopic).map(topic => ({\n      topic,\n      partitions: partitionsPerTopic[topic].map(({ partition, offset }) => ({\n        partition,\n        offset,\n      })),\n    }))\n  }\n\n  /**\n   * Retrieve the object mapping for committed offsets for a single consumer group\n   * @param {object} options\n   * @param {string} options.groupId\n   * @returns {Object}\n   */\n  committedOffsets({ groupId }) {\n    if (!this.committedOffsetsByGroup.has(groupId)) {\n      this.committedOffsetsByGroup.set(groupId, {})\n    }\n\n    return this.committedOffsetsByGroup.get(groupId)\n  }\n\n  /**\n   * Mark offset as committed for a single consumer group's topic-partition\n   * @param {object} options\n   * @param {string} options.groupId\n   * @param {string} options.topic\n   * @param {string|number} options.partition\n   * @param {string} options.offset\n   */\n  markOffsetAsCommitted({ groupId, topic, partition, offset }) {\n    const committedOffsets = this.committedOffsets({ groupId })\n\n    committedOffsets[topic] = committedOffsets[topic] || {}\n    committedOffsets[topic][partition] = offset\n  }\n}\n","const EARLIEST_OFFSET = -2\nconst LATEST_OFFSET = -1\nconst INT_32_MAX_VALUE = Math.pow(2, 32)\n\nmodule.exports = {\n  EARLIEST_OFFSET,\n  LATEST_OFFSET,\n  INT_32_MAX_VALUE,\n}\n","const Encoder = require('../protocol/encoder')\nconst Decoder = require('../protocol/decoder')\n\nconst MemberMetadata = {\n  /**\n   * @param {Object} metadata\n   * @param {number} metadata.version\n   * @param {Array<string>} metadata.topics\n   * @param {Buffer} [metadata.userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, topics, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(topics)\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    return {\n      version: decoder.readInt16(),\n      topics: decoder.readArray(d => d.readString()),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nconst MemberAssignment = {\n  /**\n   * @param {number} version\n   * @param {Object<String,Array>} assignment, example:\n   *                               {\n   *                                 'topic-A': [0, 2, 4, 6],\n   *                                 'topic-B': [0, 2],\n   *                               }\n   * @param {Buffer} [userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, assignment, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(\n        Object.keys(assignment).map(topic =>\n          new Encoder().writeString(topic).writeArray(assignment[topic])\n        )\n      )\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object|null}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    const decodePartitions = d => d.readInt32()\n    const decodeAssignment = d => ({\n      topic: d.readString(),\n      partitions: d.readArray(decodePartitions),\n    })\n    const indexAssignment = (obj, { topic, partitions }) =>\n      Object.assign(obj, { [topic]: partitions })\n\n    if (!decoder.canReadInt16()) {\n      return null\n    }\n\n    return {\n      version: decoder.readInt16(),\n      assignment: decoder.readArray(decodeAssignment).reduce(indexAssignment, {}),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nmodule.exports = {\n  MemberMetadata,\n  MemberAssignment,\n}\n","const roundRobin = require('./roundRobinAssigner')\n\nmodule.exports = {\n  roundRobin,\n}\n","const { MemberMetadata, MemberAssignment } = require('../../assignerProtocol')\nconst flatten = require('../../../utils/flatten')\n\n/**\n * RoundRobinAssigner\n * @param {Cluster} cluster\n * @returns {function}\n */\nmodule.exports = ({ cluster }) => ({\n  name: 'RoundRobinAssigner',\n  version: 1,\n\n  /**\n   * Assign the topics to the provided members.\n   *\n   * The members array contains information about each member, `memberMetadata` is the result of the\n   * `protocol` operation.\n   *\n   * @param {array} members array of members, e.g:\n                              [{ memberId: 'test-5f93f5a3', memberMetadata: Buffer }]\n   * @param {array} topics\n   * @returns {array} object partitions per topic per member, e.g:\n   *                   [\n   *                     {\n   *                       memberId: 'test-5f93f5a3',\n   *                       memberAssignment: {\n   *                         'topic-A': [0, 2, 4, 6],\n   *                         'topic-B': [1],\n   *                       },\n   *                     },\n   *                     {\n   *                       memberId: 'test-3d3d5341',\n   *                       memberAssignment: {\n   *                         'topic-A': [1, 3, 5],\n   *                         'topic-B': [0, 2],\n   *                       },\n   *                     }\n   *                   ]\n   */\n  async assign({ members, topics }) {\n    const membersCount = members.length\n    const sortedMembers = members.map(({ memberId }) => memberId).sort()\n    const assignment = {}\n\n    const topicsPartionArrays = topics.map(topic => {\n      const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n      return partitionMetadata.map(m => ({ topic: topic, partitionId: m.partitionId }))\n    })\n    const topicsPartitions = flatten(topicsPartionArrays)\n\n    topicsPartitions.forEach((topicPartition, i) => {\n      const assignee = sortedMembers[i % membersCount]\n\n      if (!assignment[assignee]) {\n        assignment[assignee] = Object.create(null)\n      }\n\n      if (!assignment[assignee][topicPartition.topic]) {\n        assignment[assignee][topicPartition.topic] = []\n      }\n\n      assignment[assignee][topicPartition.topic].push(topicPartition.partitionId)\n    })\n\n    return Object.keys(assignment).map(memberId => ({\n      memberId,\n      memberAssignment: MemberAssignment.encode({\n        version: this.version,\n        assignment: assignment[memberId],\n      }),\n    }))\n  },\n\n  protocol({ topics }) {\n    return {\n      name: this.name,\n      metadata: MemberMetadata.encode({\n        version: this.version,\n        topics,\n      }),\n    }\n  },\n})\n","/**\n * @template T\n * @return {{lock: Promise<T>, unlock: (v?: T) => void, unlockWithError: (e: Error) => void}}\n */\nmodule.exports = () => {\n  let unlock\n  let unlockWithError\n  const lock = new Promise(resolve => {\n    unlock = resolve\n    unlockWithError = resolve\n  })\n\n  return { lock, unlock, unlockWithError }\n}\n","const Long = require('../utils/long')\nconst filterAbortedMessages = require('./filterAbortedMessages')\n\n/**\n * A batch collects messages returned from a single fetch call.\n *\n * A batch could contain _multiple_ Kafka RecordBatches.\n */\nmodule.exports = class Batch {\n  constructor(topic, fetchedOffset, partitionData) {\n    this.fetchedOffset = fetchedOffset\n    const longFetchedOffset = Long.fromValue(this.fetchedOffset)\n    const { abortedTransactions, messages } = partitionData\n\n    this.topic = topic\n    this.partition = partitionData.partition\n    this.highWatermark = partitionData.highWatermark\n\n    this.rawMessages = messages\n    // Apparently fetch can return different offsets than the target offset provided to the fetch API.\n    // Discard messages that are not in the requested offset\n    // https://github.com/apache/kafka/blob/bf237fa7c576bd141d78fdea9f17f65ea269c290/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L912\n    this.messagesWithinOffset = this.rawMessages.filter(message =>\n      Long.fromValue(message.offset).gte(longFetchedOffset)\n    )\n\n    // 1. Don't expose aborted messages\n    // 2. Don't expose control records\n    // @see https://kafka.apache.org/documentation/#controlbatch\n    this.messages = filterAbortedMessages({\n      messages: this.messagesWithinOffset,\n      abortedTransactions,\n    }).filter(message => !message.isControlRecord)\n  }\n\n  isEmpty() {\n    return this.messages.length === 0\n  }\n\n  isEmptyIncludingFiltered() {\n    return this.messagesWithinOffset.length === 0\n  }\n\n  /**\n   * If the batch contained raw messages (i.e was not truely empty) but all messages were filtered out due to\n   * log compaction, control records or other reasons\n   */\n  isEmptyDueToFiltering() {\n    return this.isEmpty() && this.rawMessages.length > 0\n  }\n\n  isEmptyControlRecord() {\n    return (\n      this.isEmpty() && this.messagesWithinOffset.some(({ isControlRecord }) => isControlRecord)\n    )\n  }\n\n  /**\n   * With compressed messages, it's possible for the returned messages to have offsets smaller than the starting offset.\n   * These messages will be filtered out (i.e. they are not even included in this.messagesWithinOffset)\n   * If these are the only messages, the batch will appear as an empty batch.\n   *\n   * isEmpty() and isEmptyIncludingFiltered() will always return true if the batch is empty,\n   * but this method will only return true if the batch is empty due to log compacted messages.\n   *\n   * @returns boolean True if the batch is empty, because of log compacted messages in the partition.\n   */\n  isEmptyDueToLogCompactedMessages() {\n    const hasMessages = this.rawMessages.length > 0\n    return hasMessages && this.isEmptyIncludingFiltered()\n  }\n\n  firstOffset() {\n    return this.isEmptyIncludingFiltered() ? null : this.messagesWithinOffset[0].offset\n  }\n\n  lastOffset() {\n    if (this.isEmptyDueToLogCompactedMessages()) {\n      return this.fetchedOffset\n    }\n\n    if (this.isEmptyIncludingFiltered()) {\n      return Long.fromValue(this.highWatermark)\n        .add(-1)\n        .toString()\n    }\n\n    return this.messagesWithinOffset[this.messagesWithinOffset.length - 1].offset\n  }\n\n  /**\n   * Returns the lag based on the last offset in the batch (also known as \"high\")\n   */\n  offsetLag() {\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const lastConsumedOffset = Long.fromValue(this.lastOffset())\n    return lastOffsetOfPartition.add(lastConsumedOffset.multiply(-1)).toString()\n  }\n\n  /**\n   * Returns the lag based on the first offset in the batch\n   */\n  offsetLagLow() {\n    if (this.isEmptyIncludingFiltered()) {\n      return '0'\n    }\n\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const firstConsumedOffset = Long.fromValue(this.firstOffset())\n    return lastOffsetOfPartition.add(firstConsumedOffset.multiply(-1)).toString()\n  }\n}\n","const flatten = require('../utils/flatten')\nconst sleep = require('../utils/sleep')\nconst BufferedAsyncIterator = require('../utils/bufferedAsyncIterator')\nconst websiteUrl = require('../utils/websiteUrl')\nconst arrayDiff = require('../utils/arrayDiff')\nconst createRetry = require('../retry')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst OffsetManager = require('./offsetManager')\nconst Batch = require('./batch')\nconst SeekOffsets = require('./seekOffsets')\nconst SubscriptionState = require('./subscriptionState')\nconst {\n  events: { GROUP_JOIN, HEARTBEAT, CONNECT, RECEIVED_UNSUBSCRIBED_TOPICS },\n} = require('./instrumentationEvents')\nconst { MemberAssignment } = require('./assignerProtocol')\nconst {\n  KafkaJSError,\n  KafkaJSNonRetriableError,\n  KafkaJSStaleTopicMetadataAssignment,\n} = require('../errors')\n\nconst { keys } = Object\n\nconst STALE_METADATA_ERRORS = [\n  'LEADER_NOT_AVAILABLE',\n  // Fetch before v9 uses NOT_LEADER_FOR_PARTITION\n  'NOT_LEADER_FOR_PARTITION',\n  // Fetch after v9 uses {FENCED,UNKNOWN}_LEADER_EPOCH\n  'FENCED_LEADER_EPOCH',\n  'UNKNOWN_LEADER_EPOCH',\n  'UNKNOWN_TOPIC_OR_PARTITION',\n]\n\nconst isRebalancing = e =>\n  e.type === 'REBALANCE_IN_PROGRESS' || e.type === 'NOT_COORDINATOR_FOR_GROUP'\n\nconst PRIVATE = {\n  JOIN: Symbol('private:ConsumerGroup:join'),\n  SYNC: Symbol('private:ConsumerGroup:sync'),\n  HEARTBEAT: Symbol('private:ConsumerGroup:heartbeat'),\n  SHAREDHEARTBEAT: Symbol('private:ConsumerGroup:sharedHeartbeat'),\n}\n\nmodule.exports = class ConsumerGroup {\n  constructor({\n    retry,\n    cluster,\n    groupId,\n    topics,\n    topicConfigurations,\n    logger,\n    instrumentationEmitter,\n    assigners,\n    sessionTimeout,\n    rebalanceTimeout,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    autoCommit,\n    autoCommitInterval,\n    autoCommitThreshold,\n    isolationLevel,\n    rackId,\n    metadataMaxAge,\n  }) {\n    /** @type {import(\"../../types\").Cluster} */\n    this.cluster = cluster\n    this.groupId = groupId\n    this.topics = topics\n    this.topicsSubscribed = topics\n    this.topicConfigurations = topicConfigurations\n    this.logger = logger.namespace('ConsumerGroup')\n    this.instrumentationEmitter = instrumentationEmitter\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.assigners = assigners\n    this.sessionTimeout = sessionTimeout\n    this.rebalanceTimeout = rebalanceTimeout\n    this.maxBytesPerPartition = maxBytesPerPartition\n    this.minBytes = minBytes\n    this.maxBytes = maxBytes\n    this.maxWaitTime = maxWaitTimeInMs\n    this.autoCommit = autoCommit\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.isolationLevel = isolationLevel\n    this.rackId = rackId\n    this.metadataMaxAge = metadataMaxAge\n\n    this.seekOffset = new SeekOffsets()\n    this.coordinator = null\n    this.generationId = null\n    this.leaderId = null\n    this.memberId = null\n    this.members = null\n    this.groupProtocol = null\n\n    this.partitionsPerSubscribedTopic = null\n    /**\n     * Preferred read replica per topic and partition\n     *\n     * Each of the partitions tracks the preferred read replica (`nodeId`) and a timestamp\n     * until when that preference is valid.\n     *\n     * @type {{[topicName: string]: {[partition: number]: {nodeId: number, expireAt: number}}}}\n     */\n    this.preferredReadReplicasPerTopicPartition = {}\n    this.offsetManager = null\n    this.subscriptionState = new SubscriptionState()\n\n    this.lastRequest = Date.now()\n\n    this[PRIVATE.SHAREDHEARTBEAT] = sharedPromiseTo(async ({ interval }) => {\n      const { groupId, generationId, memberId } = this\n      const now = Date.now()\n\n      if (memberId && now >= this.lastRequest + interval) {\n        const payload = {\n          groupId,\n          memberId,\n          groupGenerationId: generationId,\n        }\n\n        await this.coordinator.heartbeat(payload)\n        this.instrumentationEmitter.emit(HEARTBEAT, payload)\n        this.lastRequest = Date.now()\n      }\n    })\n  }\n\n  isLeader() {\n    return this.leaderId && this.memberId === this.leaderId\n  }\n\n  async connect() {\n    await this.cluster.connect()\n    this.instrumentationEmitter.emit(CONNECT)\n    await this.cluster.refreshMetadataIfNecessary()\n  }\n\n  async [PRIVATE.JOIN]() {\n    const { groupId, sessionTimeout, rebalanceTimeout } = this\n\n    this.coordinator = await this.cluster.findGroupCoordinator({ groupId })\n\n    const groupData = await this.coordinator.joinGroup({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId: this.memberId || '',\n      groupProtocols: this.assigners.map(assigner =>\n        assigner.protocol({\n          topics: this.topicsSubscribed,\n        })\n      ),\n    })\n\n    this.generationId = groupData.generationId\n    this.leaderId = groupData.leaderId\n    this.memberId = groupData.memberId\n    this.members = groupData.members\n    this.groupProtocol = groupData.groupProtocol\n  }\n\n  async leave() {\n    const { groupId, memberId } = this\n    if (memberId) {\n      await this.coordinator.leaveGroup({ groupId, memberId })\n      this.memberId = null\n    }\n  }\n\n  async [PRIVATE.SYNC]() {\n    let assignment = []\n    const {\n      groupId,\n      generationId,\n      memberId,\n      members,\n      groupProtocol,\n      topics,\n      topicsSubscribed,\n      coordinator,\n    } = this\n\n    if (this.isLeader()) {\n      this.logger.debug('Chosen as group leader', { groupId, generationId, memberId, topics })\n      const assigner = this.assigners.find(({ name }) => name === groupProtocol)\n\n      if (!assigner) {\n        throw new KafkaJSNonRetriableError(\n          `Unsupported partition assigner \"${groupProtocol}\", the assigner wasn't found in the assigners list`\n        )\n      }\n\n      await this.cluster.refreshMetadata()\n      assignment = await assigner.assign({ members, topics: topicsSubscribed })\n\n      this.logger.debug('Group assignment', {\n        groupId,\n        generationId,\n        groupProtocol,\n        assignment,\n        topics: topicsSubscribed,\n      })\n    }\n\n    // Keep track of the partitions for the subscribed topics\n    this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n    const { memberAssignment } = await this.coordinator.syncGroup({\n      groupId,\n      generationId,\n      memberId,\n      groupAssignment: assignment,\n    })\n\n    const decodedMemberAssignment = MemberAssignment.decode(memberAssignment)\n    const decodedAssignment =\n      decodedMemberAssignment != null ? decodedMemberAssignment.assignment : {}\n\n    this.logger.debug('Received assignment', {\n      groupId,\n      generationId,\n      memberId,\n      memberAssignment: decodedAssignment,\n    })\n\n    const assignedTopics = keys(decodedAssignment)\n    const topicsNotSubscribed = arrayDiff(assignedTopics, topicsSubscribed)\n\n    if (topicsNotSubscribed.length > 0) {\n      const payload = {\n        groupId,\n        generationId,\n        memberId,\n        assignedTopics,\n        topicsSubscribed,\n        topicsNotSubscribed,\n      }\n\n      this.instrumentationEmitter.emit(RECEIVED_UNSUBSCRIBED_TOPICS, payload)\n      this.logger.warn('Consumer group received unsubscribed topics', {\n        ...payload,\n        helpUrl: websiteUrl(\n          'docs/faq',\n          'why-am-i-receiving-messages-for-topics-i-m-not-subscribed-to'\n        ),\n      })\n    }\n\n    // Remove unsubscribed topics from the list\n    const safeAssignment = arrayDiff(assignedTopics, topicsNotSubscribed)\n    const currentMemberAssignment = safeAssignment.map(topic => ({\n      topic,\n      partitions: decodedAssignment[topic],\n    }))\n\n    // Check if the consumer is aware of all assigned partitions\n    for (const assignment of currentMemberAssignment) {\n      const { topic, partitions: assignedPartitions } = assignment\n      const knownPartitions = this.partitionsPerSubscribedTopic.get(topic)\n      const isAwareOfAllAssignedPartitions = assignedPartitions.every(partition =>\n        knownPartitions.includes(partition)\n      )\n\n      if (!isAwareOfAllAssignedPartitions) {\n        this.logger.warn('Consumer is not aware of all assigned partitions, refreshing metadata', {\n          groupId,\n          generationId,\n          memberId,\n          topic,\n          knownPartitions,\n          assignedPartitions,\n        })\n\n        // If the consumer is not aware of all assigned partitions, refresh metadata\n        // and update the list of partitions per subscribed topic. It's enough to perform\n        // this operation once since refresh metadata will update metadata for all topics\n        await this.cluster.refreshMetadata()\n        this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n        break\n      }\n    }\n\n    this.topics = currentMemberAssignment.map(({ topic }) => topic)\n    this.subscriptionState.assign(currentMemberAssignment)\n    this.offsetManager = new OffsetManager({\n      cluster: this.cluster,\n      topicConfigurations: this.topicConfigurations,\n      instrumentationEmitter: this.instrumentationEmitter,\n      memberAssignment: currentMemberAssignment.reduce(\n        (partitionsByTopic, { topic, partitions }) => ({\n          ...partitionsByTopic,\n          [topic]: partitions,\n        }),\n        {}\n      ),\n      autoCommit: this.autoCommit,\n      autoCommitInterval: this.autoCommitInterval,\n      autoCommitThreshold: this.autoCommitThreshold,\n      coordinator,\n      groupId,\n      generationId,\n      memberId,\n    })\n  }\n\n  joinAndSync() {\n    const startJoin = Date.now()\n    return this.retrier(async bail => {\n      try {\n        await this[PRIVATE.JOIN]()\n        await this[PRIVATE.SYNC]()\n\n        const memberAssignment = this.assigned().reduce(\n          (result, { topic, partitions }) => ({ ...result, [topic]: partitions }),\n          {}\n        )\n\n        const payload = {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          leaderId: this.leaderId,\n          isLeader: this.isLeader(),\n          memberAssignment,\n          groupProtocol: this.groupProtocol,\n          duration: Date.now() - startJoin,\n        }\n\n        this.instrumentationEmitter.emit(GROUP_JOIN, payload)\n        this.logger.info('Consumer has joined the group', payload)\n      } catch (e) {\n        if (isRebalancing(e)) {\n          // Rebalance in progress isn't a retriable protocol error since the consumer\n          // has to go through find coordinator and join again before it can\n          // actually retry the operation. We wrap the original error in a retriable error\n          // here instead in order to restart the join + sync sequence using the retrier.\n          throw new KafkaJSError(e)\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {import(\"../../types\").TopicPartition} topicPartition\n   */\n  resetOffset({ topic, partition }) {\n    this.offsetManager.resetOffset({ topic, partition })\n  }\n\n  /**\n   * @param {import(\"../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.offsetManager.resolveOffset({ topic, partition, offset })\n  }\n\n  /**\n   * Update the consumer offset for the given topic/partition. This will be used\n   * on the next fetch. If this API is invoked for the same topic/partition more\n   * than once, the latest offset will be used on the next fetch.\n   *\n   * @param {import(\"../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  seek({ topic, partition, offset }) {\n    this.seekOffset.set(topic, partition, offset)\n  }\n\n  pause(topicPartitions) {\n    this.logger.info(`Pausing fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.pause(topicPartitions)\n  }\n\n  resume(topicPartitions) {\n    this.logger.info(`Resuming fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.resume(topicPartitions)\n  }\n\n  assigned() {\n    return this.subscriptionState.assigned()\n  }\n\n  paused() {\n    return this.subscriptionState.paused()\n  }\n\n  async commitOffsetsIfNecessary() {\n    await this.offsetManager.commitOffsetsIfNecessary()\n  }\n\n  async commitOffsets(offsets) {\n    await this.offsetManager.commitOffsets(offsets)\n  }\n\n  uncommittedOffsets() {\n    return this.offsetManager.uncommittedOffsets()\n  }\n\n  async heartbeat({ interval }) {\n    return this[PRIVATE.SHAREDHEARTBEAT]({ interval })\n  }\n\n  async fetch() {\n    try {\n      const { topics, maxBytesPerPartition, maxWaitTime, minBytes, maxBytes } = this\n      /** @type {{[nodeId: string]: {topic: string, partitions: { partition: number; fetchOffset: string; maxBytes: number }[]}[]}} */\n      const requestsPerNode = {}\n\n      await this.cluster.refreshMetadataIfNecessary()\n      this.checkForStaleAssignment()\n\n      while (this.seekOffset.size > 0) {\n        const seekEntry = this.seekOffset.pop()\n        this.logger.debug('Seek offset', {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          seek: seekEntry,\n        })\n        await this.offsetManager.seek(seekEntry)\n      }\n\n      const pausedTopicPartitions = this.subscriptionState.paused()\n      const activeTopicPartitions = this.subscriptionState.active()\n\n      const activePartitions = flatten(activeTopicPartitions.map(({ partitions }) => partitions))\n      const activeTopics = activeTopicPartitions\n        .filter(({ partitions }) => partitions.length > 0)\n        .map(({ topic }) => topic)\n\n      if (activePartitions.length === 0) {\n        this.logger.debug(`No active topic partitions, sleeping for ${this.maxWaitTime}ms`, {\n          topics,\n          activeTopicPartitions,\n          pausedTopicPartitions,\n        })\n\n        await sleep(this.maxWaitTime)\n        return BufferedAsyncIterator([])\n      }\n\n      await this.offsetManager.resolveOffsets()\n\n      this.logger.debug(\n        `Fetching from ${activePartitions.length} partitions for ${activeTopics.length} out of ${topics.length} topics`,\n        {\n          topics,\n          activeTopicPartitions,\n          pausedTopicPartitions,\n        }\n      )\n\n      for (const topicPartition of activeTopicPartitions) {\n        const partitionsPerNode = this.findReadReplicaForPartitions(\n          topicPartition.topic,\n          topicPartition.partitions\n        )\n\n        const nodeIds = keys(partitionsPerNode)\n        const committedOffsets = this.offsetManager.committedOffsets()\n\n        for (const nodeId of nodeIds) {\n          const partitions = partitionsPerNode[nodeId]\n            .filter(partition => {\n              /**\n               * When recovering from OffsetOutOfRange, each partition can recover\n               * concurrently, which invalidates resolved and committed offsets as part\n               * of the recovery mechanism (see OffsetManager.clearOffsets). In concurrent\n               * scenarios this can initiate a new fetch with invalid offsets.\n               *\n               * This was further highlighted by https://github.com/tulios/kafkajs/pull/570,\n               * which increased concurrency, making this more likely to happen.\n               *\n               * This is solved by only making requests for partitions with initialized offsets.\n               *\n               * See the following pull request which explains the context of the problem:\n               * @issue https://github.com/tulios/kafkajs/pull/578\n               */\n              return committedOffsets[topicPartition.topic][partition] != null\n            })\n            .map(partition => ({\n              partition,\n              fetchOffset: this.offsetManager\n                .nextOffset(topicPartition.topic, partition)\n                .toString(),\n              maxBytes: maxBytesPerPartition,\n            }))\n\n          requestsPerNode[nodeId] = requestsPerNode[nodeId] || []\n          requestsPerNode[nodeId].push({ topic: topicPartition.topic, partitions })\n        }\n      }\n\n      const requests = keys(requestsPerNode).map(async nodeId => {\n        const broker = await this.cluster.findBroker({ nodeId })\n        const { responses } = await broker.fetch({\n          maxWaitTime,\n          minBytes,\n          maxBytes,\n          isolationLevel: this.isolationLevel,\n          topics: requestsPerNode[nodeId],\n          rackId: this.rackId,\n        })\n\n        const batchesPerPartition = responses.map(({ topicName, partitions }) => {\n          const topicRequestData = requestsPerNode[nodeId].find(({ topic }) => topic === topicName)\n          let preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topicName]\n          if (!preferredReadReplicas) {\n            this.preferredReadReplicasPerTopicPartition[topicName] = preferredReadReplicas = {}\n          }\n\n          return partitions\n            .filter(\n              partitionData =>\n                !this.seekOffset.has(topicName, partitionData.partition) &&\n                !this.subscriptionState.isPaused(topicName, partitionData.partition)\n            )\n            .map(partitionData => {\n              const { partition, preferredReadReplica } = partitionData\n              if (preferredReadReplica != null && preferredReadReplica !== -1) {\n                const { nodeId: currentPreferredReadReplica } =\n                  preferredReadReplicas[partition] || {}\n                if (currentPreferredReadReplica !== preferredReadReplica) {\n                  this.logger.info(`Preferred read replica is now ${preferredReadReplica}`, {\n                    groupId: this.groupId,\n                    memberId: this.memberId,\n                    topic: topicName,\n                    partition,\n                  })\n                }\n                preferredReadReplicas[partition] = {\n                  nodeId: preferredReadReplica,\n                  expireAt: Date.now() + this.metadataMaxAge,\n                }\n              }\n\n              const partitionRequestData = topicRequestData.partitions.find(\n                ({ partition }) => partition === partitionData.partition\n              )\n\n              const fetchedOffset = partitionRequestData.fetchOffset\n              return new Batch(topicName, fetchedOffset, partitionData)\n            })\n        })\n\n        return flatten(batchesPerPartition)\n      })\n\n      // fetch can generate empty requests when the consumer group receives an assignment\n      // with more topics than the subscribed, so to prevent a busy loop we wait the\n      // configured max wait time\n      if (requests.length === 0) {\n        await sleep(this.maxWaitTime)\n        return BufferedAsyncIterator([])\n      }\n\n      return BufferedAsyncIterator(requests, e => this.recoverFromFetch(e))\n    } catch (e) {\n      await this.recoverFromFetch(e)\n    }\n  }\n\n  async recoverFromFetch(e) {\n    if (STALE_METADATA_ERRORS.includes(e.type) || e.name === 'KafkaJSTopicMetadataNotLoaded') {\n      this.logger.debug('Stale cluster metadata, refreshing...', {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        error: e.message,\n      })\n\n      await this.cluster.refreshMetadata()\n      await this.joinAndSync()\n      throw new KafkaJSError(e.message)\n    }\n\n    if (e.name === 'KafkaJSStaleTopicMetadataAssignment') {\n      this.logger.warn(`${e.message}, resync group`, {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        topic: e.topic,\n        unknownPartitions: e.unknownPartitions,\n      })\n\n      await this.joinAndSync()\n    }\n\n    if (e.name === 'KafkaJSOffsetOutOfRange') {\n      await this.recoverFromOffsetOutOfRange(e)\n    }\n\n    if (e.name === 'KafkaJSConnectionClosedError') {\n      this.cluster.removeBroker({ host: e.host, port: e.port })\n    }\n\n    if (e.name === 'KafkaJSBrokerNotFound' || e.name === 'KafkaJSConnectionClosedError') {\n      this.logger.debug(`${e.message}, refreshing metadata and retrying...`)\n      await this.cluster.refreshMetadata()\n    }\n\n    throw e\n  }\n\n  async recoverFromOffsetOutOfRange(e) {\n    // If we are fetching from a follower try with the leader before resetting offsets\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[e.topic]\n    if (preferredReadReplicas && typeof preferredReadReplicas[e.partition] === 'number') {\n      this.logger.info('Offset out of range while fetching from follower, retrying with leader', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n      delete preferredReadReplicas[e.partition]\n    } else {\n      this.logger.error('Offset out of range, resetting to default offset', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n\n      await this.offsetManager.setDefaultOffset({\n        topic: e.topic,\n        partition: e.partition,\n      })\n    }\n  }\n\n  generatePartitionsPerSubscribedTopic() {\n    const map = new Map()\n\n    for (const topic of this.topicsSubscribed) {\n      const partitions = this.cluster\n        .findTopicPartitionMetadata(topic)\n        .map(m => m.partitionId)\n        .sort()\n\n      map.set(topic, partitions)\n    }\n\n    return map\n  }\n\n  checkForStaleAssignment() {\n    if (!this.partitionsPerSubscribedTopic) {\n      return\n    }\n\n    const newPartitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n\n    for (const [topic, partitions] of newPartitionsPerSubscribedTopic) {\n      const diff = arrayDiff(partitions, this.partitionsPerSubscribedTopic.get(topic))\n\n      if (diff.length > 0) {\n        throw new KafkaJSStaleTopicMetadataAssignment('Topic has been updated', {\n          topic,\n          unknownPartitions: diff,\n        })\n      }\n    }\n  }\n\n  hasSeekOffset({ topic, partition }) {\n    return this.seekOffset.has(topic, partition)\n  }\n\n  /**\n   * For each of the partitions find the best nodeId to read it from\n   *\n   * @param {string} topic\n   * @param {number[]} partitions\n   * @returns {{[nodeId: number]: number[]}} per-node assignment of partitions\n   * @see Cluster~findLeaderForPartitions\n   */\n  // Invariant: The resulting object has each partition referenced exactly once\n  findReadReplicaForPartitions(topic, partitions) {\n    const partitionMetadata = this.cluster.findTopicPartitionMetadata(topic)\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topic]\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader == null) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      // Pick the preferred replica if there is one, and it isn't known to be offline, otherwise the leader.\n      let nodeId = metadata.leader\n      if (preferredReadReplicas) {\n        const { nodeId: preferredReadReplica, expireAt } = preferredReadReplicas[partitionId] || {}\n        if (Date.now() >= expireAt) {\n          this.logger.debug('Preferred read replica information has expired, using leader', {\n            topic,\n            partitionId,\n            groupId: this.groupId,\n            memberId: this.memberId,\n            preferredReadReplica,\n            leader: metadata.leader,\n          })\n          // Drop the entry\n          delete preferredReadReplicas[partitionId]\n        } else if (preferredReadReplica != null) {\n          // Valid entry, check whether it is not offline\n          // Note that we don't delete the preference here, and rather hope that eventually that replica comes online again\n          const offlineReplicas = metadata.offlineReplicas\n          if (Array.isArray(offlineReplicas) && offlineReplicas.includes(nodeId)) {\n            this.logger.debug('Preferred read replica is offline, using leader', {\n              topic,\n              partitionId,\n              groupId: this.groupId,\n              memberId: this.memberId,\n              preferredReadReplica,\n              leader: metadata.leader,\n            })\n          } else {\n            nodeId = preferredReadReplica\n          }\n        }\n      }\n      const current = result[nodeId] || []\n      return { ...result, [nodeId]: [...current, partitionId] }\n    }, {})\n  }\n}\n","const Long = require('../utils/long')\nconst ABORTED_MESSAGE_KEY = Buffer.from([0, 0, 0, 0])\n\nconst isAbortMarker = ({ key }) => {\n  // Handle null/undefined keys.\n  if (!key) return false\n  // Cast key to buffer defensively\n  return Buffer.from(key).equals(ABORTED_MESSAGE_KEY)\n}\n\n/**\n * Remove messages marked as aborted according to the aborted transactions list.\n *\n * Start of an aborted transaction is determined by message offset.\n * End of an aborted transaction is determined by control messages.\n * @param {Message[]} messages\n * @param {Transaction[]} [abortedTransactions]\n * @returns {Message[]} Messages which did not participate in an aborted transaction\n *\n * @typedef {object} Message\n * @param {Buffer} key\n * @param {lastOffset} key  Int64\n * @param {RecordBatch}  batchContext\n *\n * @typedef {object} Transaction\n * @param {string} firstOffset  Int64\n * @param {string} producerId  Int64\n *\n * @typedef {object} RecordBatch\n * @param {string}  producerId  Int64\n * @param {boolean}  inTransaction\n */\nmodule.exports = ({ messages, abortedTransactions }) => {\n  const currentAbortedTransactions = new Map()\n\n  if (!abortedTransactions || !abortedTransactions.length) {\n    return messages\n  }\n\n  const remainingAbortedTransactions = [...abortedTransactions]\n\n  return messages.filter(message => {\n    // If the message offset is GTE the first offset of the next aborted transaction\n    // then we have stepped into an aborted transaction.\n    if (\n      remainingAbortedTransactions.length &&\n      Long.fromValue(message.offset).gte(remainingAbortedTransactions[0].firstOffset)\n    ) {\n      const { producerId } = remainingAbortedTransactions.shift()\n      currentAbortedTransactions.set(producerId, true)\n    }\n\n    const { producerId, inTransaction } = message.batchContext\n\n    if (isAbortMarker(message)) {\n      // Transaction is over, we no longer need to ignore messages from this producer\n      currentAbortedTransactions.delete(producerId)\n    } else if (currentAbortedTransactions.has(producerId) && inTransaction) {\n      return false\n    }\n\n    return true\n  })\n}\n","const Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst { initialRetryTime } = require('../retry/defaults')\nconst ConsumerGroup = require('./consumerGroup')\nconst Runner = require('./runner')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { KafkaJSNonRetriableError } = require('../errors')\nconst { roundRobin } = require('./assigners')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst ISOLATION_LEVEL = require('../protocol/isolationLevel')\n\nconst { keys, values } = Object\nconst { CONNECT, DISCONNECT, STOP, CRASH } = events\n\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `consumer.events.${key}`)\n  .join(', ')\n\nconst specialOffsets = [\n  Long.fromValue(EARLIEST_OFFSET).toString(),\n  Long.fromValue(LATEST_OFFSET).toString(),\n]\n\n/**\n * @param {Object} params\n * @param {import(\"../../types\").Cluster} params.cluster\n * @param {String} params.groupId\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').PartitionAssigner[]} [params.partitionAssigners]\n * @param {number} [params.sessionTimeout]\n * @param {number} [params.rebalanceTimeout]\n * @param {number} [params.heartbeatInterval]\n * @param {number} [params.maxBytesPerPartition]\n * @param {number} [params.minBytes]\n * @param {number} [params.maxBytes]\n * @param {number} [params.maxWaitTimeInMs]\n * @param {number} [params.isolationLevel]\n * @param {string} [params.rackId]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {number} params.metadataMaxAge\n *\n * @returns {import(\"../../types\").Consumer}\n */\nmodule.exports = ({\n  cluster,\n  groupId,\n  retry,\n  logger: rootLogger,\n  partitionAssigners = [roundRobin],\n  sessionTimeout = 30000,\n  rebalanceTimeout = 60000,\n  heartbeatInterval = 3000,\n  maxBytesPerPartition = 1048576, // 1MB\n  minBytes = 1,\n  maxBytes = 10485760, // 10MB\n  maxWaitTimeInMs = 5000,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  rackId = '',\n  instrumentationEmitter: rootInstrumentationEmitter,\n  metadataMaxAge,\n}) => {\n  if (!groupId) {\n    throw new KafkaJSNonRetriableError('Consumer groupId must be a non-empty string.')\n  }\n\n  const logger = rootLogger.namespace('Consumer')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const assigners = partitionAssigners.map(createAssigner =>\n    createAssigner({ groupId, logger, cluster })\n  )\n\n  const topics = {}\n  let runner = null\n  let consumerGroup = null\n\n  if (heartbeatInterval >= sessionTimeout) {\n    throw new KafkaJSNonRetriableError(\n      `Consumer heartbeatInterval (${heartbeatInterval}) must be lower than sessionTimeout (${sessionTimeout}). It is recommended to set heartbeatInterval to approximately a third of the sessionTimeout.`\n    )\n  }\n\n  const createConsumerGroup = ({ autoCommit, autoCommitInterval, autoCommitThreshold }) => {\n    return new ConsumerGroup({\n      logger: rootLogger,\n      topics: keys(topics),\n      topicConfigurations: topics,\n      retry,\n      cluster,\n      groupId,\n      assigners,\n      sessionTimeout,\n      rebalanceTimeout,\n      maxBytesPerPartition,\n      minBytes,\n      maxBytes,\n      maxWaitTimeInMs,\n      instrumentationEmitter,\n      autoCommit,\n      autoCommitInterval,\n      autoCommitThreshold,\n      isolationLevel,\n      rackId,\n      metadataMaxAge,\n    })\n  }\n\n  const createRunner = ({\n    eachBatchAutoResolve,\n    eachBatch,\n    eachMessage,\n    onCrash,\n    autoCommit,\n    partitionsConsumedConcurrently,\n  }) => {\n    return new Runner({\n      autoCommit,\n      logger: rootLogger,\n      consumerGroup,\n      instrumentationEmitter,\n      eachBatchAutoResolve,\n      eachBatch,\n      eachMessage,\n      heartbeatInterval,\n      retry,\n      onCrash,\n      partitionsConsumedConcurrently,\n    })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"connect\"]} */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"disconnect\"]} */\n  const disconnect = async () => {\n    try {\n      await stop()\n      logger.debug('consumer has stopped, disconnecting', { groupId })\n      await cluster.disconnect()\n      instrumentationEmitter.emit(DISCONNECT)\n    } catch (e) {\n      logger.error(`Caught error when disconnecting the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n      throw e\n    }\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"stop\"]} */\n  const stop = async () => {\n    try {\n      if (runner) {\n        await runner.stop()\n        runner = null\n        consumerGroup = null\n        instrumentationEmitter.emit(STOP)\n      }\n\n      logger.info('Stopped', { groupId })\n    } catch (e) {\n      logger.error(`Caught error when stopping the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n\n      throw e\n    }\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"subscribe\"]} */\n  const subscribe = async ({ topic, fromBeginning = false }) => {\n    if (consumerGroup) {\n      throw new KafkaJSNonRetriableError('Cannot subscribe to topic while consumer is running')\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const isRegExp = topic instanceof RegExp\n    if (typeof topic !== 'string' && !isRegExp) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid topic ${topic} (${typeof topic}), the topic name has to be a String or a RegExp`\n      )\n    }\n\n    const topicsToSubscribe = []\n    if (isRegExp) {\n      const topicRegExp = topic\n      const metadata = await cluster.metadata()\n      const matchedTopics = metadata.topicMetadata\n        .map(({ topic: topicName }) => topicName)\n        .filter(topicName => topicRegExp.test(topicName))\n\n      logger.debug('Subscription based on RegExp', {\n        groupId,\n        topicRegExp: topicRegExp.toString(),\n        matchedTopics,\n      })\n\n      topicsToSubscribe.push(...matchedTopics)\n    } else {\n      topicsToSubscribe.push(topic)\n    }\n\n    for (const t of topicsToSubscribe) {\n      topics[t] = { fromBeginning }\n    }\n\n    await cluster.addMultipleTargetTopics(topicsToSubscribe)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"run\"]} */\n  const run = async ({\n    autoCommit = true,\n    autoCommitInterval = null,\n    autoCommitThreshold = null,\n    eachBatchAutoResolve = true,\n    partitionsConsumedConcurrently = 1,\n    eachBatch = null,\n    eachMessage = null,\n  } = {}) => {\n    if (consumerGroup) {\n      logger.warn('consumer#run was called, but the consumer is already running', { groupId })\n      return\n    }\n\n    consumerGroup = createConsumerGroup({\n      autoCommit,\n      autoCommitInterval,\n      autoCommitThreshold,\n    })\n\n    const start = async onCrash => {\n      logger.info('Starting', { groupId })\n      runner = createRunner({\n        autoCommit,\n        eachBatchAutoResolve,\n        eachBatch,\n        eachMessage,\n        onCrash,\n        partitionsConsumedConcurrently,\n      })\n\n      await runner.start()\n    }\n\n    const restart = onCrash => {\n      consumerGroup = createConsumerGroup({\n        autoCommitInterval,\n        autoCommitThreshold,\n      })\n\n      start(onCrash)\n    }\n\n    const onCrash = async e => {\n      logger.error(`Crash: ${e.name}: ${e.message}`, {\n        groupId,\n        retryCount: e.retryCount,\n        stack: e.stack,\n      })\n\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        cluster.removeBroker({ host: e.host, port: e.port })\n      }\n\n      await disconnect()\n\n      const isErrorRetriable = e.name === 'KafkaJSNumberOfRetriesExceeded' || e.retriable === true\n      const shouldRestart =\n        isErrorRetriable &&\n        (!retry ||\n          !retry.restartOnFailure ||\n          (await retry.restartOnFailure(e).catch(error => {\n            logger.error(\n              'Caught error when invoking user-provided \"restartOnFailure\" callback. Defaulting to restarting.',\n              {\n                error: error.message || error,\n                originalError: e.message || e,\n                groupId,\n              }\n            )\n\n            return true\n          })))\n\n      instrumentationEmitter.emit(CRASH, {\n        error: e,\n        groupId,\n        restart: shouldRestart,\n      })\n\n      if (shouldRestart) {\n        const retryTime = e.retryTime || (retry && retry.initialRetryTime) || initialRetryTime\n        logger.error(`Restarting the consumer in ${retryTime}ms`, {\n          retryCount: e.retryCount,\n          retryTime,\n          groupId,\n        })\n\n        setTimeout(() => restart(onCrash), retryTime)\n      }\n    }\n\n    await start(onCrash)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"commitOffsets\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partition: 0, offset: '1', metadata: 'event-id-3' }]\n   */\n  const commitOffsets = async (topicPartitions = []) => {\n    const commitsByTopic = topicPartitions.reduce(\n      (payload, { topic, partition, offset, metadata = null }) => {\n        if (!topic) {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n\n        if (isNaN(partition)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition, expected a number received ${partition}`\n          )\n        }\n\n        let commitOffset\n        try {\n          commitOffset = Long.fromValue(offset)\n        } catch (_) {\n          throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n        }\n\n        if (commitOffset.lessThan(0)) {\n          throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n        }\n\n        if (metadata !== null && typeof metadata !== 'string') {\n          throw new KafkaJSNonRetriableError(\n            `Invalid offset metadata, expected string or null, received ${metadata}`\n          )\n        }\n\n        const topicCommits = payload[topic] || []\n\n        topicCommits.push({ partition, offset: commitOffset, metadata })\n\n        return { ...payload, [topic]: topicCommits }\n      },\n      {}\n    )\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    const topics = Object.keys(commitsByTopic)\n\n    return runner.commitOffsets({\n      topics: topics.map(topic => {\n        return {\n          topic,\n          partitions: commitsByTopic[topic],\n        }\n      }),\n    })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"seek\"]} */\n  const seek = ({ topic, partition, offset }) => {\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (isNaN(partition)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid partition, expected a number received ${partition}`\n      )\n    }\n\n    let seekOffset\n    try {\n      seekOffset = Long.fromValue(offset)\n    } catch (_) {\n      throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n    }\n\n    if (seekOffset.lessThan(0) && !specialOffsets.includes(seekOffset.toString())) {\n      throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.seek({ topic, partition, offset: seekOffset.toString() })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"describeGroup\"]} */\n  const describeGroup = async () => {\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const retrier = createRetry(retry)\n    return retrier(async () => {\n      const { groups } = await coordinator.describeGroups({ groupIds: [groupId] })\n      return groups.find(group => group.groupId === groupId)\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"pause\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const pause = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to pause specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.pause(topicPartitions)\n  }\n\n  /**\n   * Returns the list of topic partitions paused on this consumer\n   *\n   * @type {import(\"../../types\").Consumer[\"paused\"]}\n   */\n  const paused = () => {\n    if (!consumerGroup) {\n      return []\n    }\n\n    return consumerGroup.paused()\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"resume\"]}\n   * @param topicPartitions\n   *  Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const resume = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to resume specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.resume(topicPartitions)\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    subscribe,\n    stop,\n    run,\n    commitOffsets,\n    seek,\n    describeGroup,\n    pause,\n    paused,\n    resume,\n    on,\n    events,\n    logger: getLogger,\n  }\n}\n","const swapObject = require('../utils/swapObject')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst networkEvents = require('../network/instrumentationEvents')\nconst consumerType = InstrumentationEventType('consumer')\n\nconst events = {\n  HEARTBEAT: consumerType('heartbeat'),\n  COMMIT_OFFSETS: consumerType('commit_offsets'),\n  GROUP_JOIN: consumerType('group_join'),\n  FETCH: consumerType('fetch'),\n  FETCH_START: consumerType('fetch_start'),\n  START_BATCH_PROCESS: consumerType('start_batch_process'),\n  END_BATCH_PROCESS: consumerType('end_batch_process'),\n  CONNECT: consumerType('connect'),\n  DISCONNECT: consumerType('disconnect'),\n  STOP: consumerType('stop'),\n  CRASH: consumerType('crash'),\n  REBALANCING: consumerType('rebalancing'),\n  RECEIVED_UNSUBSCRIBED_TOPICS: consumerType('received_unsubscribed_topics'),\n  REQUEST: consumerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: consumerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: consumerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n","const Long = require('../../utils/long')\nconst flatten = require('../../utils/flatten')\nconst isInvalidOffset = require('./isInvalidOffset')\nconst initializeConsumerOffsets = require('./initializeConsumerOffsets')\nconst {\n  events: { COMMIT_OFFSETS },\n} = require('../instrumentationEvents')\n\nconst { keys, assign } = Object\nconst indexTopics = topics => topics.reduce((obj, topic) => assign(obj, { [topic]: {} }), {})\n\nconst PRIVATE = {\n  COMMITTED_OFFSETS: Symbol('private:OffsetManager:committedOffsets'),\n}\nmodule.exports = class OffsetManager {\n  /**\n   * @param {Object} options\n   * @param {import(\"../../../types\").Cluster} options.cluster\n   * @param {import(\"../../../types\").Broker} options.coordinator\n   * @param {import(\"../../../types\").IMemberAssignment} options.memberAssignment\n   * @param {boolean} options.autoCommit\n   * @param {number | null} options.autoCommitInterval\n   * @param {number | null} options.autoCommitThreshold\n   * @param {{[topic: string]: { fromBeginning: boolean }}} options.topicConfigurations\n   * @param {import(\"../../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {string} options.groupId\n   * @param {number} options.generationId\n   * @param {string} options.memberId\n   */\n  constructor({\n    cluster,\n    coordinator,\n    memberAssignment,\n    autoCommit,\n    autoCommitInterval,\n    autoCommitThreshold,\n    topicConfigurations,\n    instrumentationEmitter,\n    groupId,\n    generationId,\n    memberId,\n  }) {\n    this.cluster = cluster\n    this.coordinator = coordinator\n\n    // memberAssignment format:\n    // {\n    //   'topic1': [0, 1, 2, 3],\n    //   'topic2': [0, 1, 2, 3, 4, 5],\n    // }\n    this.memberAssignment = memberAssignment\n\n    this.topicConfigurations = topicConfigurations\n    this.instrumentationEmitter = instrumentationEmitter\n    this.groupId = groupId\n    this.generationId = generationId\n    this.memberId = memberId\n\n    this.autoCommit = autoCommit\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.lastCommit = Date.now()\n\n    this.topics = keys(memberAssignment)\n    this.clearAllOffsets()\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} partition\n   * @returns {Long}\n   */\n  nextOffset(topic, partition) {\n    if (!this.resolvedOffsets[topic][partition]) {\n      this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n    }\n\n    let offset = this.resolvedOffsets[topic][partition]\n    if (isInvalidOffset(offset)) {\n      offset = '0'\n    }\n\n    return Long.fromValue(offset)\n  }\n\n  /**\n   * @returns {Promise<import(\"../../../types\").Broker>}\n   */\n  async getCoordinator() {\n    if (!this.coordinator.isConnected()) {\n      this.coordinator = await this.cluster.findBroker(this.coordinator)\n    }\n\n    return this.coordinator\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  resetOffset({ topic, partition }) {\n    this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.resolvedOffsets[topic][partition] = Long.fromValue(offset)\n      .add(1)\n      .toString()\n  }\n\n  /**\n   * @returns {Long}\n   */\n  countResolvedOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    const subtractOffsets = (resolvedOffset, committedOffset) => {\n      const resolvedOffsetLong = Long.fromValue(resolvedOffset)\n      return isInvalidOffset(committedOffset)\n        ? resolvedOffsetLong\n        : resolvedOffsetLong.subtract(Long.fromValue(committedOffset))\n    }\n\n    const subtractPartitionOffsets = (resolvedTopicOffsets, committedTopicOffsets) =>\n      keys(resolvedTopicOffsets).map(partition =>\n        subtractOffsets(resolvedTopicOffsets[partition], committedTopicOffsets[partition])\n      )\n\n    const subtractTopicOffsets = topic =>\n      subtractPartitionOffsets(this.resolvedOffsets[topic], committedOffsets[topic])\n\n    const offsetsDiff = this.topics.map(subtractTopicOffsets)\n    return flatten(offsetsDiff).reduce((sum, offset) => sum.add(offset), Long.fromValue(0))\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  async setDefaultOffset({ topic, partition }) {\n    const { groupId, generationId, memberId } = this\n    const defaultOffset = this.cluster.defaultOffset(this.topicConfigurations[topic])\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset: defaultOffset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  /**\n   * Commit the given offset to the topic/partition. If the consumer isn't assigned to the given\n   * topic/partition this method will be a NO-OP.\n   *\n   * @param {import(\"../../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  async seek({ topic, partition, offset }) {\n    if (!this.memberAssignment[topic] || !this.memberAssignment[topic].includes(partition)) {\n      return\n    }\n\n    if (!this.autoCommit) {\n      this.resolveOffset({\n        topic,\n        partition,\n        offset: Long.fromValue(offset)\n          .subtract(1)\n          .toString(),\n      })\n      return\n    }\n\n    const { groupId, generationId, memberId } = this\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  async commitOffsetsIfNecessary() {\n    const now = Date.now()\n\n    const timeoutReached =\n      this.autoCommitInterval != null && now >= this.lastCommit + this.autoCommitInterval\n\n    const thresholdReached =\n      this.autoCommitThreshold != null &&\n      this.countResolvedOffsets().gte(Long.fromValue(this.autoCommitThreshold))\n\n    if (timeoutReached || thresholdReached) {\n      return this.commitOffsets()\n    }\n  }\n\n  /**\n   * Return all locally resolved offsets which are not marked as committed, by topic-partition.\n   * @returns {OffsetsByTopicPartition}\n   *\n   * @typedef {Object} OffsetsByTopicPartition\n   * @property {TopicOffsets[]} topics\n   *\n   * @typedef {Object} TopicOffsets\n   * @property {PartitionOffset[]} partitions\n   *\n   * @typedef {Object} PartitionOffset\n   * @property {string} partition\n   * @property {string} offset\n   */\n  uncommittedOffsets() {\n    const offsets = topic => keys(this.resolvedOffsets[topic])\n    const emptyPartitions = ({ partitions }) => partitions.length > 0\n    const toPartitions = topic => partition => ({\n      partition,\n      offset: this.resolvedOffsets[topic][partition],\n    })\n    const changedOffsets = topic => ({ partition, offset }) => {\n      return (\n        offset !== this.committedOffsets()[topic][partition] &&\n        Long.fromValue(offset).greaterThanOrEqual(0)\n      )\n    }\n\n    // Select and format updated partitions\n    const topicsWithPartitionsToCommit = this.topics\n      .map(topic => ({\n        topic,\n        partitions: offsets(topic)\n          .map(toPartitions(topic))\n          .filter(changedOffsets(topic)),\n      }))\n      .filter(emptyPartitions)\n\n    return { topics: topicsWithPartitionsToCommit }\n  }\n\n  async commitOffsets(offsets = {}) {\n    const { groupId, generationId, memberId } = this\n    const { topics = this.uncommittedOffsets().topics } = offsets\n\n    if (topics.length === 0) {\n      this.lastCommit = Date.now()\n      return\n    }\n\n    const payload = {\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics,\n    }\n\n    try {\n      const coordinator = await this.getCoordinator()\n      await coordinator.offsetCommit(payload)\n      this.instrumentationEmitter.emit(COMMIT_OFFSETS, payload)\n\n      // Update local reference of committed offsets\n      topics.forEach(({ topic, partitions }) => {\n        const updatedOffsets = partitions.reduce(\n          (obj, { partition, offset }) => assign(obj, { [partition]: offset }),\n          {}\n        )\n\n        this[PRIVATE.COMMITTED_OFFSETS][topic] = assign(\n          {},\n          this.committedOffsets()[topic],\n          updatedOffsets\n        )\n      })\n\n      this.lastCommit = Date.now()\n    } catch (e) {\n      // metadata is stale, the coordinator has changed due to a restart or\n      // broker reassignment\n      if (e.type === 'NOT_COORDINATOR_FOR_GROUP') {\n        await this.cluster.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  async resolveOffsets() {\n    const { groupId } = this\n    const invalidOffset = topic => partition => {\n      return isInvalidOffset(this.committedOffsets()[topic][partition])\n    }\n\n    const pendingPartitions = this.topics\n      .map(topic => ({\n        topic,\n        partitions: this.memberAssignment[topic]\n          .filter(invalidOffset(topic))\n          .map(partition => ({ partition })),\n      }))\n      .filter(t => t.partitions.length > 0)\n\n    if (pendingPartitions.length === 0) {\n      return\n    }\n\n    const coordinator = await this.getCoordinator()\n    const { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: pendingPartitions,\n    })\n\n    const unresolvedPartitions = consumerOffsets.map(({ topic, partitions }) =>\n      assign(\n        {\n          topic,\n          partitions: partitions\n            .filter(({ offset }) => isInvalidOffset(offset))\n            .map(({ partition }) => assign({ partition })),\n        },\n        this.topicConfigurations[topic]\n      )\n    )\n\n    const indexPartitions = (obj, { partition, offset }) => {\n      return assign(obj, { [partition]: offset })\n    }\n\n    const hasUnresolvedPartitions = () => unresolvedPartitions.some(t => t.partitions.length > 0)\n\n    let offsets = consumerOffsets\n    if (hasUnresolvedPartitions()) {\n      const topicOffsets = await this.cluster.fetchTopicsOffset(unresolvedPartitions)\n      offsets = initializeConsumerOffsets(consumerOffsets, topicOffsets)\n    }\n\n    offsets.forEach(({ topic, partitions }) => {\n      this.committedOffsets()[topic] = partitions.reduce(indexPartitions, {\n        ...this.committedOffsets()[topic],\n      })\n    })\n  }\n\n  /**\n   * @private\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  clearOffsets({ topic, partition }) {\n    delete this.committedOffsets()[topic][partition]\n    delete this.resolvedOffsets[topic][partition]\n  }\n\n  /**\n   * @private\n   */\n  clearAllOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    for (const topic in committedOffsets) {\n      delete committedOffsets[topic]\n    }\n\n    for (const topic of this.topics) {\n      committedOffsets[topic] = {}\n    }\n\n    this.resolvedOffsets = indexTopics(this.topics)\n  }\n\n  committedOffsets() {\n    if (!this[PRIVATE.COMMITTED_OFFSETS]) {\n      this[PRIVATE.COMMITTED_OFFSETS] = this.groupId\n        ? this.cluster.committedOffsets({ groupId: this.groupId })\n        : {}\n    }\n\n    return this[PRIVATE.COMMITTED_OFFSETS]\n  }\n}\n","const isInvalidOffset = require('./isInvalidOffset')\nconst { keys, assign } = Object\n\nconst indexPartitions = (obj, { partition, offset }) => assign(obj, { [partition]: offset })\nconst indexTopics = (obj, { topic, partitions }) =>\n  assign(obj, { [topic]: partitions.reduce(indexPartitions, {}) })\n\nmodule.exports = (consumerOffsets, topicOffsets) => {\n  const indexedConsumerOffsets = consumerOffsets.reduce(indexTopics, {})\n  const indexedTopicOffsets = topicOffsets.reduce(indexTopics, {})\n\n  return keys(indexedConsumerOffsets).map(topic => {\n    const partitions = indexedConsumerOffsets[topic]\n    return {\n      topic,\n      partitions: keys(partitions).map(partition => {\n        const offset = partitions[partition]\n        const resolvedOffset = isInvalidOffset(offset)\n          ? indexedTopicOffsets[topic][partition]\n          : offset\n\n        return { partition: Number(partition), offset: resolvedOffset }\n      }),\n    }\n  })\n}\n","const Long = require('../../utils/long')\n\nmodule.exports = offset => (!offset && offset !== 0) || Long.fromValue(offset).isNegative()\n","const { EventEmitter } = require('events')\nconst Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst limitConcurrency = require('../utils/concurrency')\nconst { KafkaJSError } = require('../errors')\nconst barrier = require('./barrier')\n\nconst {\n  events: { FETCH, FETCH_START, START_BATCH_PROCESS, END_BATCH_PROCESS, REBALANCING },\n} = require('./instrumentationEvents')\n\nconst isRebalancing = e =>\n  e.type === 'REBALANCE_IN_PROGRESS' || e.type === 'NOT_COORDINATOR_FOR_GROUP'\n\nconst isKafkaJSError = e => e instanceof KafkaJSError\nconst isSameOffset = (offsetA, offsetB) => Long.fromValue(offsetA).equals(Long.fromValue(offsetB))\nconst CONSUMING_START = 'consuming-start'\nconst CONSUMING_STOP = 'consuming-stop'\n\nmodule.exports = class Runner extends EventEmitter {\n  /**\n   * @param {object} options\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"./consumerGroup\")} options.consumerGroup\n   * @param {import(\"../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {boolean} [options.eachBatchAutoResolve=true]\n   * @param {number} [options.partitionsConsumedConcurrently]\n   * @param {(payload: import(\"../../types\").EachBatchPayload) => Promise<void>} options.eachBatch\n   * @param {(payload: import(\"../../types\").EachMessagePayload) => Promise<void>} options.eachMessage\n   * @param {number} [options.heartbeatInterval]\n   * @param {(reason: Error) => void} options.onCrash\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.autoCommit=true]\n   */\n  constructor({\n    logger,\n    consumerGroup,\n    instrumentationEmitter,\n    eachBatchAutoResolve = true,\n    partitionsConsumedConcurrently,\n    eachBatch,\n    eachMessage,\n    heartbeatInterval,\n    onCrash,\n    retry,\n    autoCommit = true,\n  }) {\n    super()\n    this.logger = logger.namespace('Runner')\n    this.consumerGroup = consumerGroup\n    this.instrumentationEmitter = instrumentationEmitter\n    this.eachBatchAutoResolve = eachBatchAutoResolve\n    this.eachBatch = eachBatch\n    this.eachMessage = eachMessage\n    this.heartbeatInterval = heartbeatInterval\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.onCrash = onCrash\n    this.autoCommit = autoCommit\n    this.partitionsConsumedConcurrently = partitionsConsumedConcurrently\n\n    this.running = false\n    this.consuming = false\n  }\n\n  get consuming() {\n    return this._consuming\n  }\n\n  set consuming(value) {\n    if (this._consuming !== value) {\n      this._consuming = value\n      this.emit(value ? CONSUMING_START : CONSUMING_STOP)\n    }\n  }\n\n  async join() {\n    await this.consumerGroup.joinAndSync()\n    this.running = true\n  }\n\n  async scheduleJoin() {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n      return\n    }\n\n    return this.join().catch(this.onCrash)\n  }\n\n  async start() {\n    if (this.running) {\n      return\n    }\n\n    try {\n      await this.consumerGroup.connect()\n      await this.join()\n\n      this.running = true\n      this.scheduleFetch()\n    } catch (e) {\n      this.onCrash(e)\n    }\n  }\n\n  async stop() {\n    if (!this.running) {\n      return\n    }\n\n    this.logger.debug('stop consumer group', {\n      groupId: this.consumerGroup.groupId,\n      memberId: this.consumerGroup.memberId,\n    })\n\n    this.running = false\n\n    try {\n      await this.waitForConsumer()\n      await this.consumerGroup.leave()\n    } catch (e) {}\n  }\n\n  waitForConsumer() {\n    return new Promise(resolve => {\n      if (!this.consuming) {\n        return resolve()\n      }\n\n      this.logger.debug('waiting for consumer to finish...', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      this.once(CONSUMING_STOP, () => resolve())\n    })\n  }\n\n  async processEachMessage(batch) {\n    const { topic, partition } = batch\n\n    for (const message of batch.messages) {\n      if (!this.running || this.consumerGroup.hasSeekOffset({ topic, partition })) {\n        break\n      }\n\n      try {\n        await this.eachMessage({\n          topic,\n          partition,\n          message,\n          heartbeat: async () => {\n            await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n          },\n        })\n      } catch (e) {\n        if (!isKafkaJSError(e)) {\n          this.logger.error(`Error when calling eachMessage`, {\n            topic,\n            partition,\n            offset: message.offset,\n            stack: e.stack,\n            error: e,\n          })\n        }\n\n        // In case of errors, commit the previously consumed offsets unless autoCommit is disabled\n        await this.autoCommitOffsets()\n        throw e\n      }\n\n      this.consumerGroup.resolveOffset({ topic, partition, offset: message.offset })\n      await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n      await this.autoCommitOffsetsIfNecessary()\n    }\n  }\n\n  async processEachBatch(batch) {\n    const { topic, partition } = batch\n    const lastFilteredMessage = batch.messages[batch.messages.length - 1]\n\n    try {\n      await this.eachBatch({\n        batch,\n        resolveOffset: offset => {\n          /**\n           * The transactional producer generates a control record after committing the transaction.\n           * The control record is the last record on the RecordBatch, and it is filtered before it\n           * reaches the eachBatch callback. When disabling auto-resolve, the user-land code won't\n           * be able to resolve the control record offset, since it never reaches the callback,\n           * causing stuck consumers as the consumer will never move the offset marker.\n           *\n           * When the last offset of the batch is resolved, we should automatically resolve\n           * the control record offset as this entry doesn't have any meaning to the user-land code,\n           * and won't interfere with the stream processing.\n           *\n           * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n           */\n          const offsetToResolve =\n            lastFilteredMessage && isSameOffset(offset, lastFilteredMessage.offset)\n              ? batch.lastOffset()\n              : offset\n\n          this.consumerGroup.resolveOffset({ topic, partition, offset: offsetToResolve })\n        },\n        heartbeat: async () => {\n          await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n        },\n        /**\n         * Commit offsets if provided. Otherwise commit most recent resolved offsets\n         * if the autoCommit conditions are met.\n         *\n         * @param {OffsetsByTopicPartition} [offsets] Optional.\n         */\n        commitOffsetsIfNecessary: async offsets => {\n          return offsets\n            ? this.consumerGroup.commitOffsets(offsets)\n            : this.consumerGroup.commitOffsetsIfNecessary()\n        },\n        uncommittedOffsets: () => this.consumerGroup.uncommittedOffsets(),\n        isRunning: () => this.running,\n        isStale: () => this.consumerGroup.hasSeekOffset({ topic, partition }),\n      })\n    } catch (e) {\n      if (!isKafkaJSError(e)) {\n        this.logger.error(`Error when calling eachBatch`, {\n          topic,\n          partition,\n          offset: batch.firstOffset(),\n          stack: e.stack,\n          error: e,\n        })\n      }\n\n      // eachBatch has a special resolveOffset which can be used\n      // to keep track of the messages\n      await this.autoCommitOffsets()\n      throw e\n    }\n\n    // resolveOffset for the last offset can be disabled to allow the users of eachBatch to\n    // stop their consumers without resolving unprocessed offsets (issues/18)\n    if (this.eachBatchAutoResolve) {\n      this.consumerGroup.resolveOffset({ topic, partition, offset: batch.lastOffset() })\n    }\n  }\n\n  async fetch() {\n    const startFetch = Date.now()\n\n    this.instrumentationEmitter.emit(FETCH_START, {})\n\n    const iterator = await this.consumerGroup.fetch()\n\n    this.instrumentationEmitter.emit(FETCH, {\n      /**\n       * PR #570 removed support for the number of batches in this instrumentation event;\n       * The new implementation uses an async generation to deliver the batches, which makes\n       * this number impossible to get. The number is set to 0 to keep the event backward\n       * compatible until we bump KafkaJS to version 2, following the end of node 8 LTS.\n       *\n       * @since 2019-11-29\n       */\n      numberOfBatches: 0,\n      duration: Date.now() - startFetch,\n    })\n\n    const onBatch = async batch => {\n      const startBatchProcess = Date.now()\n      const payload = {\n        topic: batch.topic,\n        partition: batch.partition,\n        highWatermark: batch.highWatermark,\n        offsetLag: batch.offsetLag(),\n        /**\n         * @since 2019-06-24 (>= 1.8.0)\n         *\n         * offsetLag returns the lag based on the latest offset in the batch, to\n         * keep the event backward compatible we just introduced \"offsetLagLow\"\n         * which calculates the lag based on the first offset in the batch\n         */\n        offsetLagLow: batch.offsetLagLow(),\n        batchSize: batch.messages.length,\n        firstOffset: batch.firstOffset(),\n        lastOffset: batch.lastOffset(),\n      }\n\n      /**\n       * If the batch contained only control records or only aborted messages then we still\n       * need to resolve and auto-commit to ensure the consumer can move forward.\n       *\n       * We also need to emit batch instrumentation events to allow any listeners keeping\n       * track of offsets to know about the latest point of consumption.\n       *\n       * Added in #1256\n       *\n       * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n       */\n      if (batch.isEmptyDueToFiltering()) {\n        this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n        this.consumerGroup.resolveOffset({\n          topic: batch.topic,\n          partition: batch.partition,\n          offset: batch.lastOffset(),\n        })\n        await this.autoCommitOffsetsIfNecessary()\n\n        this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n          ...payload,\n          duration: Date.now() - startBatchProcess,\n        })\n        return\n      }\n\n      if (batch.isEmpty()) {\n        return\n      }\n\n      this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n      if (this.eachMessage) {\n        await this.processEachMessage(batch)\n      } else if (this.eachBatch) {\n        await this.processEachBatch(batch)\n      }\n\n      this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n        ...payload,\n        duration: Date.now() - startBatchProcess,\n      })\n\n      await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n    }\n\n    const { lock, unlock, unlockWithError } = barrier()\n    const concurrently = limitConcurrency({ limit: this.partitionsConsumedConcurrently })\n\n    let requestsCompleted = false\n    let numberOfExecutions = 0\n    let expectedNumberOfExecutions = 0\n    const enqueuedTasks = []\n\n    while (true) {\n      const result = iterator.next()\n\n      if (result.done) {\n        break\n      }\n\n      if (!this.running) {\n        result.value.catch(error => {\n          this.logger.debug('Ignoring error in fetch request while stopping runner', {\n            error: error.message || error,\n            stack: error.stack,\n          })\n        })\n\n        continue\n      }\n\n      enqueuedTasks.push(async () => {\n        const batches = await result.value\n        expectedNumberOfExecutions += batches.length\n\n        batches.map(batch =>\n          concurrently(async () => {\n            try {\n              if (!this.running) {\n                return\n              }\n\n              await onBatch(batch)\n            } catch (e) {\n              unlockWithError(e)\n            } finally {\n              numberOfExecutions++\n              if (requestsCompleted && numberOfExecutions === expectedNumberOfExecutions) {\n                unlock()\n              }\n            }\n          }).catch(unlockWithError)\n        )\n      })\n    }\n\n    await Promise.all(enqueuedTasks.map(fn => fn()))\n    requestsCompleted = true\n\n    if (expectedNumberOfExecutions === numberOfExecutions) {\n      unlock()\n    }\n\n    const error = await lock\n    if (error) {\n      throw error\n    }\n\n    await this.autoCommitOffsets()\n    await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n  }\n\n  async scheduleFetch() {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        this.consuming = true\n        await this.fetch()\n        this.consuming = false\n\n        if (this.running) {\n          setImmediate(() => this.scheduleFetch())\n        }\n      } catch (e) {\n        if (!this.running) {\n          this.logger.debug('consumer not running, exiting', {\n            error: e.message,\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n          })\n          return\n        }\n\n        if (isRebalancing(e)) {\n          this.logger.warn('The group is rebalancing, re-joining', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n            retryCount,\n            retryTime,\n          })\n\n          this.instrumentationEmitter.emit(REBALANCING, {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n          })\n\n          await this.join()\n          setImmediate(() => this.scheduleFetch())\n          return\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.logger.error('The coordinator is not aware of this member, re-joining the group', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n            retryCount,\n            retryTime,\n          })\n\n          this.consumerGroup.memberId = null\n          await this.join()\n          setImmediate(() => this.scheduleFetch())\n          return\n        }\n\n        if (e.name === 'KafkaJSOffsetOutOfRange') {\n          setImmediate(() => this.scheduleFetch())\n          return\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while fetching data, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n        })\n\n        throw e\n      } finally {\n        this.consuming = false\n      }\n    }).catch(this.onCrash)\n  }\n\n  autoCommitOffsets() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsets()\n    }\n  }\n\n  autoCommitOffsetsIfNecessary() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsetsIfNecessary()\n    }\n  }\n\n  commitOffsets(offsets) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n        offsets,\n      })\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.consumerGroup.commitOffsets(offsets)\n      } catch (e) {\n        if (!this.running) {\n          this.logger.debug('consumer not running, exiting', {\n            error: e.message,\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            offsets,\n          })\n          return\n        }\n\n        if (isRebalancing(e)) {\n          this.logger.warn('The group is rebalancing, re-joining', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n            retryCount,\n            retryTime,\n          })\n\n          this.instrumentationEmitter.emit(REBALANCING, {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n          })\n\n          setImmediate(() => this.scheduleJoin())\n\n          bail(new KafkaJSError(e))\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.logger.error('The coordinator is not aware of this member, re-joining the group', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n            retryCount,\n            retryTime,\n          })\n\n          this.consumerGroup.memberId = null\n          setImmediate(() => this.scheduleJoin())\n\n          bail(new KafkaJSError(e))\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while committing offsets, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n          offsets,\n        })\n\n        throw e\n      }\n    })\n  }\n}\n","module.exports = class SeekOffsets extends Map {\n  set(topic, partition, offset) {\n    super.set([topic, partition], offset)\n  }\n\n  has(topic, partition) {\n    return Array.from(this.keys()).some(([t, p]) => t === topic && p === partition)\n  }\n\n  pop() {\n    if (this.size === 0) {\n      return\n    }\n\n    const [key, offset] = this.entries().next().value\n    this.delete(key)\n    const [topic, partition] = key\n    return { topic, partition, offset }\n  }\n}\n","const createState = topic => ({\n  topic,\n  paused: new Set(),\n  pauseAll: false,\n  resumed: new Set(),\n})\n\nmodule.exports = class SubscriptionState {\n  constructor() {\n    this.assignedPartitionsByTopic = {}\n    this.subscriptionStatesByTopic = {}\n  }\n\n  /**\n   * Replace the current assignment with a new set of assignments\n   *\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assign(topicPartitions = []) {\n    this.assignedPartitionsByTopic = topicPartitions.reduce(\n      (assigned, { topic, partitions = [] }) => {\n        return { ...assigned, [topic]: { topic, partitions } }\n      },\n      {}\n    )\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  pause(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = true\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.add(partition)\n          state.resumed.delete(partition)\n        })\n        state.pauseAll = false\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  resume(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = false\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.delete(partition)\n\n          if (state.pauseAll) {\n            state.resumed.add(partition)\n          }\n        })\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assigned() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  active() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.filter(partition => !this.isPaused(topic, partition)).sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  paused() {\n    return Object.values(this.assignedPartitionsByTopic)\n      .map(({ topic, partitions }) => ({\n        topic,\n        partitions: partitions.filter(partition => this.isPaused(topic, partition)).sort(),\n      }))\n      .filter(({ partitions }) => partitions.length !== 0)\n  }\n\n  isPaused(topic, partition) {\n    const state = this.subscriptionStatesByTopic[topic]\n\n    if (!state) {\n      return false\n    }\n\n    const partitionResumed = state.resumed.has(partition)\n    const partitionPaused = state.paused.has(partition)\n\n    return (state.pauseAll && !partitionResumed) || partitionPaused\n  }\n}\n","module.exports = () => ({\n  KAFKAJS_DEBUG_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS,\n  KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS,\n})\n","const pkgJson = require('../package.json')\nconst { bugs } = pkgJson\n\nclass KafkaJSError extends Error {\n  constructor(e, { retriable = true } = {}) {\n    super(e)\n    Error.captureStackTrace(this, this.constructor)\n    this.message = e.message || e\n    this.name = 'KafkaJSError'\n    this.retriable = retriable\n    this.helpUrl = e.helpUrl\n  }\n}\n\nclass KafkaJSNonRetriableError extends KafkaJSError {\n  constructor(e) {\n    super(e, { retriable: false })\n    this.name = 'KafkaJSNonRetriableError'\n    this.originalError = e\n  }\n}\n\nclass KafkaJSProtocolError extends KafkaJSError {\n  constructor(e, { retriable = e.retriable } = {}) {\n    super(e, { retriable })\n    this.type = e.type\n    this.code = e.code\n    this.name = 'KafkaJSProtocolError'\n  }\n}\n\nclass KafkaJSOffsetOutOfRange extends KafkaJSProtocolError {\n  constructor(e, { topic, partition }) {\n    super(e)\n    this.topic = topic\n    this.partition = partition\n    this.name = 'KafkaJSOffsetOutOfRange'\n  }\n}\n\nclass KafkaJSMemberIdRequired extends KafkaJSProtocolError {\n  constructor(e, { memberId }) {\n    super(e)\n    this.memberId = memberId\n    this.name = 'KafkaJSMemberIdRequired'\n  }\n}\n\nclass KafkaJSNumberOfRetriesExceeded extends KafkaJSNonRetriableError {\n  constructor(e, { retryCount, retryTime }) {\n    super(e)\n    this.stack = `${this.name}\\n  Caused by: ${e.stack}`\n    this.originalError = e\n    this.retryCount = retryCount\n    this.retryTime = retryTime\n    this.name = 'KafkaJSNumberOfRetriesExceeded'\n  }\n}\n\nclass KafkaJSConnectionError extends KafkaJSError {\n  constructor(e, { broker, code } = {}) {\n    super(e)\n    this.broker = broker\n    this.code = code\n    this.name = 'KafkaJSConnectionError'\n  }\n}\n\nclass KafkaJSConnectionClosedError extends KafkaJSConnectionError {\n  constructor(e, { host, port } = {}) {\n    super(e, { broker: `${host}:${port}` })\n    this.host = host\n    this.port = port\n    this.name = 'KafkaJSConnectionClosedError'\n  }\n}\n\nclass KafkaJSRequestTimeoutError extends KafkaJSError {\n  constructor(e, { broker, correlationId, createdAt, sentAt, pendingDuration } = {}) {\n    super(e)\n    this.broker = broker\n    this.correlationId = correlationId\n    this.createdAt = createdAt\n    this.sentAt = sentAt\n    this.pendingDuration = pendingDuration\n    this.name = 'KafkaJSRequestTimeoutError'\n  }\n}\n\nclass KafkaJSMetadataNotLoaded extends KafkaJSError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSMetadataNotLoaded'\n  }\n}\nclass KafkaJSTopicMetadataNotLoaded extends KafkaJSMetadataNotLoaded {\n  constructor(e, { topic } = {}) {\n    super(e)\n    this.topic = topic\n    this.name = 'KafkaJSTopicMetadataNotLoaded'\n  }\n}\nclass KafkaJSStaleTopicMetadataAssignment extends KafkaJSError {\n  constructor(e, { topic, unknownPartitions } = {}) {\n    super(e)\n    this.topic = topic\n    this.unknownPartitions = unknownPartitions\n    this.name = 'KafkaJSStaleTopicMetadataAssignment'\n  }\n}\n\nclass KafkaJSDeleteGroupsError extends KafkaJSError {\n  constructor(e, groups = []) {\n    super(e)\n    this.groups = groups\n    this.name = 'KafkaJSDeleteGroupsError'\n  }\n}\n\nclass KafkaJSServerDoesNotSupportApiKey extends KafkaJSNonRetriableError {\n  constructor(e, { apiKey, apiName } = {}) {\n    super(e)\n    this.apiKey = apiKey\n    this.apiName = apiName\n    this.name = 'KafkaJSServerDoesNotSupportApiKey'\n  }\n}\n\nclass KafkaJSBrokerNotFound extends KafkaJSError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSBrokerNotFound'\n  }\n}\n\nclass KafkaJSPartialMessageError extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSPartialMessageError'\n  }\n}\n\nclass KafkaJSSASLAuthenticationError extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSSASLAuthenticationError'\n  }\n}\n\nclass KafkaJSGroupCoordinatorNotFound extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSGroupCoordinatorNotFound'\n  }\n}\n\nclass KafkaJSNotImplemented extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSNotImplemented'\n  }\n}\n\nclass KafkaJSTimeout extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSTimeout'\n  }\n}\n\nclass KafkaJSLockTimeout extends KafkaJSTimeout {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSLockTimeout'\n  }\n}\n\nclass KafkaJSUnsupportedMagicByteInMessageSet extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSUnsupportedMagicByteInMessageSet'\n  }\n}\n\nclass KafkaJSDeleteTopicRecordsError extends KafkaJSError {\n  constructor({ partitions }) {\n    /*\n     * This error is retriable if all the errors were retriable\n     */\n    const retriable = partitions\n      .filter(({ error }) => error != null)\n      .every(({ error }) => error.retriable === true)\n\n    super('Error while deleting records', { retriable })\n    this.name = 'KafkaJSDeleteTopicRecordsError'\n    this.partitions = partitions\n  }\n}\n\nconst issueUrl = bugs ? bugs.url : null\n\nclass KafkaJSInvariantViolation extends KafkaJSNonRetriableError {\n  constructor(e) {\n    const message = e.message || e\n    super(`Invariant violated: ${message}. This is likely a bug and should be reported.`)\n    this.name = 'KafkaJSInvariantViolation'\n\n    if (issueUrl !== null) {\n      const issueTitle = encodeURIComponent(`Invariant violation: ${message}`)\n      this.helpUrl = `${issueUrl}/new?assignees=&labels=bug&template=bug_report.md&title=${issueTitle}`\n    }\n  }\n}\n\nclass KafkaJSInvalidVarIntError extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSNonRetriableError'\n  }\n}\n\nclass KafkaJSInvalidLongError extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSNonRetriableError'\n  }\n}\n\nclass KafkaJSCreateTopicError extends KafkaJSProtocolError {\n  constructor(e, topicName) {\n    super(e)\n    this.topic = topicName\n    this.name = 'KafkaJSCreateTopicError'\n  }\n}\nclass KafkaJSAggregateError extends Error {\n  constructor(message, errors) {\n    super(message)\n    this.errors = errors\n    this.name = 'KafkaJSAggregateError'\n  }\n}\n\nmodule.exports = {\n  KafkaJSError,\n  KafkaJSNonRetriableError,\n  KafkaJSPartialMessageError,\n  KafkaJSBrokerNotFound,\n  KafkaJSProtocolError,\n  KafkaJSConnectionError,\n  KafkaJSConnectionClosedError,\n  KafkaJSRequestTimeoutError,\n  KafkaJSSASLAuthenticationError,\n  KafkaJSNumberOfRetriesExceeded,\n  KafkaJSOffsetOutOfRange,\n  KafkaJSMemberIdRequired,\n  KafkaJSGroupCoordinatorNotFound,\n  KafkaJSNotImplemented,\n  KafkaJSMetadataNotLoaded,\n  KafkaJSTopicMetadataNotLoaded,\n  KafkaJSStaleTopicMetadataAssignment,\n  KafkaJSDeleteGroupsError,\n  KafkaJSTimeout,\n  KafkaJSLockTimeout,\n  KafkaJSServerDoesNotSupportApiKey,\n  KafkaJSUnsupportedMagicByteInMessageSet,\n  KafkaJSDeleteTopicRecordsError,\n  KafkaJSInvariantViolation,\n  KafkaJSInvalidVarIntError,\n  KafkaJSInvalidLongError,\n  KafkaJSCreateTopicError,\n  KafkaJSAggregateError,\n}\n","const {\n  createLogger,\n  LEVELS: { INFO },\n} = require('./loggers')\n\nconst InstrumentationEventEmitter = require('./instrumentation/emitter')\nconst LoggerConsole = require('./loggers/console')\nconst Cluster = require('./cluster')\nconst createProducer = require('./producer')\nconst createConsumer = require('./consumer')\nconst createAdmin = require('./admin')\nconst ISOLATION_LEVEL = require('./protocol/isolationLevel')\nconst defaultSocketFactory = require('./network/socketFactory')\n\nconst PRIVATE = {\n  CREATE_CLUSTER: Symbol('private:Kafka:createCluster'),\n  CLUSTER_RETRY: Symbol('private:Kafka:clusterRetry'),\n  LOGGER: Symbol('private:Kafka:logger'),\n  OFFSETS: Symbol('private:Kafka:offsets'),\n}\n\nconst DEFAULT_METADATA_MAX_AGE = 300000\n\nmodule.exports = class Client {\n  /**\n   * @param {Object} options\n   * @param {Array<string>} options.brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n   * @param {Object} options.ssl\n   * @param {Object} options.sasl\n   * @param {string} options.clientId\n   * @param {number} options.connectionTimeout - in milliseconds\n   * @param {number} options.authenticationTimeout - in milliseconds\n   * @param {number} options.reauthenticationThreshold - in milliseconds\n   * @param {number} [options.requestTimeout=30000] - in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {import(\"../types\").RetryOptions} [options.retry]\n   * @param {import(\"../types\").ISocketFactory} [options.socketFactory]\n   */\n  constructor({\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout,\n    enforceRequestTimeout = false,\n    retry,\n    socketFactory = defaultSocketFactory(),\n    logLevel = INFO,\n    logCreator = LoggerConsole,\n  }) {\n    this[PRIVATE.OFFSETS] = new Map()\n    this[PRIVATE.LOGGER] = createLogger({ level: logLevel, logCreator })\n    this[PRIVATE.CLUSTER_RETRY] = retry\n    this[PRIVATE.CREATE_CLUSTER] = ({\n      metadataMaxAge,\n      allowAutoTopicCreation = true,\n      maxInFlightRequests = null,\n      instrumentationEmitter = null,\n      isolationLevel,\n    }) =>\n      new Cluster({\n        logger: this[PRIVATE.LOGGER],\n        retry: this[PRIVATE.CLUSTER_RETRY],\n        offsets: this[PRIVATE.OFFSETS],\n        socketFactory,\n        brokers,\n        ssl,\n        sasl,\n        clientId,\n        connectionTimeout,\n        authenticationTimeout,\n        reauthenticationThreshold,\n        requestTimeout,\n        enforceRequestTimeout,\n        metadataMaxAge,\n        instrumentationEmitter,\n        allowAutoTopicCreation,\n        maxInFlightRequests,\n        isolationLevel,\n      })\n  }\n\n  /**\n   * @public\n   */\n  producer({\n    createPartitioner,\n    retry,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    allowAutoTopicCreation,\n    idempotent,\n    transactionalId,\n    transactionTimeout,\n    maxInFlightRequests,\n  } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      instrumentationEmitter,\n    })\n\n    return createProducer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      createPartitioner,\n      idempotent,\n      transactionalId,\n      transactionTimeout,\n      instrumentationEmitter,\n    })\n  }\n\n  /**\n   * @public\n   */\n  consumer({\n    groupId,\n    partitionAssigners,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    sessionTimeout,\n    rebalanceTimeout,\n    heartbeatInterval,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    retry = { retries: 5 },\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    readUncommitted = false,\n    rackId = '',\n  } = {}) {\n    const isolationLevel = readUncommitted\n      ? ISOLATION_LEVEL.READ_UNCOMMITTED\n      : ISOLATION_LEVEL.READ_COMMITTED\n\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      isolationLevel,\n      instrumentationEmitter,\n    })\n\n    return createConsumer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      groupId,\n      partitionAssigners,\n      sessionTimeout,\n      rebalanceTimeout,\n      heartbeatInterval,\n      maxBytesPerPartition,\n      minBytes,\n      maxBytes,\n      maxWaitTimeInMs,\n      isolationLevel,\n      instrumentationEmitter,\n      rackId,\n      metadataMaxAge,\n    })\n  }\n\n  /**\n   * @public\n   */\n  admin({ retry } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      allowAutoTopicCreation: false,\n      instrumentationEmitter,\n    })\n\n    return createAdmin({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      instrumentationEmitter,\n      cluster,\n    })\n  }\n\n  /**\n   * @public\n   */\n  logger() {\n    return this[PRIVATE.LOGGER]\n  }\n}\n","const { EventEmitter } = require('events')\nconst InstrumentationEvent = require('./event')\nconst { KafkaJSError } = require('../errors')\n\nmodule.exports = class InstrumentationEventEmitter {\n  constructor() {\n    this.emitter = new EventEmitter()\n  }\n\n  /**\n   * @param {string} eventName\n   * @param {Object} payload\n   */\n  emit(eventName, payload) {\n    if (!eventName) {\n      throw new KafkaJSError('Invalid event name', { retriable: false })\n    }\n\n    if (this.emitter.listenerCount(eventName) > 0) {\n      const event = new InstrumentationEvent(eventName, payload)\n      this.emitter.emit(eventName, event)\n    }\n  }\n\n  /**\n   * @param {string} eventName\n   * @param {(...args: any[]) => void} listener\n   * @returns {import(\"../../types\").RemoveInstrumentationEventListener<string>} removeListener\n   */\n  addListener(eventName, listener) {\n    this.emitter.addListener(eventName, listener)\n    return () => this.emitter.removeListener(eventName, listener)\n  }\n}\n","let id = 0\nconst nextId = () => {\n  if (id === Number.MAX_VALUE) {\n    id = 0\n  }\n\n  return id++\n}\n\nclass InstrumentationEvent {\n  /**\n   * @param {String} type\n   * @param {Object} payload\n   */\n  constructor(type, payload) {\n    this.id = nextId()\n    this.type = type\n    this.timestamp = Date.now()\n    this.payload = payload\n  }\n}\n\nmodule.exports = InstrumentationEvent\n","module.exports = namespace => type => `${namespace}.${type}`\n","const { LEVELS: logLevel } = require('./index')\n\nmodule.exports = () => ({ namespace, level, label, log }) => {\n  const prefix = namespace ? `[${namespace}] ` : ''\n  const message = JSON.stringify(\n    Object.assign({ level: label }, log, {\n      message: `${prefix}${log.message}`,\n    })\n  )\n\n  switch (level) {\n    case logLevel.INFO:\n      return console.info(message)\n    case logLevel.ERROR:\n      return console.error(message)\n    case logLevel.WARN:\n      return console.warn(message)\n    case logLevel.DEBUG:\n      return console.log(message)\n  }\n}\n","const { assign } = Object\n\nconst LEVELS = {\n  NOTHING: 0,\n  ERROR: 1,\n  WARN: 2,\n  INFO: 4,\n  DEBUG: 5,\n}\n\nconst createLevel = (label, level, currentLevel, namespace, logFunction) => (\n  message,\n  extra = {}\n) => {\n  if (level > currentLevel()) return\n  logFunction({\n    namespace,\n    level,\n    label,\n    log: assign(\n      {\n        timestamp: new Date().toISOString(),\n        logger: 'kafkajs',\n        message,\n      },\n      extra\n    ),\n  })\n}\n\nconst evaluateLogLevel = logLevel => {\n  const envLogLevel = (process.env.KAFKAJS_LOG_LEVEL || '').toUpperCase()\n  return LEVELS[envLogLevel] == null ? logLevel : LEVELS[envLogLevel]\n}\n\nconst createLogger = ({ level = LEVELS.INFO, logCreator } = {}) => {\n  let logLevel = evaluateLogLevel(level)\n  const logFunction = logCreator(logLevel)\n\n  const createNamespace = (namespace, logLevel = null) => {\n    const namespaceLogLevel = evaluateLogLevel(logLevel)\n    return createLogFunctions(namespace, namespaceLogLevel)\n  }\n\n  const createLogFunctions = (namespace, namespaceLogLevel = null) => {\n    const currentLogLevel = () => (namespaceLogLevel == null ? logLevel : namespaceLogLevel)\n    const logger = {\n      info: createLevel('INFO', LEVELS.INFO, currentLogLevel, namespace, logFunction),\n      error: createLevel('ERROR', LEVELS.ERROR, currentLogLevel, namespace, logFunction),\n      warn: createLevel('WARN', LEVELS.WARN, currentLogLevel, namespace, logFunction),\n      debug: createLevel('DEBUG', LEVELS.DEBUG, currentLogLevel, namespace, logFunction),\n    }\n\n    return assign(logger, {\n      namespace: createNamespace,\n      setLogLevel: newLevel => {\n        logLevel = newLevel\n      },\n    })\n  }\n\n  return createLogFunctions()\n}\n\nmodule.exports = {\n  LEVELS,\n  createLogger,\n}\n","const createSocket = require('./socket')\nconst createRequest = require('../protocol/request')\nconst Decoder = require('../protocol/decoder')\nconst { KafkaJSConnectionError, KafkaJSConnectionClosedError } = require('../errors')\nconst { INT_32_MAX_VALUE } = require('../constants')\nconst getEnv = require('../env')\nconst RequestQueue = require('./requestQueue')\nconst { CONNECTION_STATUS, CONNECTED_STATUS } = require('./connectionStatus')\n\nconst requestInfo = ({ apiName, apiKey, apiVersion }) =>\n  `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n\nmodule.exports = class Connection {\n  /**\n   * @param {Object} options\n   * @param {string} options.host\n   * @param {number} options.port\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n   * @param {string} [options.clientId='kafkajs']\n   * @param {number} options.requestTimeout The maximum amount of time the client will wait for the response of a request,\n   *                                in milliseconds\n   * @param {string} [options.rack=null]\n   * @param {Object} [options.ssl=null] Options for the TLS Secure Context. It accepts all options,\n   *                            usually \"cert\", \"key\" and \"ca\". More information at\n   *                            https://nodejs.org/api/tls.html#tls_tls_createsecurecontext_options\n   * @param {Object} [options.sasl=null] Attributes used for SASL authentication. Options based on the\n   *                             key \"mechanism\". Connection is not actively using the SASL attributes\n   *                             but acting as a data object for this information\n   * @param {number} [options.connectionTimeout=1000] The connection timeout, in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {number} [options.maxInFlightRequests=null] The maximum number of unacknowledged requests on a connection before\n   *                                            enqueuing\n   * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    host,\n    port,\n    logger,\n    socketFactory,\n    requestTimeout,\n    rack = null,\n    ssl = null,\n    sasl = null,\n    clientId = 'kafkajs',\n    connectionTimeout = 1000,\n    enforceRequestTimeout = false,\n    maxInFlightRequests = null,\n    instrumentationEmitter = null,\n  }) {\n    this.host = host\n    this.port = port\n    this.rack = rack\n    this.clientId = clientId\n    this.broker = `${this.host}:${this.port}`\n    this.logger = logger.namespace('Connection')\n\n    this.socketFactory = socketFactory\n    this.ssl = ssl\n    this.sasl = sasl\n\n    this.requestTimeout = requestTimeout\n    this.connectionTimeout = connectionTimeout\n\n    this.bytesBuffered = 0\n    this.bytesNeeded = Decoder.int32Size()\n    this.chunks = []\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.correlationId = 0\n    this.requestQueue = new RequestQueue({\n      instrumentationEmitter,\n      maxInFlightRequests,\n      requestTimeout,\n      enforceRequestTimeout,\n      clientId,\n      broker: this.broker,\n      logger: logger.namespace('RequestQueue'),\n      isConnected: () => this.connected,\n    })\n\n    this.authHandlers = null\n    this.authExpectResponse = false\n\n    const log = level => (message, extra = {}) => {\n      const logFn = this.logger[level]\n      logFn(message, { broker: this.broker, clientId, ...extra })\n    }\n\n    this.logDebug = log('debug')\n    this.logError = log('error')\n\n    const env = getEnv()\n    this.shouldLogBuffers = env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS === '1'\n    this.shouldLogFetchBuffer =\n      this.shouldLogBuffers && env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS === '1'\n  }\n\n  get connected() {\n    return CONNECTED_STATUS.includes(this.connectionStatus)\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  connect() {\n    return new Promise((resolve, reject) => {\n      if (this.connected) {\n        return resolve(true)\n      }\n\n      let timeoutId\n\n      const onConnect = () => {\n        clearTimeout(timeoutId)\n        this.connectionStatus = CONNECTION_STATUS.CONNECTED\n        this.requestQueue.scheduleRequestTimeoutCheck()\n        resolve(true)\n      }\n\n      const onData = data => {\n        this.processData(data)\n      }\n\n      const onEnd = async () => {\n        clearTimeout(timeoutId)\n\n        const wasConnected = this.connected\n\n        if (this.authHandlers) {\n          this.authHandlers.onError()\n        } else if (wasConnected) {\n          this.logDebug('Kafka server has closed connection')\n          this.rejectRequests(\n            new KafkaJSConnectionClosedError('Closed connection', {\n              host: this.host,\n              port: this.port,\n            })\n          )\n        }\n\n        await this.disconnect()\n      }\n\n      const onError = async e => {\n        clearTimeout(timeoutId)\n\n        const error = new KafkaJSConnectionError(`Connection error: ${e.message}`, {\n          broker: `${this.host}:${this.port}`,\n          code: e.code,\n        })\n\n        this.logError(error.message, { stack: e.stack })\n        this.rejectRequests(error)\n        await this.disconnect()\n\n        reject(error)\n      }\n\n      const onTimeout = async () => {\n        const error = new KafkaJSConnectionError('Connection timeout', {\n          broker: `${this.host}:${this.port}`,\n        })\n\n        this.logError(error.message)\n        this.rejectRequests(error)\n        await this.disconnect()\n        reject(error)\n      }\n\n      this.logDebug(`Connecting`, {\n        ssl: !!this.ssl,\n        sasl: !!this.sasl,\n      })\n\n      try {\n        timeoutId = setTimeout(onTimeout, this.connectionTimeout)\n        this.socket = createSocket({\n          socketFactory: this.socketFactory,\n          host: this.host,\n          port: this.port,\n          ssl: this.ssl,\n          onConnect,\n          onData,\n          onEnd,\n          onError,\n          onTimeout,\n        })\n      } catch (e) {\n        clearTimeout(timeoutId)\n        reject(\n          new KafkaJSConnectionError(`Failed to connect: ${e.message}`, {\n            broker: `${this.host}:${this.port}`,\n          })\n        )\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTING\n    this.logDebug('disconnecting...')\n\n    await this.requestQueue.waitForPendingRequests()\n    this.requestQueue.destroy()\n\n    if (this.socket) {\n      this.socket.end()\n      this.socket.unref()\n    }\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.logDebug('disconnected')\n    return true\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  authenticate({ authExpectResponse = false, request, response }) {\n    this.authExpectResponse = authExpectResponse\n\n    /**\n     * TODO: rewrite removing the async promise executor\n     */\n\n    /* eslint-disable no-async-promise-executor */\n    return new Promise(async (resolve, reject) => {\n      this.authHandlers = {\n        onSuccess: rawData => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          response\n            .decode(rawData)\n            .then(data => response.parse(data))\n            .then(resolve)\n            .catch(reject)\n        },\n        onError: () => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          reject(\n            new KafkaJSConnectionError('Connection closed by the server', {\n              broker: `${this.host}:${this.port}`,\n            })\n          )\n        },\n      }\n\n      try {\n        const requestPayload = await request.encode()\n\n        this.failIfNotConnected()\n        this.socket.write(requestPayload.buffer, 'binary')\n      } catch (e) {\n        reject(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} protocol\n   * @param {object} protocol.request It is defined by the protocol and consists of an object with \"apiKey\",\n   *                         \"apiVersion\", \"apiName\" and an \"encode\" function. The encode function\n   *                         must return an instance of Encoder\n   *\n   * @param {object} protocol.response It is defined by the protocol and consists of an object with two functions:\n   *                          \"decode\" and \"parse\"\n   *\n   * @param {number} [protocol.requestTimeout=null] Override for the default requestTimeout\n   * @param {boolean} [protocol.logResponseError=true] Whether to log errors\n   * @returns {Promise<data>} where data is the return of \"response#parse\"\n   */\n  async send({ request, response, requestTimeout = null, logResponseError = true }) {\n    this.failIfNotConnected()\n\n    const expectResponse = !request.expectResponse || request.expectResponse()\n    const sendRequest = async () => {\n      const { clientId } = this\n      const correlationId = this.nextCorrelationId()\n\n      const requestPayload = await createRequest({ request, correlationId, clientId })\n      const { apiKey, apiName, apiVersion } = request\n      this.logDebug(`Request ${requestInfo(request)}`, {\n        correlationId,\n        expectResponse,\n        size: Buffer.byteLength(requestPayload.buffer),\n      })\n\n      return new Promise((resolve, reject) => {\n        try {\n          this.failIfNotConnected()\n          const entry = { apiKey, apiName, apiVersion, correlationId, resolve, reject }\n\n          this.requestQueue.push({\n            entry,\n            expectResponse,\n            requestTimeout,\n            sendRequest: () => {\n              this.socket.write(requestPayload.buffer, 'binary')\n            },\n          })\n        } catch (e) {\n          reject(e)\n        }\n      })\n    }\n\n    const { correlationId, size, entry, payload } = await sendRequest()\n\n    if (!expectResponse) {\n      return\n    }\n\n    try {\n      const payloadDecoded = await response.decode(payload)\n\n      /**\n       * @see KIP-219\n       * If the response indicates that the client-side needs to throttle, do that.\n       */\n      this.requestQueue.maybeThrottle(payloadDecoded.clientSideThrottleTime)\n\n      const data = await response.parse(payloadDecoded)\n      const isFetchApi = entry.apiName === 'Fetch'\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        correlationId,\n        size,\n        data: isFetchApi && !this.shouldLogFetchBuffer ? '[filtered]' : data,\n      })\n\n      return data\n    } catch (e) {\n      if (logResponseError) {\n        this.logError(`Response ${requestInfo(entry)}`, {\n          error: e.message,\n          correlationId,\n          size,\n        })\n      }\n\n      const isBuffer = Buffer.isBuffer(payload)\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        error: e.message,\n        correlationId,\n        payload:\n          isBuffer && !this.shouldLogBuffers ? { type: 'Buffer', data: '[filtered]' } : payload,\n      })\n\n      throw e\n    }\n  }\n\n  /**\n   * @private\n   */\n  failIfNotConnected() {\n    if (!this.connected) {\n      throw new KafkaJSConnectionError('Not connected', {\n        broker: `${this.host}:${this.port}`,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  nextCorrelationId() {\n    if (this.correlationId >= INT_32_MAX_VALUE) {\n      this.correlationId = 0\n    }\n\n    return this.correlationId++\n  }\n\n  /**\n   * @private\n   */\n  processData(rawData) {\n    if (this.authHandlers && !this.authExpectResponse) {\n      return this.authHandlers.onSuccess(rawData)\n    }\n\n    // Accumulate the new chunk\n    this.chunks.push(rawData)\n    this.bytesBuffered += Buffer.byteLength(rawData)\n\n    // Process data if there are enough bytes to read the expected response size,\n    // otherwise keep buffering\n    while (this.bytesNeeded <= this.bytesBuffered) {\n      const buffer = this.chunks.length > 1 ? Buffer.concat(this.chunks) : this.chunks[0]\n      const decoder = new Decoder(buffer)\n      const expectedResponseSize = decoder.readInt32()\n\n      // Return early if not enough bytes to read the full response\n      if (!decoder.canReadBytes(expectedResponseSize)) {\n        this.chunks = [buffer]\n        this.bytesBuffered = Buffer.byteLength(buffer)\n        this.bytesNeeded = Decoder.int32Size() + expectedResponseSize\n        return\n      }\n\n      const response = new Decoder(decoder.readBytes(expectedResponseSize))\n\n      // Reset the buffered chunks as the rest of the bytes\n      const remainderBuffer = decoder.readAll()\n      this.chunks = [remainderBuffer]\n      this.bytesBuffered = Buffer.byteLength(remainderBuffer)\n      this.bytesNeeded = Decoder.int32Size()\n\n      if (this.authHandlers) {\n        const rawResponseSize = Decoder.int32Size() + expectedResponseSize\n        const rawResponseBuffer = buffer.slice(0, rawResponseSize)\n        return this.authHandlers.onSuccess(rawResponseBuffer)\n      }\n\n      const correlationId = response.readInt32()\n      const payload = response.readAll()\n\n      this.requestQueue.fulfillRequest({\n        size: expectedResponseSize,\n        correlationId,\n        payload,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  rejectRequests(error) {\n    this.requestQueue.rejectAll(error)\n  }\n}\n","const CONNECTION_STATUS = {\n  CONNECTED: 'connected',\n  DISCONNECTING: 'disconnecting',\n  DISCONNECTED: 'disconnected',\n}\n\nconst CONNECTED_STATUS = [CONNECTION_STATUS.CONNECTED, CONNECTION_STATUS.DISCONNECTING]\n\nmodule.exports = {\n  CONNECTION_STATUS,\n  CONNECTED_STATUS,\n}\n","const InstrumentationEventType = require('../instrumentation/eventType')\nconst eventType = InstrumentationEventType('network')\n\nmodule.exports = {\n  NETWORK_REQUEST: eventType('request'),\n  NETWORK_REQUEST_TIMEOUT: eventType('request_timeout'),\n  NETWORK_REQUEST_QUEUE_SIZE: eventType('request_queue_size'),\n}\n","const { EventEmitter } = require('events')\nconst SocketRequest = require('./socketRequest')\nconst events = require('../instrumentationEvents')\nconst { KafkaJSInvariantViolation } = require('../../errors')\n\nconst PRIVATE = {\n  EMIT_QUEUE_SIZE_EVENT: Symbol('private:RequestQueue:emitQueueSizeEvent'),\n  EMIT_REQUEST_QUEUE_EMPTY: Symbol('private:RequestQueue:emitQueueEmpty'),\n}\n\nconst REQUEST_QUEUE_EMPTY = 'requestQueueEmpty'\n\nmodule.exports = class RequestQueue extends EventEmitter {\n  /**\n   * @param {Object} options\n   * @param {number} options.maxInFlightRequests\n   * @param {number} options.requestTimeout\n   * @param {boolean} options.enforceRequestTimeout\n   * @param {string} options.clientId\n   * @param {string} options.broker\n   * @param {import(\"../../../types\").Logger} options.logger\n   * @param {import(\"../../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   * @param {() => boolean} [options.isConnected]\n   */\n  constructor({\n    instrumentationEmitter = null,\n    maxInFlightRequests,\n    requestTimeout,\n    enforceRequestTimeout,\n    clientId,\n    broker,\n    logger,\n    isConnected = () => true,\n  }) {\n    super()\n    this.instrumentationEmitter = instrumentationEmitter\n    this.maxInFlightRequests = maxInFlightRequests\n    this.requestTimeout = requestTimeout\n    this.enforceRequestTimeout = enforceRequestTimeout\n    this.clientId = clientId\n    this.broker = broker\n    this.logger = logger\n    this.isConnected = isConnected\n\n    this.inflight = new Map()\n    this.pending = []\n\n    /**\n     * Until when this request queue is throttled and shouldn't send requests\n     *\n     * The value represents the timestamp of the end of the throttling in ms-since-epoch. If the value\n     * is smaller than the current timestamp no throttling is active.\n     *\n     * @type {number}\n     */\n    this.throttledUntil = -1\n\n    /**\n     * Timeout id if we have scheduled a check for pending requests due to client-side throttling\n     *\n     * @type {null|NodeJS.Timeout}\n     */\n    this.throttleCheckTimeoutId = null\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY] = () => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        this.emit(REQUEST_QUEUE_EMPTY)\n      }\n    }\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT] = () => {\n      instrumentationEmitter &&\n        instrumentationEmitter.emit(events.NETWORK_REQUEST_QUEUE_SIZE, {\n          broker: this.broker,\n          clientId: this.clientId,\n          queueSize: this.pending.length,\n        })\n\n      this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n    }\n  }\n\n  /**\n   * @public\n   */\n  scheduleRequestTimeoutCheck() {\n    if (this.enforceRequestTimeout) {\n      this.destroy()\n\n      this.requestTimeoutIntervalId = setInterval(() => {\n        this.inflight.forEach(request => {\n          if (Date.now() - request.sentAt > request.requestTimeout) {\n            request.timeoutRequest()\n          }\n        })\n\n        if (!this.isConnected()) {\n          this.destroy()\n        }\n      }, Math.min(this.requestTimeout, 100))\n    }\n  }\n\n  maybeThrottle(clientSideThrottleTime) {\n    if (clientSideThrottleTime) {\n      const minimumThrottledUntil = Date.now() + clientSideThrottleTime\n      this.throttledUntil = Math.max(minimumThrottledUntil, this.throttledUntil)\n    }\n  }\n\n  /**\n   * @typedef {Object} PushedRequest\n   * @property {import(\"./socketRequest\").RequestEntry} entry\n   * @property {boolean} expectResponse\n   * @property {Function} sendRequest\n   * @property {number} [requestTimeout]\n   *\n   * @public\n   * @param {PushedRequest} pushedRequest\n   */\n  push(pushedRequest) {\n    const { correlationId } = pushedRequest.entry\n    const defaultRequestTimeout = this.requestTimeout\n    const customRequestTimeout = pushedRequest.requestTimeout\n\n    // Some protocol requests have custom request timeouts (e.g JoinGroup, Fetch, etc). The custom\n    // timeouts are influenced by user configurations, which can be lower than the default requestTimeout\n    const requestTimeout = Math.max(defaultRequestTimeout, customRequestTimeout || 0)\n\n    const socketRequest = new SocketRequest({\n      entry: pushedRequest.entry,\n      expectResponse: pushedRequest.expectResponse,\n      broker: this.broker,\n      clientId: this.clientId,\n      instrumentationEmitter: this.instrumentationEmitter,\n      requestTimeout,\n      send: () => {\n        if (this.inflight.has(correlationId)) {\n          throw new KafkaJSInvariantViolation('Correlation id already exists')\n        }\n        this.inflight.set(correlationId, socketRequest)\n        pushedRequest.sendRequest()\n      },\n      timeout: () => {\n        this.inflight.delete(correlationId)\n        this.checkPendingRequests()\n        // Try to emit REQUEST_QUEUE_EMPTY. Otherwise, waitForPendingRequests may stuck forever\n        this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n      },\n    })\n\n    if (this.canSendSocketRequestImmediately()) {\n      this.sendSocketRequest(socketRequest)\n      return\n    }\n\n    this.pending.push(socketRequest)\n    this.scheduleCheckPendingRequests()\n\n    this.logger.debug(`Request enqueued`, {\n      clientId: this.clientId,\n      broker: this.broker,\n      correlationId,\n    })\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @param {SocketRequest} socketRequest\n   */\n  sendSocketRequest(socketRequest) {\n    socketRequest.send()\n\n    if (!socketRequest.expectResponse) {\n      this.logger.debug(`Request does not expect a response, resolving immediately`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: socketRequest.correlationId,\n      })\n\n      this.inflight.delete(socketRequest.correlationId)\n      socketRequest.completed({ size: 0, payload: null })\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} response\n   * @param {number} response.correlationId\n   * @param {Buffer} response.payload\n   * @param {number} response.size\n   */\n  fulfillRequest({ correlationId, payload, size }) {\n    const socketRequest = this.inflight.get(correlationId)\n    this.inflight.delete(correlationId)\n    this.checkPendingRequests()\n\n    if (socketRequest) {\n      socketRequest.completed({ size, payload })\n    } else {\n      this.logger.warn(`Response without match`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId,\n      })\n    }\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n  }\n\n  /**\n   * @public\n   * @param {Error} error\n   */\n  rejectAll(error) {\n    const requests = [...this.inflight.values(), ...this.pending]\n\n    for (const socketRequest of requests) {\n      socketRequest.rejected(error)\n      this.inflight.delete(socketRequest.correlationId)\n    }\n\n    this.pending = []\n    this.inflight.clear()\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @public\n   */\n  waitForPendingRequests() {\n    return new Promise(resolve => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        return resolve()\n      }\n\n      this.logger.debug('Waiting for pending requests', {\n        clientId: this.clientId,\n        broker: this.broker,\n        currentInflightRequests: this.inflight.size,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this.once(REQUEST_QUEUE_EMPTY, () => resolve())\n    })\n  }\n\n  /**\n   * @public\n   */\n  destroy() {\n    clearInterval(this.requestTimeoutIntervalId)\n    clearTimeout(this.throttleCheckTimeoutId)\n    this.throttleCheckTimeoutId = null\n  }\n\n  canSendSocketRequestImmediately() {\n    const shouldEnqueue =\n      (this.maxInFlightRequests != null && this.inflight.size >= this.maxInFlightRequests) ||\n      this.throttledUntil > Date.now()\n\n    return !shouldEnqueue\n  }\n\n  /**\n   * Check and process pending requests either now or in the future\n   *\n   * This function will send out as many pending requests as possible taking throttling and\n   * in-flight limits into account.\n   */\n  checkPendingRequests() {\n    while (this.pending.length > 0 && this.canSendSocketRequestImmediately()) {\n      const pendingRequest = this.pending.shift() // first in first out\n      this.sendSocketRequest(pendingRequest)\n\n      this.logger.debug(`Consumed pending request`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: pendingRequest.correlationId,\n        pendingDuration: pendingRequest.pendingDuration,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n    }\n\n    this.scheduleCheckPendingRequests()\n  }\n\n  /**\n   * Ensure that pending requests will be checked in the future\n   *\n   * If there is a client-side throttling in place this will ensure that we will check\n   * the pending request queue eventually.\n   */\n  scheduleCheckPendingRequests() {\n    // If we're throttled: Schedule checkPendingRequests when the throttle\n    // should be resolved. If there is already something scheduled we assume that that\n    // will be fine, and potentially fix up a new timeout if needed at that time.\n    // Note that if we're merely \"overloaded\" by having too many inflight requests\n    // we will anyways check the queue when one of them gets fulfilled.\n    const timeUntilUnthrottled = this.throttledUntil - Date.now()\n    if (timeUntilUnthrottled > 0 && !this.throttleCheckTimeoutId) {\n      this.throttleCheckTimeoutId = setTimeout(() => {\n        this.throttleCheckTimeoutId = null\n        this.checkPendingRequests()\n      }, timeUntilUnthrottled)\n    }\n  }\n}\n","const { KafkaJSRequestTimeoutError, KafkaJSNonRetriableError } = require('../../errors')\nconst events = require('../instrumentationEvents')\n\nconst PRIVATE = {\n  STATE: Symbol('private:SocketRequest:state'),\n  EMIT_EVENT: Symbol('private:SocketRequest:emitEvent'),\n}\n\nconst REQUEST_STATE = {\n  PENDING: Symbol('PENDING'),\n  SENT: Symbol('SENT'),\n  COMPLETED: Symbol('COMPLETED'),\n  REJECTED: Symbol('REJECTED'),\n}\n\n/**\n * SocketRequest abstracts the life cycle of a socket request, making it easier to track\n * request durations and to have individual timeouts per request.\n *\n * @typedef {Object} SocketRequest\n * @property {number} createdAt\n * @property {number} sentAt\n * @property {number} pendingDuration\n * @property {number} duration\n * @property {number} requestTimeout\n * @property {string} broker\n * @property {string} clientId\n * @property {RequestEntry} entry\n * @property {boolean} expectResponse\n * @property {Function} send\n * @property {Function} timeout\n *\n * @typedef {Object} RequestEntry\n * @property {string} apiKey\n * @property {string} apiName\n * @property {number} apiVersion\n * @property {number} correlationId\n * @property {Function} resolve\n * @property {Function} reject\n */\nmodule.exports = class SocketRequest {\n  /**\n   * @param {Object} options\n   * @param {number} options.requestTimeout\n   * @param {string} options.broker - e.g: 127.0.0.1:9092\n   * @param {string} options.clientId\n   * @param {RequestEntry} options.entry\n   * @param {boolean} options.expectResponse\n   * @param {Function} options.send\n   * @param {() => void} options.timeout\n   * @param {import(\"../../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    requestTimeout,\n    broker,\n    clientId,\n    entry,\n    expectResponse,\n    send,\n    timeout,\n    instrumentationEmitter = null,\n  }) {\n    this.createdAt = Date.now()\n    this.requestTimeout = requestTimeout\n    this.broker = broker\n    this.clientId = clientId\n    this.entry = entry\n    this.correlationId = entry.correlationId\n    this.expectResponse = expectResponse\n    this.sendRequest = send\n    this.timeoutHandler = timeout\n\n    this.sentAt = null\n    this.duration = null\n    this.pendingDuration = null\n\n    this[PRIVATE.STATE] = REQUEST_STATE.PENDING\n    this[PRIVATE.EMIT_EVENT] = (eventName, payload) =>\n      instrumentationEmitter && instrumentationEmitter.emit(eventName, payload)\n  }\n\n  send() {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING],\n      next: REQUEST_STATE.SENT,\n    })\n\n    this.sendRequest()\n    this.sentAt = Date.now()\n    this.pendingDuration = this.sentAt - this.createdAt\n    this[PRIVATE.STATE] = REQUEST_STATE.SENT\n  }\n\n  timeoutRequest() {\n    const { apiName, apiKey, apiVersion } = this.entry\n    const requestInfo = `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n    const eventData = {\n      broker: this.broker,\n      clientId: this.clientId,\n      correlationId: this.correlationId,\n      createdAt: this.createdAt,\n      sentAt: this.sentAt,\n      pendingDuration: this.pendingDuration,\n    }\n\n    this.timeoutHandler()\n    this.rejected(new KafkaJSRequestTimeoutError(`Request ${requestInfo} timed out`, eventData))\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST_TIMEOUT, {\n      ...eventData,\n      apiName,\n      apiKey,\n      apiVersion,\n    })\n  }\n\n  completed({ size, payload }) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.SENT],\n      next: REQUEST_STATE.COMPLETED,\n    })\n\n    const { entry, correlationId, broker, clientId, createdAt, sentAt, pendingDuration } = this\n\n    this[PRIVATE.STATE] = REQUEST_STATE.COMPLETED\n    this.duration = Date.now() - this.sentAt\n    entry.resolve({ correlationId, entry, size, payload })\n\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST, {\n      broker,\n      clientId,\n      correlationId,\n      size,\n      createdAt,\n      sentAt,\n      pendingDuration,\n      duration: this.duration,\n      apiName: entry.apiName,\n      apiKey: entry.apiKey,\n      apiVersion: entry.apiVersion,\n    })\n  }\n\n  rejected(error) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING, REQUEST_STATE.SENT],\n      next: REQUEST_STATE.REJECTED,\n    })\n\n    this[PRIVATE.STATE] = REQUEST_STATE.REJECTED\n    this.duration = Date.now() - this.sentAt\n    this.entry.reject(error)\n  }\n\n  /**\n   * @private\n   */\n  throwIfInvalidState({ accepted, next }) {\n    if (accepted.includes(this[PRIVATE.STATE])) {\n      return\n    }\n\n    const current = this[PRIVATE.STATE].toString()\n\n    throw new KafkaJSNonRetriableError(\n      `Invalid state, can't transition from ${current} to ${next.toString()}`\n    )\n  }\n}\n","/**\n * @param {Object} options\n * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n * @param {string} options.host\n * @param {number} options.port\n * @param {Object} options.ssl\n * @param {() => void} options.onConnect\n * @param {(data: Buffer) => void} options.onData\n * @param {() => void} options.onEnd\n * @param {(err: Error) => void} options.onError\n * @param {() => void} options.onTimeout\n */\nmodule.exports = ({\n  socketFactory,\n  host,\n  port,\n  ssl,\n  onConnect,\n  onData,\n  onEnd,\n  onError,\n  onTimeout,\n}) => {\n  const socket = socketFactory({ host, port, ssl, onConnect })\n\n  socket.on('data', onData)\n  socket.on('end', onEnd)\n  socket.on('error', onError)\n  socket.on('timeout', onTimeout)\n\n  return socket\n}\n","const KEEP_ALIVE_DELAY = 60000 // in ms\n\n/**\n * @returns {import(\"../../types\").ISocketFactory}\n */\nmodule.exports = () => {\n  const net = require('net')\n  const tls = require('tls')\n\n  return ({ host, port, ssl, onConnect }) => {\n    const socket = ssl\n      ? tls.connect(Object.assign({ host, port, servername: host }, ssl), onConnect)\n      : net.connect({ host, port }, onConnect)\n\n    socket.setKeepAlive(true, KEEP_ALIVE_DELAY)\n\n    return socket\n  }\n}\n","module.exports = topicDataForBroker => {\n  return topicDataForBroker.map(\n    ({ topic, partitions, messagesPerPartition, sequencePerPartition }) => ({\n      topic,\n      partitions: partitions.map(partition => ({\n        partition,\n        firstSequence: sequencePerPartition[partition],\n        messages: messagesPerPartition[partition],\n      })),\n    })\n  )\n}\n","const createRetry = require('../../retry')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst COORDINATOR_TYPES = require('../../protocol/coordinatorTypes')\nconst createStateMachine = require('./transactionStateMachine')\nconst assert = require('assert')\n\nconst STATES = require('./transactionStates')\nconst NO_PRODUCER_ID = -1\nconst SEQUENCE_START = 0\nconst INT_32_MAX_VALUE = Math.pow(2, 32)\nconst INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS = [\n  'NOT_COORDINATOR_FOR_GROUP',\n  'GROUP_COORDINATOR_NOT_AVAILABLE',\n  'GROUP_LOAD_IN_PROGRESS',\n  /**\n   * The producer might have crashed and never committed the transaction; retry the\n   * request so Kafka can abort the current transaction\n   * @see https://github.com/apache/kafka/blob/201da0542726472d954080d54bc585b111aaf86f/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L1001-L1002\n   */\n  'CONCURRENT_TRANSACTIONS',\n]\nconst COMMIT_RETRIABLE_PROTOCOL_ERRORS = [\n  'UNKNOWN_TOPIC_OR_PARTITION',\n  'COORDINATOR_LOAD_IN_PROGRESS',\n]\nconst COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS = ['COORDINATOR_NOT_AVAILABLE', 'NOT_COORDINATOR']\n\n/**\n * @typedef {Object} EosManager\n */\n\n/**\n * Manage behavior for an idempotent producer and transactions.\n *\n * @returns {EosManager}\n */\nmodule.exports = ({\n  logger,\n  cluster,\n  transactionTimeout = 60000,\n  transactional,\n  transactionalId,\n}) => {\n  if (transactional && !transactionalId) {\n    throw new KafkaJSNonRetriableError('Cannot manage transactions without a transactionalId')\n  }\n\n  const retrier = createRetry(cluster.retry)\n\n  /**\n   * Current producer ID\n   */\n  let producerId = NO_PRODUCER_ID\n\n  /**\n   * Current producer epoch\n   */\n  let producerEpoch = 0\n\n  /**\n   * Idempotent production requires that the producer track the sequence number of messages.\n   *\n   * Sequences are sent with every Record Batch and tracked per Topic-Partition\n   */\n  let producerSequence = {}\n\n  /**\n   * Topic partitions already participating in the transaction\n   */\n  let transactionTopicPartitions = {}\n\n  const stateMachine = createStateMachine({ logger })\n  stateMachine.on('transition', ({ to }) => {\n    if (to === STATES.READY) {\n      transactionTopicPartitions = {}\n    }\n  })\n\n  const findTransactionCoordinator = () => {\n    return cluster.findGroupCoordinator({\n      groupId: transactionalId,\n      coordinatorType: COORDINATOR_TYPES.TRANSACTION,\n    })\n  }\n\n  const transactionalGuard = () => {\n    if (!transactional) {\n      throw new KafkaJSNonRetriableError('Method unavailable if non-transactional')\n    }\n  }\n\n  const eosManager = stateMachine.createGuarded(\n    {\n      /**\n       * Get the current producer id\n       * @returns {number}\n       */\n      getProducerId() {\n        return producerId\n      },\n\n      /**\n       * Get the current producer epoch\n       * @returns {number}\n       */\n      getProducerEpoch() {\n        return producerEpoch\n      },\n\n      getTransactionalId() {\n        return transactionalId\n      },\n\n      /**\n       * Initialize the idempotent producer by making an `InitProducerId` request.\n       * Overwrites any existing state in this transaction manager\n       */\n      async initProducerId() {\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await cluster.refreshMetadataIfNecessary()\n\n            // If non-transactional we can request the PID from any broker\n            const broker = await (transactional\n              ? findTransactionCoordinator()\n              : cluster.findControllerBroker())\n\n            const result = await broker.initProducerId({\n              transactionalId: transactional ? transactionalId : undefined,\n              transactionTimeout,\n            })\n\n            stateMachine.transitionTo(STATES.READY)\n            producerId = result.producerId\n            producerEpoch = result.producerEpoch\n            producerSequence = {}\n\n            logger.debug('Initialized producer id & epoch', { producerId, producerEpoch })\n          } catch (e) {\n            if (INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              if (e.type === 'CONCURRENT_TRANSACTIONS') {\n                logger.debug('There is an ongoing transaction on this transactionId, retrying', {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                })\n              }\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n\n      /**\n       * Get the current sequence for a given Topic-Partition. Defaults to 0.\n       *\n       * @param {string} topic\n       * @param {string} partition\n       * @returns {number}\n       */\n      getSequence(topic, partition) {\n        if (!eosManager.isInitialized()) {\n          return SEQUENCE_START\n        }\n\n        producerSequence[topic] = producerSequence[topic] || {}\n        producerSequence[topic][partition] = producerSequence[topic][partition] || SEQUENCE_START\n\n        return producerSequence[topic][partition]\n      },\n\n      /**\n       * Update the sequence for a given Topic-Partition.\n       *\n       * Do nothing if not yet initialized (not idempotent)\n       * @param {string} topic\n       * @param {string} partition\n       * @param {number} increment\n       */\n      updateSequence(topic, partition, increment) {\n        if (!eosManager.isInitialized()) {\n          return\n        }\n\n        const previous = eosManager.getSequence(topic, partition)\n        let sequence = previous + increment\n\n        // Sequence is defined as Int32 in the Record Batch,\n        // so theoretically should need to rotate here\n        if (sequence >= INT_32_MAX_VALUE) {\n          logger.debug(\n            `Sequence for ${topic} ${partition} exceeds max value (${sequence}). Rotating to 0.`\n          )\n          sequence = 0\n        }\n\n        producerSequence[topic][partition] = sequence\n      },\n\n      /**\n       * Begin a transaction\n       */\n      beginTransaction() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.TRANSACTING)\n      },\n\n      /**\n       * Add partitions to a transaction if they are not already marked as participating.\n       *\n       * Should be called prior to sending any messages during a transaction\n       * @param {TopicData[]} topicData\n       *\n       * @typedef {Object} TopicData\n       * @property {string} topic\n       * @property {object[]} partitions\n       * @property {number} partitions[].partition\n       */\n      async addPartitionsToTransaction(topicData) {\n        transactionalGuard()\n        const newTopicPartitions = {}\n\n        topicData.forEach(({ topic, partitions }) => {\n          transactionTopicPartitions[topic] = transactionTopicPartitions[topic] || {}\n\n          partitions.forEach(({ partition }) => {\n            if (!transactionTopicPartitions[topic][partition]) {\n              newTopicPartitions[topic] = newTopicPartitions[topic] || []\n              newTopicPartitions[topic].push(partition)\n            }\n          })\n        })\n\n        const topics = Object.keys(newTopicPartitions).map(topic => ({\n          topic,\n          partitions: newTopicPartitions[topic],\n        }))\n\n        if (topics.length) {\n          const broker = await findTransactionCoordinator()\n          await broker.addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n        }\n\n        topics.forEach(({ topic, partitions }) => {\n          partitions.forEach(partition => {\n            transactionTopicPartitions[topic][partition] = true\n          })\n        })\n      },\n\n      /**\n       * Commit the ongoing transaction\n       */\n      async commit() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.COMMITTING)\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: true,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Abort the ongoing transaction\n       */\n      async abort() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.ABORTING)\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: false,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Whether the producer id has already been initialized\n       */\n      isInitialized() {\n        return producerId !== NO_PRODUCER_ID\n      },\n\n      isTransactional() {\n        return transactional\n      },\n\n      isInTransaction() {\n        return stateMachine.state() === STATES.TRANSACTING\n      },\n\n      /**\n       * Mark the provided offsets as participating in the transaction for the given consumer group.\n       *\n       * This allows us to commit an offset as consumed only if the transaction passes.\n       * @param {string} consumerGroupId The unique group identifier\n       * @param {OffsetCommitTopic[]} topics The unique group identifier\n       * @returns {Promise}\n       *\n       * @typedef {Object} OffsetCommitTopic\n       * @property {string} topic\n       * @property {OffsetCommitTopicPartition[]} partitions\n       *\n       * @typedef {Object} OffsetCommitTopicPartition\n       * @property {number} partition\n       * @property {number} offset\n       */\n      async sendOffsets({ consumerGroupId, topics }) {\n        assert(consumerGroupId, 'Missing consumerGroupId')\n        assert(topics, 'Missing offset topics')\n\n        const transactionCoordinator = await findTransactionCoordinator()\n\n        // Do we need to add offsets if we've already done so for this consumer group?\n        await transactionCoordinator.addOffsetsToTxn({\n          transactionalId,\n          producerId,\n          producerEpoch,\n          groupId: consumerGroupId,\n        })\n\n        let groupCoordinator = await cluster.findGroupCoordinator({\n          groupId: consumerGroupId,\n          coordinatorType: COORDINATOR_TYPES.GROUP,\n        })\n\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await groupCoordinator.txnOffsetCommit({\n              transactionalId,\n              producerId,\n              producerEpoch,\n              groupId: consumerGroupId,\n              topics,\n            })\n          } catch (e) {\n            if (COMMIT_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              logger.debug('Group coordinator is not ready yet, retrying', {\n                error: e.message,\n                stack: e.stack,\n                transactionalId,\n                retryCount,\n                retryTime,\n              })\n\n              throw e\n            }\n\n            if (\n              COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS.includes(e.type) ||\n              e.code === 'ECONNREFUSED'\n            ) {\n              logger.debug(\n                'Invalid group coordinator, finding new group coordinator and retrying',\n                {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                }\n              )\n\n              groupCoordinator = await cluster.findGroupCoordinator({\n                groupId: consumerGroupId,\n                coordinatorType: COORDINATOR_TYPES.GROUP,\n              })\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n    },\n\n    /**\n     * Transaction state guards\n     */\n    {\n      initProducerId: { legalStates: [STATES.UNINITIALIZED, STATES.READY] },\n      beginTransaction: { legalStates: [STATES.READY], async: false },\n      addPartitionsToTransaction: { legalStates: [STATES.TRANSACTING] },\n      sendOffsets: { legalStates: [STATES.TRANSACTING] },\n      commit: { legalStates: [STATES.TRANSACTING] },\n      abort: { legalStates: [STATES.TRANSACTING] },\n    }\n  )\n\n  return eosManager\n}\n","const { EventEmitter } = require('events')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst STATES = require('./transactionStates')\n\nconst VALID_STATE_TRANSITIONS = {\n  [STATES.UNINITIALIZED]: [STATES.READY],\n  [STATES.READY]: [STATES.READY, STATES.TRANSACTING],\n  [STATES.TRANSACTING]: [STATES.COMMITTING, STATES.ABORTING],\n  [STATES.COMMITTING]: [STATES.READY],\n  [STATES.ABORTING]: [STATES.READY],\n}\n\nmodule.exports = ({ logger, initialState = STATES.UNINITIALIZED }) => {\n  let currentState = initialState\n\n  const guard = (object, method, { legalStates, async: isAsync = true }) => {\n    if (!object[method]) {\n      throw new KafkaJSNonRetriableError(`Cannot add guard on missing method \"${method}\"`)\n    }\n\n    return (...args) => {\n      const fn = object[method]\n\n      if (!legalStates.includes(currentState)) {\n        const error = new KafkaJSNonRetriableError(\n          `Transaction state exception: Cannot call \"${method}\" in state \"${currentState}\"`\n        )\n\n        if (isAsync) {\n          return Promise.reject(error)\n        } else {\n          throw error\n        }\n      }\n\n      return fn.apply(object, args)\n    }\n  }\n\n  const stateMachine = Object.assign(new EventEmitter(), {\n    /**\n     * Create a clone of \"object\" where we ensure state machine is in correct state\n     * prior to calling any of the configured methods\n     * @param {Object} object The object whose methods we will guard\n     * @param {Object} methodStateMapping Keys are method names on \"object\"\n     * @param {string[]} methodStateMapping.legalStates Legal states for this method\n     * @param {boolean=true} methodStateMapping.async Whether this method is async (throw vs reject)\n     */\n    createGuarded(object, methodStateMapping) {\n      const guardedMethods = Object.keys(methodStateMapping).reduce((guards, method) => {\n        guards[method] = guard(object, method, methodStateMapping[method])\n        return guards\n      }, {})\n\n      return { ...object, ...guardedMethods }\n    },\n    /**\n     * Transition safely to a new state\n     */\n    transitionTo(state) {\n      logger.debug(`Transaction state transition ${currentState} --> ${state}`)\n\n      if (!VALID_STATE_TRANSITIONS[currentState].includes(state)) {\n        throw new KafkaJSNonRetriableError(\n          `Transaction state exception: Invalid transition ${currentState} --> ${state}`\n        )\n      }\n\n      stateMachine.emit('transition', { to: state, from: currentState })\n      currentState = state\n    },\n\n    state() {\n      return currentState\n    },\n  })\n\n  return stateMachine\n}\n","module.exports = {\n  UNINITIALIZED: 'UNINITIALIZED',\n  READY: 'READY',\n  TRANSACTING: 'TRANSACTING',\n  COMMITTING: 'COMMITTING',\n  ABORTING: 'ABORTING',\n}\n","module.exports = ({ topic, partitionMetadata, messages, partitioner }) => {\n  if (partitionMetadata.length === 0) {\n    return {}\n  }\n\n  return messages.reduce((result, message) => {\n    const partition = partitioner({ topic, partitionMetadata, message })\n    const current = result[partition] || []\n    return Object.assign(result, { [partition]: [...current, message] })\n  }, {})\n}\n","const createRetry = require('../retry')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\nconst { DefaultPartitioner } = require('./partitioners/')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst createEosManager = require('./eosManager')\nconst createMessageProducer = require('./messageProducer')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { KafkaJSNonRetriableError } = require('../errors')\n\nconst { values, keys } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `producer.events.${key}`)\n  .join(', ')\n\nconst { CONNECT, DISCONNECT } = events\n\n/**\n *\n * @param {Object} params\n * @param {import('../../types').Cluster} params.cluster\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').ICustomPartitioner} [params.createPartitioner]\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {boolean} [params.idempotent]\n * @param {string} [params.transactionalId]\n * @param {number} [params.transactionTimeout]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n *\n * @returns {import('../../types').Producer}\n */\nmodule.exports = ({\n  cluster,\n  logger: rootLogger,\n  createPartitioner = DefaultPartitioner,\n  retry,\n  idempotent = false,\n  transactionalId,\n  transactionTimeout,\n  instrumentationEmitter: rootInstrumentationEmitter,\n}) => {\n  let connectionStatus = CONNECTION_STATUS.DISCONNECTED\n  retry = retry || { retries: idempotent ? Number.MAX_SAFE_INTEGER : 5 }\n\n  if (idempotent && retry.retries < 1) {\n    throw new KafkaJSNonRetriableError(\n      'Idempotent producer must allow retries to protect against transient errors'\n    )\n  }\n\n  const logger = rootLogger.namespace('Producer')\n\n  if (idempotent && retry.retries < Number.MAX_SAFE_INTEGER) {\n    logger.warn('Limiting retries for the idempotent producer may invalidate EoS guarantees')\n  }\n\n  const partitioner = createPartitioner()\n  const retrier = createRetry(Object.assign({}, cluster.retry, retry))\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const idempotentEosManager = createEosManager({\n    logger,\n    cluster,\n    transactionTimeout,\n    transactional: false,\n    transactionalId,\n  })\n\n  const { send, sendBatch } = createMessageProducer({\n    logger,\n    cluster,\n    partitioner,\n    eosManager: idempotentEosManager,\n    idempotent,\n    retrier,\n    getConnectionStatus: () => connectionStatus,\n  })\n\n  let transactionalEosManager\n\n  /** @type {import(\"../../types\").Producer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * Begin a transaction. The returned object contains methods to send messages\n   * to the transaction and end the transaction by committing or aborting.\n   *\n   * Only messages sent on the transaction object will participate in the transaction.\n   *\n   * Calling any of the transactional methods after the transaction has ended\n   * will raise an exception (use `isActive` to ascertain if ended).\n   * @returns {Promise<Transaction>}\n   *\n   * @typedef {Object} Transaction\n   * @property {Function} send  Identical to the producer \"send\" method\n   * @property {Function} sendBatch Identical to the producer \"sendBatch\" method\n   * @property {Function} abort Abort the transaction\n   * @property {Function} commit  Commit the transaction\n   * @property {Function} isActive  Whether the transaction is active\n   */\n  const transaction = async () => {\n    if (!transactionalId) {\n      throw new KafkaJSNonRetriableError('Must provide transactional id for transactional producer')\n    }\n\n    let transactionDidEnd = false\n    transactionalEosManager =\n      transactionalEosManager ||\n      createEosManager({\n        logger,\n        cluster,\n        transactionTimeout,\n        transactional: true,\n        transactionalId,\n      })\n\n    if (transactionalEosManager.isInTransaction()) {\n      throw new KafkaJSNonRetriableError(\n        'There is already an ongoing transaction for this producer. Please end the transaction before beginning another.'\n      )\n    }\n\n    // We only initialize the producer id once\n    if (!transactionalEosManager.isInitialized()) {\n      await transactionalEosManager.initProducerId()\n    }\n    transactionalEosManager.beginTransaction()\n\n    const { send: sendTxn, sendBatch: sendBatchTxn } = createMessageProducer({\n      logger,\n      cluster,\n      partitioner,\n      retrier,\n      eosManager: transactionalEosManager,\n      idempotent: true,\n      getConnectionStatus: () => connectionStatus,\n    })\n\n    const isActive = () => transactionalEosManager.isInTransaction() && !transactionDidEnd\n\n    const transactionGuard = fn => (...args) => {\n      if (!isActive()) {\n        return Promise.reject(\n          new KafkaJSNonRetriableError('Cannot continue to use transaction once ended')\n        )\n      }\n\n      return fn(...args)\n    }\n\n    return {\n      sendBatch: transactionGuard(sendBatchTxn),\n      send: transactionGuard(sendTxn),\n      /**\n       * Abort the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      abort: transactionGuard(async () => {\n        await transactionalEosManager.abort()\n        transactionDidEnd = true\n      }),\n      /**\n       * Commit the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      commit: transactionGuard(async () => {\n        await transactionalEosManager.commit()\n        transactionDidEnd = true\n      }),\n      /**\n       * Sends a list of specified offsets to the consumer group coordinator, and also marks those offsets as part of the current transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      sendOffsets: transactionGuard(async ({ consumerGroupId, topics }) => {\n        await transactionalEosManager.sendOffsets({ consumerGroupId, topics })\n\n        for (const topicOffsets of topics) {\n          const { topic, partitions } = topicOffsets\n          for (const { partition, offset } of partitions) {\n            cluster.markOffsetAsCommitted({\n              groupId: consumerGroupId,\n              topic,\n              partition,\n              offset,\n            })\n          }\n        }\n      }),\n      isActive,\n    }\n  }\n\n  /**\n   * @returns {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    /**\n     * @returns {Promise}\n     */\n    connect: async () => {\n      await cluster.connect()\n      connectionStatus = CONNECTION_STATUS.CONNECTED\n      instrumentationEmitter.emit(CONNECT)\n\n      if (idempotent && !idempotentEosManager.isInitialized()) {\n        await idempotentEosManager.initProducerId()\n      }\n    },\n    /**\n     * @return {Promise}\n     */\n    disconnect: async () => {\n      connectionStatus = CONNECTION_STATUS.DISCONNECTING\n      await cluster.disconnect()\n      connectionStatus = CONNECTION_STATUS.DISCONNECTED\n      instrumentationEmitter.emit(DISCONNECT)\n    },\n    isIdempotent: () => {\n      return idempotent\n    },\n    events,\n    on,\n    send,\n    sendBatch,\n    transaction,\n    logger: getLogger,\n  }\n}\n","const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst producerType = InstrumentationEventType('producer')\n\nconst events = {\n  CONNECT: producerType('connect'),\n  DISCONNECT: producerType('disconnect'),\n  REQUEST: producerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: producerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: producerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n","const createSendMessages = require('./sendMessages')\nconst { KafkaJSError, KafkaJSNonRetriableError } = require('../errors')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\n\nmodule.exports = ({\n  logger,\n  cluster,\n  partitioner,\n  eosManager,\n  idempotent,\n  retrier,\n  getConnectionStatus,\n}) => {\n  const sendMessages = createSendMessages({\n    logger,\n    cluster,\n    retrier,\n    partitioner,\n    eosManager,\n  })\n\n  const validateConnectionStatus = () => {\n    const connectionStatus = getConnectionStatus()\n\n    switch (connectionStatus) {\n      case CONNECTION_STATUS.DISCONNECTING:\n        throw new KafkaJSNonRetriableError(\n          `The producer is disconnecting; therefore, it can't safely accept messages anymore`\n        )\n      case CONNECTION_STATUS.DISCONNECTED:\n        throw new KafkaJSError('The producer is disconnected')\n    }\n  }\n\n  /**\n   * @typedef {Object} TopicMessages\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   *\n   * @typedef {Object} SendBatchRequest\n   * @property {Array<TopicMessages>} topicMessages\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   *\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   *\n   * @param {SendBatchRequest}\n   * @returns {Promise}\n   */\n  const sendBatch = async ({ acks = -1, timeout, compression, topicMessages = [] }) => {\n    if (topicMessages.some(({ topic }) => !topic)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic`)\n    }\n\n    if (idempotent && acks !== -1) {\n      throw new KafkaJSNonRetriableError(\n        `Not requiring ack for all messages invalidates the idempotent producer's EoS guarantees`\n      )\n    }\n\n    for (const { topic, messages } of topicMessages) {\n      if (!messages) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid messages array [${messages}] for topic \"${topic}\"`\n        )\n      }\n\n      const messageWithoutValue = messages.find(message => message.value === undefined)\n      if (messageWithoutValue) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid message without value for topic \"${topic}\": ${JSON.stringify(\n            messageWithoutValue\n          )}`\n        )\n      }\n    }\n\n    validateConnectionStatus()\n    const mergedTopicMessages = topicMessages.reduce((merged, { topic, messages }) => {\n      const index = merged.findIndex(({ topic: mergedTopic }) => topic === mergedTopic)\n\n      if (index === -1) {\n        merged.push({ topic, messages })\n      } else {\n        merged[index].messages = [...merged[index].messages, ...messages]\n      }\n\n      return merged\n    }, [])\n\n    return await sendMessages({\n      acks,\n      timeout,\n      compression,\n      topicMessages: mergedTopicMessages,\n    })\n  }\n\n  /**\n   * @param {ProduceRequest} ProduceRequest\n   * @returns {Promise}\n   *\n   * @typedef {Object} ProduceRequest\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   */\n  const send = async ({ acks, timeout, compression, topic, messages }) => {\n    const topicMessage = { topic, messages }\n    return sendBatch({\n      acks,\n      timeout,\n      compression,\n      topicMessages: [topicMessage],\n    })\n  }\n\n  return {\n    send,\n    sendBatch,\n  }\n}\n","const murmur2 = require('./murmur2')\nconst createDefaultPartitioner = require('./partitioner')\n\nmodule.exports = createDefaultPartitioner(murmur2)\n","/* eslint-disable */\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = 0x9747b28c\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = 0x5bd1e995\nconst R = 24\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = SEED ^ length\n  let length4 = length / 4\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k *= M\n    k ^= k >>> R\n    k *= M\n    h *= M\n    h ^= k\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h ^= (data[(length & ~3) + 2] & 0xff) << 16\n    case 2:\n      h ^= (data[(length & ~3) + 1] & 0xff) << 8\n    case 1:\n      h ^= data[length & ~3] & 0xff\n      h *= M\n  }\n\n  h ^= h >>> 13\n  h *= M\n  h ^= h >>> 15\n\n  return h\n}\n","const randomBytes = require('./randomBytes')\n\n// Based on the java client 0.10.2\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java\n\n/**\n * A cheap way to deterministically convert a number to a positive value. When the input is\n * positive, the original value is returned. When the input number is negative, the returned\n * positive value is the original value bit AND against 0x7fffffff which is not its absolutely\n * value.\n */\nconst toPositive = x => x & 0x7fffffff\n\n/**\n * The default partitioning strategy:\n *  - If a partition is specified in the message, use it\n *  - If no partition is specified but a key is present choose a partition based on a hash of the key\n *  - If no partition or key is present choose a partition in a round-robin fashion\n */\nmodule.exports = murmur2 => () => {\n  const counters = {}\n\n  return ({ topic, partitionMetadata, message }) => {\n    if (!(topic in counters)) {\n      counters[topic] = randomBytes(32).readUInt32BE(0)\n    }\n    const numPartitions = partitionMetadata.length\n    const availablePartitions = partitionMetadata.filter(p => p.leader >= 0)\n    const numAvailablePartitions = availablePartitions.length\n\n    if (message.partition !== null && message.partition !== undefined) {\n      return message.partition\n    }\n\n    if (message.key !== null && message.key !== undefined) {\n      return toPositive(murmur2(message.key)) % numPartitions\n    }\n\n    if (numAvailablePartitions > 0) {\n      const i = toPositive(++counters[topic]) % numAvailablePartitions\n      return availablePartitions[i].partitionId\n    }\n\n    // no partitions are available, give a non-available partition\n    return toPositive(++counters[topic]) % numPartitions\n  }\n}\n","const { KafkaJSNonRetriableError } = require('../../../errors')\n\nconst toNodeCompatible = crypto => ({\n  randomBytes: size => crypto.getRandomValues(Buffer.allocUnsafe(size)),\n})\n\nlet cryptoImplementation = null\nif (global && global.crypto) {\n  cryptoImplementation =\n    global.crypto.randomBytes === undefined ? toNodeCompatible(global.crypto) : global.crypto\n} else if (global && global.msCrypto) {\n  cryptoImplementation = toNodeCompatible(global.msCrypto)\n} else if (global && !global.crypto) {\n  cryptoImplementation = require('crypto')\n}\n\nconst MAX_BYTES = 65536\n\nmodule.exports = size => {\n  if (size > MAX_BYTES) {\n    throw new KafkaJSNonRetriableError(\n      `Byte length (${size}) exceeds the max number of bytes of entropy available (${MAX_BYTES})`\n    )\n  }\n\n  if (!cryptoImplementation) {\n    throw new KafkaJSNonRetriableError('No available crypto implementation')\n  }\n\n  return cryptoImplementation.randomBytes(size)\n}\n","const murmur2 = require('./murmur2')\nconst createDefaultPartitioner = require('../default/partitioner')\n\nmodule.exports = createDefaultPartitioner(murmur2)\n","/* eslint-disable */\nconst Long = require('../../../utils/long')\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = Long.fromValue(0x9747b28c)\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = Long.fromValue(0x5bd1e995)\nconst R = Long.fromValue(24)\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = Long.fromValue(SEED.xor(length))\n  let length4 = Math.floor(length / 4)\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k = Long.fromValue(k)\n    k = k.multiply(M)\n    k = k.xor(k.toInt() >>> R)\n    k = Long.fromValue(k).multiply(M)\n    h = h.multiply(M)\n    h = h.xor(k)\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h = h.xor((data[(length & ~3) + 2] & 0xff) << 16)\n    case 2:\n      h = h.xor((data[(length & ~3) + 1] & 0xff) << 8)\n    case 1:\n      h = h.xor(data[length & ~3] & 0xff)\n      h = h.multiply(M)\n  }\n\n  h = h.xor(h.toInt() >>> 13)\n  h = h.multiply(M)\n  h = h.xor(h.toInt() >>> 15)\n\n  return h.toInt()\n}\n","const DefaultPartitioner = require('./default')\nconst JavaCompatiblePartitioner = require('./defaultJava')\n\nmodule.exports = {\n  DefaultPartitioner,\n  JavaCompatiblePartitioner,\n}\n","const flatten = require('../utils/flatten')\n\nmodule.exports = ({ topics }) => {\n  const partitions = topics.map(({ topicName, partitions }) =>\n    partitions.map(partition => ({ topicName, ...partition }))\n  )\n\n  return flatten(partitions)\n}\n","const flatten = require('../utils/flatten')\nconst { KafkaJSMetadataNotLoaded } = require('../errors')\nconst { staleMetadata } = require('../protocol/error')\nconst groupMessagesPerPartition = require('./groupMessagesPerPartition')\nconst createTopicData = require('./createTopicData')\nconst responseSerializer = require('./responseSerializer')\n\nconst { keys } = Object\n\n/**\n * @param {Object} options\n * @param {import(\"../../types\").Logger} options.logger\n * @param {import(\"../../types\").Cluster} options.cluster\n * @param {ReturnType<import(\"../../types\").ICustomPartitioner>} options.partitioner\n * @param {import(\"./eosManager\").EosManager} options.eosManager\n * @param {import(\"../retry\").Retrier} options.retrier\n */\nmodule.exports = ({ logger, cluster, partitioner, eosManager, retrier }) => {\n  return async ({ acks, timeout, compression, topicMessages }) => {\n    /** @type {Map<import(\"../../types\").Broker, any[]>} */\n    const responsePerBroker = new Map()\n\n    /** @param {Map<import(\"../../types\").Broker, any[]>} responsePerBroker */\n    const createProducerRequests = async responsePerBroker => {\n      const topicMetadata = new Map()\n\n      await cluster.refreshMetadataIfNecessary()\n\n      for (const { topic, messages } of topicMessages) {\n        const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n\n        if (partitionMetadata.length === 0) {\n          logger.debug('Producing to topic without metadata', {\n            topic,\n            targetTopics: Array.from(cluster.targetTopics),\n          })\n\n          throw new KafkaJSMetadataNotLoaded('Producing to topic without metadata')\n        }\n\n        const messagesPerPartition = groupMessagesPerPartition({\n          topic,\n          partitionMetadata,\n          messages,\n          partitioner,\n        })\n\n        const partitions = keys(messagesPerPartition)\n        const sequencePerPartition = partitions.reduce((result, partition) => {\n          result[partition] = eosManager.getSequence(topic, partition)\n          return result\n        }, {})\n\n        const partitionsPerLeader = cluster.findLeaderForPartitions(topic, partitions)\n        const leaders = keys(partitionsPerLeader)\n\n        topicMetadata.set(topic, {\n          partitionsPerLeader,\n          messagesPerPartition,\n          sequencePerPartition,\n        })\n\n        for (const nodeId of leaders) {\n          const broker = await cluster.findBroker({ nodeId })\n          if (!responsePerBroker.has(broker)) {\n            responsePerBroker.set(broker, null)\n          }\n        }\n      }\n\n      const brokers = Array.from(responsePerBroker.keys())\n      const brokersWithoutResponse = brokers.filter(broker => !responsePerBroker.get(broker))\n\n      return brokersWithoutResponse.map(async broker => {\n        const entries = Array.from(topicMetadata.entries())\n        const topicDataForBroker = entries\n          .filter(([_, { partitionsPerLeader }]) => !!partitionsPerLeader[broker.nodeId])\n          .map(([topic, { partitionsPerLeader, messagesPerPartition, sequencePerPartition }]) => ({\n            topic,\n            partitions: partitionsPerLeader[broker.nodeId],\n            sequencePerPartition,\n            messagesPerPartition,\n          }))\n\n        const topicData = createTopicData(topicDataForBroker)\n\n        try {\n          if (eosManager.isTransactional()) {\n            await eosManager.addPartitionsToTransaction(topicData)\n          }\n\n          const response = await broker.produce({\n            transactionalId: eosManager.isTransactional()\n              ? eosManager.getTransactionalId()\n              : undefined,\n            producerId: eosManager.getProducerId(),\n            producerEpoch: eosManager.getProducerEpoch(),\n            acks,\n            timeout,\n            compression,\n            topicData,\n          })\n\n          const expectResponse = acks !== 0\n          const formattedResponse = expectResponse ? responseSerializer(response) : []\n\n          formattedResponse.forEach(({ topicName, partition }) => {\n            const increment = topicMetadata.get(topicName).messagesPerPartition[partition].length\n\n            eosManager.updateSequence(topicName, partition, increment)\n          })\n\n          responsePerBroker.set(broker, formattedResponse)\n        } catch (e) {\n          responsePerBroker.delete(broker)\n          throw e\n        }\n      })\n    }\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      const topics = topicMessages.map(({ topic }) => topic)\n      await cluster.addMultipleTargetTopics(topics)\n\n      try {\n        const requests = await createProducerRequests(responsePerBroker)\n        await Promise.all(requests)\n        const responses = Array.from(responsePerBroker.values())\n        return flatten(responses)\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionClosedError') {\n          cluster.removeBroker({ host: e.host, port: e.port })\n        }\n\n        if (!cluster.isConnected()) {\n          logger.debug(`Cluster has disconnected, reconnecting: ${e.message}`, {\n            retryCount,\n            retryTime,\n          })\n          await cluster.connect()\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        // This is necessary in case the metadata is stale and the number of partitions\n        // for this topic has increased in the meantime\n        if (\n          staleMetadata(e) ||\n          e.name === 'KafkaJSMetadataNotLoaded' ||\n          e.name === 'KafkaJSConnectionError' ||\n          e.name === 'KafkaJSConnectionClosedError' ||\n          (e.name === 'KafkaJSProtocolError' && e.retriable)\n        ) {\n          logger.error(`Failed to send messages: ${e.message}`, { retryCount, retryTime })\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        logger.error(`${e.message}`, { retryCount, retryTime })\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n}\n","// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclOperation.java#L44\n\n/**\n * @typedef {number} ACLOperationTypes\n *\n * Enum for ACL Operations Types\n * @readonly\n * @enum {ACLOperationTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any AclOperation which this client cannot understand, perhaps because this\n   * client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any AclOperation.\n   */\n  ANY: 1,\n  /**\n   * ALL operation.\n   */\n  ALL: 2,\n  /**\n   * READ operation.\n   */\n  READ: 3,\n  /**\n   * WRITE operation.\n   */\n  WRITE: 4,\n  /**\n   * CREATE operation.\n   */\n  CREATE: 5,\n  /**\n   * DELETE operation.\n   */\n  DELETE: 6,\n  /**\n   * ALTER operation.\n   */\n  ALTER: 7,\n  /**\n   * DESCRIBE operation.\n   */\n  DESCRIBE: 8,\n  /**\n   * CLUSTER_ACTION operation.\n   */\n  CLUSTER_ACTION: 9,\n  /**\n   * DESCRIBE_CONFIGS operation.\n   */\n  DESCRIBE_CONFIGS: 10,\n  /**\n   * ALTER_CONFIGS operation.\n   */\n  ALTER_CONFIGS: 11,\n  /**\n   * IDEMPOTENT_WRITE operation.\n   */\n  IDEMPOTENT_WRITE: 12,\n}\n","// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclPermissionType.java/#L31\n\n/**\n * @typedef {number} ACLPermissionTypes\n *\n * Enum for Permission Types\n * @readonly\n * @enum {ACLPermissionTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any AclPermissionType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any AclPermissionType.\n   */\n  ANY: 1,\n  /**\n   * Disallows access.\n   */\n  DENY: 2,\n  /**\n   * Grants access.\n   */\n  ALLOW: 3,\n}\n","/**\n * @see https://github.com/apache/kafka/blob/a15387f34d142684859c2a57fcbef25edcdce25a/clients/src/main/java/org/apache/kafka/common/resource/ResourceType.java#L25-L31\n * @typedef {number} ACLResourceTypes\n *\n * Enum for ACL Resource Types\n * @readonly\n * @enum {ACLResourceTypes}\n */\n\nmodule.exports = {\n  /**\n   * Represents any ResourceType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any ResourceType.\n   */\n  ANY: 1,\n  /**\n   * A Kafka topic.\n   * @see http://kafka.apache.org/documentation/#topicconfigs\n   */\n  TOPIC: 2,\n  /**\n   * A consumer group.\n   * @see http://kafka.apache.org/documentation/#consumerconfigs\n   */\n  GROUP: 3,\n  /**\n   * The cluster as a whole.\n   */\n  CLUSTER: 4,\n  /**\n   * A transactional ID.\n   */\n  TRANSACTIONAL_ID: 5,\n  /**\n   * A token ID.\n   */\n  DELEGATION_TOKEN: 6,\n}\n","/**\n * @see https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/config/ConfigResource.java\n */\nmodule.exports = {\n  UNKNOWN: 0,\n  TOPIC: 2,\n  BROKER: 4,\n  BROKER_LOGGER: 8,\n}\n","/**\n * @see https://github.com/apache/kafka/blob/1f240ce1793cab09e1c4823e17436d2b030df2bc/clients/src/main/java/org/apache/kafka/common/requests/DescribeConfigsResponse.java#L115-L122\n */\nmodule.exports = {\n  UNKNOWN: 0,\n  TOPIC_CONFIG: 1,\n  DYNAMIC_BROKER_CONFIG: 2,\n  DYNAMIC_DEFAULT_BROKER_CONFIG: 3,\n  STATIC_BROKER_CONFIG: 4,\n  DEFAULT_CONFIG: 5,\n  DYNAMIC_BROKER_LOGGER_CONFIG: 6,\n}\n","// From: https://kafka.apache.org/protocol.html#The_Messages_FindCoordinator\n\n/**\n * @typedef {number} CoordinatorType\n *\n * Enum for the types of coordinator to find.\n * @enum {CoordinatorType}\n */\nmodule.exports = {\n  GROUP: 0,\n  TRANSACTION: 1,\n}\n","// Based on https://github.com/brianloveswords/buffer-crc32/blob/master/index.js\n\nvar CRC_TABLE = new Int32Array([\n  0x00000000,\n  0x77073096,\n  0xee0e612c,\n  0x990951ba,\n  0x076dc419,\n  0x706af48f,\n  0xe963a535,\n  0x9e6495a3,\n  0x0edb8832,\n  0x79dcb8a4,\n  0xe0d5e91e,\n  0x97d2d988,\n  0x09b64c2b,\n  0x7eb17cbd,\n  0xe7b82d07,\n  0x90bf1d91,\n  0x1db71064,\n  0x6ab020f2,\n  0xf3b97148,\n  0x84be41de,\n  0x1adad47d,\n  0x6ddde4eb,\n  0xf4d4b551,\n  0x83d385c7,\n  0x136c9856,\n  0x646ba8c0,\n  0xfd62f97a,\n  0x8a65c9ec,\n  0x14015c4f,\n  0x63066cd9,\n  0xfa0f3d63,\n  0x8d080df5,\n  0x3b6e20c8,\n  0x4c69105e,\n  0xd56041e4,\n  0xa2677172,\n  0x3c03e4d1,\n  0x4b04d447,\n  0xd20d85fd,\n  0xa50ab56b,\n  0x35b5a8fa,\n  0x42b2986c,\n  0xdbbbc9d6,\n  0xacbcf940,\n  0x32d86ce3,\n  0x45df5c75,\n  0xdcd60dcf,\n  0xabd13d59,\n  0x26d930ac,\n  0x51de003a,\n  0xc8d75180,\n  0xbfd06116,\n  0x21b4f4b5,\n  0x56b3c423,\n  0xcfba9599,\n  0xb8bda50f,\n  0x2802b89e,\n  0x5f058808,\n  0xc60cd9b2,\n  0xb10be924,\n  0x2f6f7c87,\n  0x58684c11,\n  0xc1611dab,\n  0xb6662d3d,\n  0x76dc4190,\n  0x01db7106,\n  0x98d220bc,\n  0xefd5102a,\n  0x71b18589,\n  0x06b6b51f,\n  0x9fbfe4a5,\n  0xe8b8d433,\n  0x7807c9a2,\n  0x0f00f934,\n  0x9609a88e,\n  0xe10e9818,\n  0x7f6a0dbb,\n  0x086d3d2d,\n  0x91646c97,\n  0xe6635c01,\n  0x6b6b51f4,\n  0x1c6c6162,\n  0x856530d8,\n  0xf262004e,\n  0x6c0695ed,\n  0x1b01a57b,\n  0x8208f4c1,\n  0xf50fc457,\n  0x65b0d9c6,\n  0x12b7e950,\n  0x8bbeb8ea,\n  0xfcb9887c,\n  0x62dd1ddf,\n  0x15da2d49,\n  0x8cd37cf3,\n  0xfbd44c65,\n  0x4db26158,\n  0x3ab551ce,\n  0xa3bc0074,\n  0xd4bb30e2,\n  0x4adfa541,\n  0x3dd895d7,\n  0xa4d1c46d,\n  0xd3d6f4fb,\n  0x4369e96a,\n  0x346ed9fc,\n  0xad678846,\n  0xda60b8d0,\n  0x44042d73,\n  0x33031de5,\n  0xaa0a4c5f,\n  0xdd0d7cc9,\n  0x5005713c,\n  0x270241aa,\n  0xbe0b1010,\n  0xc90c2086,\n  0x5768b525,\n  0x206f85b3,\n  0xb966d409,\n  0xce61e49f,\n  0x5edef90e,\n  0x29d9c998,\n  0xb0d09822,\n  0xc7d7a8b4,\n  0x59b33d17,\n  0x2eb40d81,\n  0xb7bd5c3b,\n  0xc0ba6cad,\n  0xedb88320,\n  0x9abfb3b6,\n  0x03b6e20c,\n  0x74b1d29a,\n  0xead54739,\n  0x9dd277af,\n  0x04db2615,\n  0x73dc1683,\n  0xe3630b12,\n  0x94643b84,\n  0x0d6d6a3e,\n  0x7a6a5aa8,\n  0xe40ecf0b,\n  0x9309ff9d,\n  0x0a00ae27,\n  0x7d079eb1,\n  0xf00f9344,\n  0x8708a3d2,\n  0x1e01f268,\n  0x6906c2fe,\n  0xf762575d,\n  0x806567cb,\n  0x196c3671,\n  0x6e6b06e7,\n  0xfed41b76,\n  0x89d32be0,\n  0x10da7a5a,\n  0x67dd4acc,\n  0xf9b9df6f,\n  0x8ebeeff9,\n  0x17b7be43,\n  0x60b08ed5,\n  0xd6d6a3e8,\n  0xa1d1937e,\n  0x38d8c2c4,\n  0x4fdff252,\n  0xd1bb67f1,\n  0xa6bc5767,\n  0x3fb506dd,\n  0x48b2364b,\n  0xd80d2bda,\n  0xaf0a1b4c,\n  0x36034af6,\n  0x41047a60,\n  0xdf60efc3,\n  0xa867df55,\n  0x316e8eef,\n  0x4669be79,\n  0xcb61b38c,\n  0xbc66831a,\n  0x256fd2a0,\n  0x5268e236,\n  0xcc0c7795,\n  0xbb0b4703,\n  0x220216b9,\n  0x5505262f,\n  0xc5ba3bbe,\n  0xb2bd0b28,\n  0x2bb45a92,\n  0x5cb36a04,\n  0xc2d7ffa7,\n  0xb5d0cf31,\n  0x2cd99e8b,\n  0x5bdeae1d,\n  0x9b64c2b0,\n  0xec63f226,\n  0x756aa39c,\n  0x026d930a,\n  0x9c0906a9,\n  0xeb0e363f,\n  0x72076785,\n  0x05005713,\n  0x95bf4a82,\n  0xe2b87a14,\n  0x7bb12bae,\n  0x0cb61b38,\n  0x92d28e9b,\n  0xe5d5be0d,\n  0x7cdcefb7,\n  0x0bdbdf21,\n  0x86d3d2d4,\n  0xf1d4e242,\n  0x68ddb3f8,\n  0x1fda836e,\n  0x81be16cd,\n  0xf6b9265b,\n  0x6fb077e1,\n  0x18b74777,\n  0x88085ae6,\n  0xff0f6a70,\n  0x66063bca,\n  0x11010b5c,\n  0x8f659eff,\n  0xf862ae69,\n  0x616bffd3,\n  0x166ccf45,\n  0xa00ae278,\n  0xd70dd2ee,\n  0x4e048354,\n  0x3903b3c2,\n  0xa7672661,\n  0xd06016f7,\n  0x4969474d,\n  0x3e6e77db,\n  0xaed16a4a,\n  0xd9d65adc,\n  0x40df0b66,\n  0x37d83bf0,\n  0xa9bcae53,\n  0xdebb9ec5,\n  0x47b2cf7f,\n  0x30b5ffe9,\n  0xbdbdf21c,\n  0xcabac28a,\n  0x53b39330,\n  0x24b4a3a6,\n  0xbad03605,\n  0xcdd70693,\n  0x54de5729,\n  0x23d967bf,\n  0xb3667a2e,\n  0xc4614ab8,\n  0x5d681b02,\n  0x2a6f2b94,\n  0xb40bbe37,\n  0xc30c8ea1,\n  0x5a05df1b,\n  0x2d02ef8d,\n])\n\nmodule.exports = encoder => {\n  const { buffer } = encoder\n  const l = buffer.length\n  let crc = -1\n  for (let n = 0; n < l; n++) {\n    crc = CRC_TABLE[(crc ^ buffer[n]) & 0xff] ^ (crc >>> 8)\n  }\n  return crc ^ -1\n}\n","const { KafkaJSInvalidVarIntError, KafkaJSInvalidLongError } = require('../errors')\nconst Long = require('../utils/long')\n\nconst INT8_SIZE = 1\nconst INT16_SIZE = 2\nconst INT32_SIZE = 4\nconst INT64_SIZE = 8\nconst DOUBLE_SIZE = 8\n\nconst MOST_SIGNIFICANT_BIT = 0x80 // 128\nconst OTHER_BITS = 0x7f // 127\n\nmodule.exports = class Decoder {\n  static int32Size() {\n    return INT32_SIZE\n  }\n\n  static decodeZigZag(value) {\n    return (value >>> 1) ^ -(value & 1)\n  }\n\n  static decodeZigZag64(longValue) {\n    return longValue.shiftRightUnsigned(1).xor(longValue.and(Long.fromInt(1)).negate())\n  }\n\n  constructor(buffer) {\n    this.buffer = buffer\n    this.offset = 0\n  }\n\n  readInt8() {\n    const value = this.buffer.readInt8(this.offset)\n    this.offset += INT8_SIZE\n    return value\n  }\n\n  canReadInt16() {\n    return this.canReadBytes(INT16_SIZE)\n  }\n\n  readInt16() {\n    const value = this.buffer.readInt16BE(this.offset)\n    this.offset += INT16_SIZE\n    return value\n  }\n\n  canReadInt32() {\n    return this.canReadBytes(INT32_SIZE)\n  }\n\n  readInt32() {\n    const value = this.buffer.readInt32BE(this.offset)\n    this.offset += INT32_SIZE\n    return value\n  }\n\n  canReadInt64() {\n    return this.canReadBytes(INT64_SIZE)\n  }\n\n  readInt64() {\n    const first = this.buffer[this.offset]\n    const last = this.buffer[this.offset + 7]\n\n    const low =\n      (first << 24) + // Overflow\n      this.buffer[this.offset + 1] * 2 ** 16 +\n      this.buffer[this.offset + 2] * 2 ** 8 +\n      this.buffer[this.offset + 3]\n    const high =\n      this.buffer[this.offset + 4] * 2 ** 24 +\n      this.buffer[this.offset + 5] * 2 ** 16 +\n      this.buffer[this.offset + 6] * 2 ** 8 +\n      last\n    this.offset += INT64_SIZE\n\n    return (BigInt(low) << 32n) + BigInt(high)\n  }\n\n  readDouble() {\n    const value = this.buffer.readDoubleBE(this.offset)\n    this.offset += DOUBLE_SIZE\n    return value\n  }\n\n  readString() {\n    const byteLength = this.readInt16()\n\n    if (byteLength === -1) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    const value = stringBuffer.toString('utf8')\n    this.offset += byteLength\n    return value\n  }\n\n  readVarIntString() {\n    const byteLength = this.readVarInt()\n\n    if (byteLength === -1) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    const value = stringBuffer.toString('utf8')\n    this.offset += byteLength\n    return value\n  }\n\n  readUVarIntString() {\n    const byteLength = this.readUVarInt()\n\n    if (byteLength === 0) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    const value = stringBuffer.toString('utf8')\n    this.offset += byteLength\n    return value\n  }\n\n  canReadBytes(length) {\n    return Buffer.byteLength(this.buffer) - this.offset >= length\n  }\n\n  readBytes(byteLength = this.readInt32()) {\n    if (byteLength === -1) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    this.offset += byteLength\n    return stringBuffer\n  }\n\n  readVarIntBytes() {\n    const byteLength = this.readVarInt()\n\n    if (byteLength === -1) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    this.offset += byteLength\n    return stringBuffer\n  }\n\n  readUVarIntBytes() {\n    const byteLength = this.readUVarInt()\n\n    if (byteLength === 0) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    this.offset += byteLength\n    return stringBuffer\n  }\n\n  readBoolean() {\n    return this.readInt8() === 1\n  }\n\n  readAll() {\n    const result = this.buffer.slice(this.offset)\n    this.offset += Buffer.byteLength(this.buffer)\n    return result\n  }\n\n  readArray(reader) {\n    const length = this.readInt32()\n\n    if (length === -1) {\n      return []\n    }\n\n    const array = new Array(length)\n    for (let i = 0; i < length; i++) {\n      array[i] = reader(this)\n    }\n\n    return array\n  }\n\n  readVarIntArray(reader) {\n    const length = this.readVarInt()\n\n    if (length === -1) {\n      return []\n    }\n\n    const array = new Array(length)\n    for (let i = 0; i < length; i++) {\n      array[i] = reader(this)\n    }\n\n    return array\n  }\n\n  readUVarIntArray(reader) {\n    const length = this.readUVarInt()\n\n    if (length === 0) {\n      return []\n    }\n\n    const array = new Array(length - 1)\n    for (let i = 0; i < length - 1; i++) {\n      array[i] = reader(this)\n    }\n\n    return array\n  }\n\n  async readArrayAsync(reader) {\n    const length = this.readInt32()\n\n    if (length === -1) {\n      return []\n    }\n\n    const array = new Array(length)\n    for (let i = 0; i < length; i++) {\n      array[i] = await reader(this)\n    }\n\n    return array\n  }\n\n  readVarInt() {\n    let currentByte\n    let result = 0\n    let i = 0\n\n    do {\n      currentByte = this.buffer[this.offset++]\n      result += (currentByte & OTHER_BITS) << i\n      i += 7\n    } while (currentByte >= MOST_SIGNIFICANT_BIT)\n\n    return Decoder.decodeZigZag(result)\n  }\n\n  // By default JavaScript's numbers are of type float64, performing bitwise operations converts the numbers to a signed 32-bit integer\n  // Unsigned Right Shift Operator >>> ensures the returned value is an unsigned 32-bit integer\n  readUVarInt() {\n    let currentByte\n    let result = 0\n    let i = 0\n    while (((currentByte = this.buffer[this.offset++]) & MOST_SIGNIFICANT_BIT) !== 0) {\n      result |= (currentByte & OTHER_BITS) << i\n      i += 7\n      if (i > 28) {\n        throw new KafkaJSInvalidVarIntError('Invalid VarInt, must contain 5 bytes or less')\n      }\n    }\n    result |= currentByte << i\n    return result >>> 0\n  }\n\n  readVarLong() {\n    let currentByte\n    let result = Long.fromInt(0)\n    let i = 0\n\n    do {\n      if (i > 63) {\n        throw new KafkaJSInvalidLongError('Invalid Long, must contain 9 bytes or less')\n      }\n      currentByte = this.buffer[this.offset++]\n      result = result.add(Long.fromInt(currentByte & OTHER_BITS).shiftLeft(i))\n      i += 7\n    } while (currentByte >= MOST_SIGNIFICANT_BIT)\n\n    return Decoder.decodeZigZag64(result)\n  }\n\n  slice(size) {\n    return new Decoder(this.buffer.slice(this.offset, this.offset + size))\n  }\n\n  forward(size) {\n    this.offset += size\n  }\n}\n","const Long = require('../utils/long')\n\nconst INT8_SIZE = 1\nconst INT16_SIZE = 2\nconst INT32_SIZE = 4\nconst INT64_SIZE = 8\nconst DOUBLE_SIZE = 8\n\nconst MOST_SIGNIFICANT_BIT = 0x80 // 128\nconst OTHER_BITS = 0x7f // 127\nconst UNSIGNED_INT32_MAX_NUMBER = 0xffffff80\nconst UNSIGNED_INT64_MAX_NUMBER = 0xffffffffffffff80n\n\nmodule.exports = class Encoder {\n  static encodeZigZag(value) {\n    return (value << 1) ^ (value >> 31)\n  }\n\n  static encodeZigZag64(value) {\n    const longValue = Long.fromValue(value)\n    return longValue.shiftLeft(1).xor(longValue.shiftRight(63))\n  }\n\n  static sizeOfVarInt(value) {\n    let encodedValue = this.encodeZigZag(value)\n    let bytes = 1\n\n    while ((encodedValue & UNSIGNED_INT32_MAX_NUMBER) !== 0) {\n      bytes += 1\n      encodedValue >>>= 7\n    }\n\n    return bytes\n  }\n\n  static sizeOfVarLong(value) {\n    let longValue = Encoder.encodeZigZag64(value)\n    let bytes = 1\n\n    while (longValue.and(UNSIGNED_INT64_MAX_NUMBER).notEquals(Long.fromInt(0))) {\n      bytes += 1\n      longValue = longValue.shiftRightUnsigned(7)\n    }\n\n    return bytes\n  }\n\n  static sizeOfVarIntBytes(value) {\n    const size = value == null ? -1 : Buffer.byteLength(value)\n\n    if (size < 0) {\n      return Encoder.sizeOfVarInt(-1)\n    }\n\n    return Encoder.sizeOfVarInt(size) + size\n  }\n\n  static nextPowerOfTwo(value) {\n    return 1 << (31 - Math.clz32(value) + 1)\n  }\n\n  /**\n   * Construct a new encoder with the given initial size\n   *\n   * @param {number} [initialSize] initial size\n   */\n  constructor(initialSize = 511) {\n    this.buf = Buffer.alloc(Encoder.nextPowerOfTwo(initialSize))\n    this.offset = 0\n  }\n\n  /**\n   * @param {Buffer} buffer\n   */\n  writeBufferInternal(buffer) {\n    const bufferLength = buffer.length\n    this.ensureAvailable(bufferLength)\n    buffer.copy(this.buf, this.offset, 0)\n    this.offset += bufferLength\n  }\n\n  ensureAvailable(length) {\n    if (this.offset + length > this.buf.length) {\n      const newLength = Encoder.nextPowerOfTwo(this.offset + length)\n      const newBuffer = Buffer.alloc(newLength)\n      this.buf.copy(newBuffer, 0, 0, this.offset)\n      this.buf = newBuffer\n    }\n  }\n\n  get buffer() {\n    return this.buf.slice(0, this.offset)\n  }\n\n  writeInt8(value) {\n    this.ensureAvailable(INT8_SIZE)\n    this.buf.writeInt8(value, this.offset)\n    this.offset += INT8_SIZE\n    return this\n  }\n\n  writeInt16(value) {\n    this.ensureAvailable(INT16_SIZE)\n    this.buf.writeInt16BE(value, this.offset)\n    this.offset += INT16_SIZE\n    return this\n  }\n\n  writeInt32(value) {\n    this.ensureAvailable(INT32_SIZE)\n    this.buf.writeInt32BE(value, this.offset)\n    this.offset += INT32_SIZE\n    return this\n  }\n\n  writeUInt32(value) {\n    this.ensureAvailable(INT32_SIZE)\n    this.buf.writeUInt32BE(value, this.offset)\n    this.offset += INT32_SIZE\n    return this\n  }\n\n  writeInt64(value) {\n    this.ensureAvailable(INT64_SIZE)\n    const longValue = Long.fromValue(value)\n    this.buf.writeInt32BE(longValue.getHighBits(), this.offset)\n    this.buf.writeInt32BE(longValue.getLowBits(), this.offset + INT32_SIZE)\n    this.offset += INT64_SIZE\n    return this\n  }\n\n  writeDouble(value) {\n    this.ensureAvailable(DOUBLE_SIZE)\n    this.buf.writeDoubleBE(value, this.offset)\n    this.offset += DOUBLE_SIZE\n    return this\n  }\n\n  writeBoolean(value) {\n    value ? this.writeInt8(1) : this.writeInt8(0)\n    return this\n  }\n\n  writeString(value) {\n    if (value == null) {\n      this.writeInt16(-1)\n      return this\n    }\n\n    const byteLength = Buffer.byteLength(value, 'utf8')\n    this.ensureAvailable(INT16_SIZE + byteLength)\n    this.writeInt16(byteLength)\n    this.buf.write(value, this.offset, byteLength, 'utf8')\n    this.offset += byteLength\n    return this\n  }\n\n  writeVarIntString(value) {\n    if (value == null) {\n      this.writeVarInt(-1)\n      return this\n    }\n\n    const byteLength = Buffer.byteLength(value, 'utf8')\n    this.writeVarInt(byteLength)\n    this.ensureAvailable(byteLength)\n    this.buf.write(value, this.offset, byteLength, 'utf8')\n    this.offset += byteLength\n    return this\n  }\n\n  writeUVarIntString(value) {\n    if (value == null) {\n      this.writeUVarInt(0)\n      return this\n    }\n\n    const byteLength = Buffer.byteLength(value, 'utf8')\n    this.writeUVarInt(byteLength + 1)\n    this.ensureAvailable(byteLength)\n    this.buf.write(value, this.offset, byteLength, 'utf8')\n    this.offset += byteLength\n    return this\n  }\n\n  writeBytes(value) {\n    if (value == null) {\n      this.writeInt32(-1)\n      return this\n    }\n\n    if (Buffer.isBuffer(value)) {\n      // raw bytes\n      this.ensureAvailable(INT32_SIZE + value.length)\n      this.writeInt32(value.length)\n      this.writeBufferInternal(value)\n    } else {\n      const valueToWrite = String(value)\n      const byteLength = Buffer.byteLength(valueToWrite, 'utf8')\n      this.ensureAvailable(INT32_SIZE + byteLength)\n      this.writeInt32(byteLength)\n      this.buf.write(valueToWrite, this.offset, byteLength, 'utf8')\n      this.offset += byteLength\n    }\n\n    return this\n  }\n\n  writeVarIntBytes(value) {\n    if (value == null) {\n      this.writeVarInt(-1)\n      return this\n    }\n\n    if (Buffer.isBuffer(value)) {\n      // raw bytes\n      this.writeVarInt(value.length)\n      this.writeBufferInternal(value)\n    } else {\n      const valueToWrite = String(value)\n      const byteLength = Buffer.byteLength(valueToWrite, 'utf8')\n      this.writeVarInt(byteLength)\n      this.ensureAvailable(byteLength)\n      this.buf.write(valueToWrite, this.offset, byteLength, 'utf8')\n      this.offset += byteLength\n    }\n\n    return this\n  }\n\n  writeUVarIntBytes(value) {\n    if (value == null) {\n      this.writeVarInt(0)\n      return this\n    }\n\n    if (Buffer.isBuffer(value)) {\n      // raw bytes\n      this.writeUVarInt(value.length + 1)\n      this.writeBufferInternal(value)\n    } else {\n      const valueToWrite = String(value)\n      const byteLength = Buffer.byteLength(valueToWrite, 'utf8')\n      this.writeUVarInt(byteLength + 1)\n      this.ensureAvailable(byteLength)\n      this.buf.write(valueToWrite, this.offset, byteLength, 'utf8')\n      this.offset += byteLength\n    }\n\n    return this\n  }\n\n  writeEncoder(value) {\n    if (value == null || !Buffer.isBuffer(value.buf)) {\n      throw new Error('value should be an instance of Encoder')\n    }\n\n    this.writeBufferInternal(value.buffer)\n    return this\n  }\n\n  writeEncoderArray(value) {\n    if (!Array.isArray(value) || value.some(v => v == null || !Buffer.isBuffer(v.buf))) {\n      throw new Error('all values should be an instance of Encoder[]')\n    }\n\n    value.forEach(v => {\n      this.writeBufferInternal(v.buffer)\n    })\n    return this\n  }\n\n  writeBuffer(value) {\n    if (!Buffer.isBuffer(value)) {\n      throw new Error('value should be an instance of Buffer')\n    }\n\n    this.writeBufferInternal(value)\n    return this\n  }\n\n  /**\n   * @param {any[]} array\n   * @param {'int32'|'number'|'string'|'object'} [type]\n   */\n  writeNullableArray(array, type) {\n    // A null value is encoded with length of -1 and there are no following bytes\n    // On the context of this library, empty array and null are the same thing\n    const length = array.length !== 0 ? array.length : -1\n    this.writeArray(array, type, length)\n    return this\n  }\n\n  /**\n   * @param {any[]} array\n   * @param {'int32'|'number'|'string'|'object'} [type]\n   * @param {number} [length]\n   */\n  writeArray(array, type, length) {\n    const arrayLength = length == null ? array.length : length\n    this.writeInt32(arrayLength)\n    if (type !== undefined) {\n      switch (type) {\n        case 'int32':\n        case 'number':\n          array.forEach(value => this.writeInt32(value))\n          break\n        case 'string':\n          array.forEach(value => this.writeString(value))\n          break\n        case 'object':\n          this.writeEncoderArray(array)\n          break\n      }\n    } else {\n      array.forEach(value => {\n        switch (typeof value) {\n          case 'number':\n            this.writeInt32(value)\n            break\n          case 'string':\n            this.writeString(value)\n            break\n          case 'object':\n            this.writeEncoder(value)\n            break\n        }\n      })\n    }\n    return this\n  }\n\n  writeVarIntArray(array, type) {\n    if (type === 'object') {\n      this.writeVarInt(array.length)\n      this.writeEncoderArray(array)\n    } else {\n      const objectArray = array.filter(v => typeof v === 'object')\n      this.writeVarInt(objectArray.length)\n      this.writeEncoderArray(objectArray)\n    }\n    return this\n  }\n\n  writeUVarIntArray(array, type) {\n    if (type === 'object') {\n      this.writeUVarInt(array.length + 1)\n      this.writeEncoderArray(array)\n    } else {\n      const objectArray = array.filter(v => typeof v === 'object')\n      this.writeUVarInt(objectArray.length + 1)\n      this.writeEncoderArray(objectArray)\n    }\n    return this\n  }\n\n  // Based on:\n  // https://en.wikipedia.org/wiki/LEB128 Using LEB128 format similar to VLQ.\n  // https://github.com/addthis/stream-lib/blob/master/src/main/java/com/clearspring/analytics/util/Varint.java#L106\n  writeVarInt(value) {\n    return this.writeUVarInt(Encoder.encodeZigZag(value))\n  }\n\n  writeUVarInt(value) {\n    const byteArray = []\n    while ((value & UNSIGNED_INT32_MAX_NUMBER) !== 0) {\n      byteArray.push((value & OTHER_BITS) | MOST_SIGNIFICANT_BIT)\n      value >>>= 7\n    }\n    byteArray.push(value & OTHER_BITS)\n    this.writeBufferInternal(Buffer.from(byteArray))\n    return this\n  }\n\n  writeVarLong(value) {\n    const byteArray = []\n    let longValue = Encoder.encodeZigZag64(value)\n\n    while (longValue.and(UNSIGNED_INT64_MAX_NUMBER).notEquals(Long.fromInt(0))) {\n      byteArray.push(\n        longValue\n          .and(OTHER_BITS)\n          .or(MOST_SIGNIFICANT_BIT)\n          .toInt()\n      )\n      longValue = longValue.shiftRightUnsigned(7)\n    }\n\n    byteArray.push(longValue.toInt())\n\n    this.writeBufferInternal(Buffer.from(byteArray))\n    return this\n  }\n\n  size() {\n    // We can use the offset here directly, because we anyways will not re-encode the buffer when writing\n    return this.offset\n  }\n\n  toJSON() {\n    return this.buffer.toJSON()\n  }\n}\n","const { KafkaJSProtocolError } = require('../errors')\nconst websiteUrl = require('../utils/websiteUrl')\n\nconst errorCodes = [\n  {\n    type: 'UNKNOWN',\n    code: -1,\n    retriable: false,\n    message: 'The server experienced an unexpected error when processing the request',\n  },\n  {\n    type: 'OFFSET_OUT_OF_RANGE',\n    code: 1,\n    retriable: false,\n    message: 'The requested offset is not within the range of offsets maintained by the server',\n  },\n  {\n    type: 'CORRUPT_MESSAGE',\n    code: 2,\n    retriable: true,\n    message:\n      'This message has failed its CRC checksum, exceeds the valid size, or is otherwise corrupt',\n  },\n  {\n    type: 'UNKNOWN_TOPIC_OR_PARTITION',\n    code: 3,\n    retriable: true,\n    message: 'This server does not host this topic-partition',\n  },\n  {\n    type: 'INVALID_FETCH_SIZE',\n    code: 4,\n    retriable: false,\n    message: 'The requested fetch size is invalid',\n  },\n  {\n    type: 'LEADER_NOT_AVAILABLE',\n    code: 5,\n    retriable: true,\n    message:\n      'There is no leader for this topic-partition as we are in the middle of a leadership election',\n  },\n  {\n    type: 'NOT_LEADER_FOR_PARTITION',\n    code: 6,\n    retriable: true,\n    message: 'This server is not the leader for that topic-partition',\n  },\n  {\n    type: 'REQUEST_TIMED_OUT',\n    code: 7,\n    retriable: true,\n    message: 'The request timed out',\n  },\n  {\n    type: 'BROKER_NOT_AVAILABLE',\n    code: 8,\n    retriable: false,\n    message: 'The broker is not available',\n  },\n  {\n    type: 'REPLICA_NOT_AVAILABLE',\n    code: 9,\n    retriable: false,\n    message: 'The replica is not available for the requested topic-partition',\n  },\n  {\n    type: 'MESSAGE_TOO_LARGE',\n    code: 10,\n    retriable: false,\n    message:\n      'The request included a message larger than the max message size the server will accept',\n  },\n  {\n    type: 'STALE_CONTROLLER_EPOCH',\n    code: 11,\n    retriable: false,\n    message: 'The controller moved to another broker',\n  },\n  {\n    type: 'OFFSET_METADATA_TOO_LARGE',\n    code: 12,\n    retriable: false,\n    message: 'The metadata field of the offset request was too large',\n  },\n  {\n    type: 'NETWORK_EXCEPTION',\n    code: 13,\n    retriable: true,\n    message: 'The server disconnected before a response was received',\n  },\n  {\n    type: 'GROUP_LOAD_IN_PROGRESS',\n    code: 14,\n    retriable: true,\n    message: \"The coordinator is loading and hence can't process requests for this group\",\n  },\n  {\n    type: 'GROUP_COORDINATOR_NOT_AVAILABLE',\n    code: 15,\n    retriable: true,\n    message: 'The group coordinator is not available',\n  },\n  {\n    type: 'NOT_COORDINATOR_FOR_GROUP',\n    code: 16,\n    retriable: true,\n    message: 'This is not the correct coordinator for this group',\n  },\n  {\n    type: 'INVALID_TOPIC_EXCEPTION',\n    code: 17,\n    retriable: false,\n    message: 'The request attempted to perform an operation on an invalid topic',\n  },\n  {\n    type: 'RECORD_LIST_TOO_LARGE',\n    code: 18,\n    retriable: false,\n    message:\n      'The request included message batch larger than the configured segment size on the server',\n  },\n  {\n    type: 'NOT_ENOUGH_REPLICAS',\n    code: 19,\n    retriable: true,\n    message: 'Messages are rejected since there are fewer in-sync replicas than required',\n  },\n  {\n    type: 'NOT_ENOUGH_REPLICAS_AFTER_APPEND',\n    code: 20,\n    retriable: true,\n    message: 'Messages are written to the log, but to fewer in-sync replicas than required',\n  },\n  {\n    type: 'INVALID_REQUIRED_ACKS',\n    code: 21,\n    retriable: false,\n    message: 'Produce request specified an invalid value for required acks',\n  },\n  {\n    type: 'ILLEGAL_GENERATION',\n    code: 22,\n    retriable: false,\n    message: 'Specified group generation id is not valid',\n  },\n  {\n    type: 'INCONSISTENT_GROUP_PROTOCOL',\n    code: 23,\n    retriable: false,\n    message:\n      \"The group member's supported protocols are incompatible with those of existing members\",\n  },\n  {\n    type: 'INVALID_GROUP_ID',\n    code: 24,\n    retriable: false,\n    message: 'The configured groupId is invalid',\n  },\n  {\n    type: 'UNKNOWN_MEMBER_ID',\n    code: 25,\n    retriable: false,\n    message: 'The coordinator is not aware of this member',\n  },\n  {\n    type: 'INVALID_SESSION_TIMEOUT',\n    code: 26,\n    retriable: false,\n    message:\n      'The session timeout is not within the range allowed by the broker (as configured by group.min.session.timeout.ms and group.max.session.timeout.ms)',\n  },\n  {\n    type: 'REBALANCE_IN_PROGRESS',\n    code: 27,\n    retriable: false,\n    message: 'The group is rebalancing, so a rejoin is needed',\n    helpUrl: websiteUrl('docs/faq', 'what-does-it-mean-to-get-rebalance-in-progress-errors'),\n  },\n  {\n    type: 'INVALID_COMMIT_OFFSET_SIZE',\n    code: 28,\n    retriable: false,\n    message: 'The committing offset data size is not valid',\n  },\n  {\n    type: 'TOPIC_AUTHORIZATION_FAILED',\n    code: 29,\n    retriable: false,\n    message: 'Not authorized to access topics: [Topic authorization failed]',\n  },\n  {\n    type: 'GROUP_AUTHORIZATION_FAILED',\n    code: 30,\n    retriable: false,\n    message: 'Not authorized to access group: Group authorization failed',\n  },\n  {\n    type: 'CLUSTER_AUTHORIZATION_FAILED',\n    code: 31,\n    retriable: false,\n    message: 'Cluster authorization failed',\n  },\n  {\n    type: 'INVALID_TIMESTAMP',\n    code: 32,\n    retriable: false,\n    message: 'The timestamp of the message is out of acceptable range',\n  },\n  {\n    type: 'UNSUPPORTED_SASL_MECHANISM',\n    code: 33,\n    retriable: false,\n    message: 'The broker does not support the requested SASL mechanism',\n  },\n  {\n    type: 'ILLEGAL_SASL_STATE',\n    code: 34,\n    retriable: false,\n    message: 'Request is not valid given the current SASL state',\n  },\n  {\n    type: 'UNSUPPORTED_VERSION',\n    code: 35,\n    retriable: false,\n    message: 'The version of API is not supported',\n  },\n  {\n    type: 'TOPIC_ALREADY_EXISTS',\n    code: 36,\n    retriable: false,\n    message: 'Topic with this name already exists',\n  },\n  {\n    type: 'INVALID_PARTITIONS',\n    code: 37,\n    retriable: false,\n    message: 'Number of partitions is invalid',\n  },\n  {\n    type: 'INVALID_REPLICATION_FACTOR',\n    code: 38,\n    retriable: false,\n    message: 'Replication-factor is invalid',\n  },\n  {\n    type: 'INVALID_REPLICA_ASSIGNMENT',\n    code: 39,\n    retriable: false,\n    message: 'Replica assignment is invalid',\n  },\n  {\n    type: 'INVALID_CONFIG',\n    code: 40,\n    retriable: false,\n    message: 'Configuration is invalid',\n  },\n  {\n    type: 'NOT_CONTROLLER',\n    code: 41,\n    retriable: true,\n    message: 'This is not the correct controller for this cluster',\n  },\n  {\n    type: 'INVALID_REQUEST',\n    code: 42,\n    retriable: false,\n    message:\n      'This most likely occurs because of a request being malformed by the client library or the message was sent to an incompatible broker. See the broker logs for more details',\n  },\n  {\n    type: 'UNSUPPORTED_FOR_MESSAGE_FORMAT',\n    code: 43,\n    retriable: false,\n    message: 'The message format version on the broker does not support the request',\n  },\n  {\n    type: 'POLICY_VIOLATION',\n    code: 44,\n    retriable: false,\n    message: 'Request parameters do not satisfy the configured policy',\n  },\n  {\n    type: 'OUT_OF_ORDER_SEQUENCE_NUMBER',\n    code: 45,\n    retriable: false,\n    message: 'The broker received an out of order sequence number',\n  },\n  {\n    type: 'DUPLICATE_SEQUENCE_NUMBER',\n    code: 46,\n    retriable: false,\n    message: 'The broker received a duplicate sequence number',\n  },\n  {\n    type: 'INVALID_PRODUCER_EPOCH',\n    code: 47,\n    retriable: false,\n    message:\n      \"Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker\",\n  },\n  {\n    type: 'INVALID_TXN_STATE',\n    code: 48,\n    retriable: false,\n    message: 'The producer attempted a transactional operation in an invalid state',\n  },\n  {\n    type: 'INVALID_PRODUCER_ID_MAPPING',\n    code: 49,\n    retriable: false,\n    message:\n      'The producer attempted to use a producer id which is not currently assigned to its transactional id',\n  },\n  {\n    type: 'INVALID_TRANSACTION_TIMEOUT',\n    code: 50,\n    retriable: false,\n    message:\n      'The transaction timeout is larger than the maximum value allowed by the broker (as configured by max.transaction.timeout.ms)',\n  },\n  {\n    type: 'CONCURRENT_TRANSACTIONS',\n    code: 51,\n    /**\n     * The concurrent transactions error has \"retriable\" set to false on the protocol documentation (https://kafka.apache.org/protocol.html#protocol_error_codes)\n     * but the server expects the clients to retry. PR #223\n     * @see https://github.com/apache/kafka/blob/12f310d50e7f5b1c18c4f61a119a6cd830da3bc0/core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala#L153\n     */\n    retriable: true,\n    message:\n      'The producer attempted to update a transaction while another concurrent operation on the same transaction was ongoing',\n  },\n  {\n    type: 'TRANSACTION_COORDINATOR_FENCED',\n    code: 52,\n    retriable: false,\n    message:\n      'Indicates that the transaction coordinator sending a WriteTxnMarker is no longer the current coordinator for a given producer',\n  },\n  {\n    type: 'TRANSACTIONAL_ID_AUTHORIZATION_FAILED',\n    code: 53,\n    retriable: false,\n    message: 'Transactional Id authorization failed',\n  },\n  {\n    type: 'SECURITY_DISABLED',\n    code: 54,\n    retriable: false,\n    message: 'Security features are disabled',\n  },\n  {\n    type: 'OPERATION_NOT_ATTEMPTED',\n    code: 55,\n    retriable: false,\n    message:\n      'The broker did not attempt to execute this operation. This may happen for batched RPCs where some operations in the batch failed, causing the broker to respond without trying the rest',\n  },\n  {\n    type: 'KAFKA_STORAGE_ERROR',\n    code: 56,\n    retriable: true,\n    message: 'Disk error when trying to access log file on the disk',\n  },\n  {\n    type: 'LOG_DIR_NOT_FOUND',\n    code: 57,\n    retriable: false,\n    message: 'The user-specified log directory is not found in the broker config',\n  },\n  {\n    type: 'SASL_AUTHENTICATION_FAILED',\n    code: 58,\n    retriable: false,\n    message: 'SASL Authentication failed',\n    helpUrl: websiteUrl('docs/configuration', 'sasl'),\n  },\n  {\n    type: 'UNKNOWN_PRODUCER_ID',\n    code: 59,\n    retriable: false,\n    message:\n      \"This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception\",\n  },\n  {\n    type: 'REASSIGNMENT_IN_PROGRESS',\n    code: 60,\n    retriable: false,\n    message: 'A partition reassignment is in progress',\n  },\n  {\n    type: 'DELEGATION_TOKEN_AUTH_DISABLED',\n    code: 61,\n    retriable: false,\n    message: 'Delegation Token feature is not enabled',\n  },\n  {\n    type: 'DELEGATION_TOKEN_NOT_FOUND',\n    code: 62,\n    retriable: false,\n    message: 'Delegation Token is not found on server',\n  },\n  {\n    type: 'DELEGATION_TOKEN_OWNER_MISMATCH',\n    code: 63,\n    retriable: false,\n    message: 'Specified Principal is not valid Owner/Renewer',\n  },\n  {\n    type: 'DELEGATION_TOKEN_REQUEST_NOT_ALLOWED',\n    code: 64,\n    retriable: false,\n    message:\n      'Delegation Token requests are not allowed on PLAINTEXT/1-way SSL channels and on delegation token authenticated channels',\n  },\n  {\n    type: 'DELEGATION_TOKEN_AUTHORIZATION_FAILED',\n    code: 65,\n    retriable: false,\n    message: 'Delegation Token authorization failed',\n  },\n  {\n    type: 'DELEGATION_TOKEN_EXPIRED',\n    code: 66,\n    retriable: false,\n    message: 'Delegation Token is expired',\n  },\n  {\n    type: 'INVALID_PRINCIPAL_TYPE',\n    code: 67,\n    retriable: false,\n    message: 'Supplied principalType is not supported',\n  },\n  {\n    type: 'NON_EMPTY_GROUP',\n    code: 68,\n    retriable: false,\n    message: 'The group is not empty',\n  },\n  {\n    type: 'GROUP_ID_NOT_FOUND',\n    code: 69,\n    retriable: false,\n    message: 'The group id was not found',\n  },\n  {\n    type: 'FETCH_SESSION_ID_NOT_FOUND',\n    code: 70,\n    retriable: true,\n    message: 'The fetch session ID was not found',\n  },\n  {\n    type: 'INVALID_FETCH_SESSION_EPOCH',\n    code: 71,\n    retriable: true,\n    message: 'The fetch session epoch is invalid',\n  },\n  {\n    type: 'LISTENER_NOT_FOUND',\n    code: 72,\n    retriable: true,\n    message:\n      'There is no listener on the leader broker that matches the listener on which metadata request was processed',\n  },\n  {\n    type: 'TOPIC_DELETION_DISABLED',\n    code: 73,\n    retriable: false,\n    message: 'Topic deletion is disabled',\n  },\n  {\n    type: 'FENCED_LEADER_EPOCH',\n    code: 74,\n    retriable: true,\n    message: 'The leader epoch in the request is older than the epoch on the broker',\n  },\n  {\n    type: 'UNKNOWN_LEADER_EPOCH',\n    code: 75,\n    retriable: true,\n    message: 'The leader epoch in the request is newer than the epoch on the broker',\n  },\n  {\n    type: 'UNSUPPORTED_COMPRESSION_TYPE',\n    code: 76,\n    retriable: false,\n    message: 'The requesting client does not support the compression type of given partition',\n  },\n  {\n    type: 'STALE_BROKER_EPOCH',\n    code: 77,\n    retriable: false,\n    message: 'Broker epoch has changed',\n  },\n  {\n    type: 'OFFSET_NOT_AVAILABLE',\n    code: 78,\n    retriable: true,\n    message:\n      'The leader high watermark has not caught up from a recent leader election so the offsets cannot be guaranteed to be monotonically increasing',\n  },\n  {\n    type: 'MEMBER_ID_REQUIRED',\n    code: 79,\n    retriable: false,\n    message:\n      'The group member needs to have a valid member id before actually entering a consumer group',\n  },\n  {\n    type: 'PREFERRED_LEADER_NOT_AVAILABLE',\n    code: 80,\n    retriable: true,\n    message: 'The preferred leader was not available',\n  },\n  {\n    type: 'GROUP_MAX_SIZE_REACHED',\n    code: 81,\n    retriable: false,\n    message:\n      'The consumer group has reached its max size. It already has the configured maximum number of members',\n  },\n  {\n    type: 'FENCED_INSTANCE_ID',\n    code: 82,\n    retriable: false,\n    message:\n      'The broker rejected this static consumer since another consumer with the same group instance id has registered with a different member id',\n  },\n  {\n    type: 'ELIGIBLE_LEADERS_NOT_AVAILABLE',\n    code: 83,\n    retriable: true,\n    message: 'Eligible topic partition leaders are not available',\n  },\n  {\n    type: 'ELECTION_NOT_NEEDED',\n    code: 84,\n    retriable: true,\n    message: 'Leader election not needed for topic partition',\n  },\n  {\n    type: 'NO_REASSIGNMENT_IN_PROGRESS',\n    code: 85,\n    retriable: false,\n    message: 'No partition reassignment is in progress',\n  },\n  {\n    type: 'GROUP_SUBSCRIBED_TO_TOPIC',\n    code: 86,\n    retriable: false,\n    message:\n      'Deleting offsets of a topic is forbidden while the consumer group is actively subscribed to it',\n  },\n  {\n    type: 'INVALID_RECORD',\n    code: 87,\n    retriable: false,\n    message: 'This record has failed the validation on broker and hence be rejected',\n  },\n  {\n    type: 'UNSTABLE_OFFSET_COMMIT',\n    code: 88,\n    retriable: true,\n    message: 'There are unstable offsets that need to be cleared',\n  },\n]\n\nconst unknownErrorCode = errorCode => ({\n  type: 'KAFKAJS_UNKNOWN_ERROR_CODE',\n  code: -99,\n  retriable: false,\n  message: `Unknown error code ${errorCode}`,\n})\n\nconst SUCCESS_CODE = 0\nconst UNSUPPORTED_VERSION_CODE = 35\n\nconst failure = code => code !== SUCCESS_CODE\nconst createErrorFromCode = code => {\n  return new KafkaJSProtocolError(errorCodes.find(e => e.code === code) || unknownErrorCode(code))\n}\n\nconst failIfVersionNotSupported = code => {\n  if (code === UNSUPPORTED_VERSION_CODE) {\n    throw createErrorFromCode(UNSUPPORTED_VERSION_CODE)\n  }\n}\n\nconst staleMetadata = e =>\n  ['UNKNOWN_TOPIC_OR_PARTITION', 'LEADER_NOT_AVAILABLE', 'NOT_LEADER_FOR_PARTITION'].includes(\n    e.type\n  )\n\nmodule.exports = {\n  failure,\n  errorCodes,\n  createErrorFromCode,\n  failIfVersionNotSupported,\n  staleMetadata,\n}\n","/**\n * Enum for isolation levels\n * @readonly\n * @enum {number}\n */\nmodule.exports = {\n  // Makes all records visible\n  READ_UNCOMMITTED: 0,\n\n  // non-transactional and COMMITTED transactional records are visible. It returns all data\n  // from offsets smaller than the current LSO (last stable offset), and enables the inclusion of\n  // the list of aborted transactions in the result, which allows consumers to discard ABORTED\n  // transactional records\n  READ_COMMITTED: 1,\n}\n","const { promisify } = require('util')\nconst zlib = require('zlib')\n\nconst gzip = promisify(zlib.gzip)\nconst unzip = promisify(zlib.unzip)\n\nmodule.exports = {\n  /**\n   * @param {Encoder} encoder\n   * @returns {Promise}\n   */\n  async compress(encoder) {\n    return await gzip(encoder.buffer)\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Promise}\n   */\n  async decompress(buffer) {\n    return await unzip(buffer)\n  },\n}\n","const { KafkaJSNotImplemented } = require('../../../errors')\n\nconst COMPRESSION_CODEC_MASK = 0x07\n\nconst Types = {\n  None: 0,\n  GZIP: 1,\n  Snappy: 2,\n  LZ4: 3,\n  ZSTD: 4,\n}\n\nconst Codecs = {\n  [Types.GZIP]: () => require('./gzip'),\n  [Types.Snappy]: () => {\n    throw new KafkaJSNotImplemented('Snappy compression not implemented')\n  },\n  [Types.LZ4]: () => {\n    throw new KafkaJSNotImplemented('LZ4 compression not implemented')\n  },\n  [Types.ZSTD]: () => {\n    throw new KafkaJSNotImplemented('ZSTD compression not implemented')\n  },\n}\n\nconst lookupCodec = type => (Codecs[type] ? Codecs[type]() : null)\nconst lookupCodecByAttributes = attributes => {\n  const codec = Codecs[attributes & COMPRESSION_CODEC_MASK]\n  return codec ? codec() : null\n}\n\nmodule.exports = {\n  Types,\n  Codecs,\n  lookupCodec,\n  lookupCodecByAttributes,\n  COMPRESSION_CODEC_MASK,\n}\n","const {\n  KafkaJSPartialMessageError,\n  KafkaJSUnsupportedMagicByteInMessageSet,\n} = require('../../errors')\n\nconst V0Decoder = require('./v0/decoder')\nconst V1Decoder = require('./v1/decoder')\n\nconst decodeMessage = (decoder, magicByte) => {\n  switch (magicByte) {\n    case 0:\n      return V0Decoder(decoder)\n    case 1:\n      return V1Decoder(decoder)\n    default:\n      throw new KafkaJSUnsupportedMagicByteInMessageSet(\n        `Unsupported MessageSet message version, magic byte: ${magicByte}`\n      )\n  }\n}\n\nmodule.exports = (offset, size, decoder) => {\n  // Don't decrement decoder.offset because slice is already considering the current\n  // offset of the decoder\n  const remainingBytes = Buffer.byteLength(decoder.slice(size).buffer)\n\n  if (remainingBytes < size) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial message: remainingBytes(${remainingBytes}) < messageSize(${size})`\n    )\n  }\n\n  const crc = decoder.readInt32()\n  const magicByte = decoder.readInt8()\n  const message = decodeMessage(decoder, magicByte)\n  return Object.assign({ offset, size, crc, magicByte }, message)\n}\n","const versions = {\n  0: require('./v0'),\n  1: require('./v1'),\n}\n\nmodule.exports = ({ version = 0 }) => versions[version]\n","module.exports = decoder => ({\n  attributes: decoder.readInt8(),\n  key: decoder.readBytes(),\n  value: decoder.readBytes(),\n})\n","const Encoder = require('../../encoder')\nconst crc32 = require('../../crc32')\nconst { Types: Compression, COMPRESSION_CODEC_MASK } = require('../compression')\n\n/**\n * v0\n * Message => Crc MagicByte Attributes Key Value\n *   Crc => int32\n *   MagicByte => int8\n *   Attributes => int8\n *   Key => bytes\n *   Value => bytes\n */\n\nmodule.exports = ({ compression = Compression.None, key, value }) => {\n  const content = new Encoder()\n    .writeInt8(0) // magicByte\n    .writeInt8(compression & COMPRESSION_CODEC_MASK)\n    .writeBytes(key)\n    .writeBytes(value)\n\n  const crc = crc32(content)\n  return new Encoder().writeInt32(crc).writeEncoder(content)\n}\n","module.exports = decoder => ({\n  attributes: decoder.readInt8(),\n  timestamp: decoder.readInt64().toString(),\n  key: decoder.readBytes(),\n  value: decoder.readBytes(),\n})\n","const Encoder = require('../../encoder')\nconst crc32 = require('../../crc32')\nconst { Types: Compression, COMPRESSION_CODEC_MASK } = require('../compression')\n\n/**\n * v1 (supported since 0.10.0)\n * Message => Crc MagicByte Attributes Key Value\n *   Crc => int32\n *   MagicByte => int8\n *   Attributes => int8\n *   Timestamp => int64\n *   Key => bytes\n *   Value => bytes\n */\n\nmodule.exports = ({ compression = Compression.None, timestamp = Date.now(), key, value }) => {\n  const content = new Encoder()\n    .writeInt8(1) // magicByte\n    .writeInt8(compression & COMPRESSION_CODEC_MASK)\n    .writeInt64(timestamp)\n    .writeBytes(key)\n    .writeBytes(value)\n\n  const crc = crc32(content)\n  return new Encoder().writeInt32(crc).writeEncoder(content)\n}\n","const Long = require('../../utils/long')\nconst Decoder = require('../decoder')\nconst MessageDecoder = require('../message/decoder')\nconst { lookupCodecByAttributes } = require('../message/compression')\nconst { KafkaJSPartialMessageError } = require('../../errors')\n\n/**\n * MessageSet => [Offset MessageSize Message]\n *  Offset => int64\n *  MessageSize => int32\n *  Message => Bytes\n */\n\nmodule.exports = async (primaryDecoder, size = null) => {\n  const messages = []\n  const messageSetSize = size || primaryDecoder.readInt32()\n  const messageSetDecoder = primaryDecoder.slice(messageSetSize)\n\n  while (messageSetDecoder.offset < messageSetSize) {\n    try {\n      const message = EntryDecoder(messageSetDecoder)\n      const codec = lookupCodecByAttributes(message.attributes)\n\n      if (codec) {\n        const buffer = await codec.decompress(message.value)\n        messages.push(...EntriesDecoder(new Decoder(buffer), message))\n      } else {\n        messages.push(message)\n      }\n    } catch (e) {\n      if (e.name === 'KafkaJSPartialMessageError') {\n        // We tried to decode a partial message, it means that minBytes\n        // is probably too low\n        break\n      }\n\n      if (e.name === 'KafkaJSUnsupportedMagicByteInMessageSet') {\n        // Received a MessageSet and a RecordBatch on the same response, the cluster is probably\n        // upgrading the message format from 0.10 to 0.11. Stop processing this message set to\n        // receive the full record batch on the next request\n        break\n      }\n\n      throw e\n    }\n  }\n\n  primaryDecoder.forward(messageSetSize)\n  return messages\n}\n\nconst EntriesDecoder = (decoder, compressedMessage) => {\n  const messages = []\n\n  while (decoder.offset < decoder.buffer.length) {\n    messages.push(EntryDecoder(decoder))\n  }\n\n  if (compressedMessage.magicByte > 0 && compressedMessage.offset >= 0) {\n    const compressedOffset = Long.fromValue(compressedMessage.offset)\n    const lastMessageOffset = Long.fromValue(messages[messages.length - 1].offset)\n    const baseOffset = compressedOffset - lastMessageOffset\n\n    for (const message of messages) {\n      message.offset = Long.fromValue(message.offset)\n        .add(baseOffset)\n        .toString()\n    }\n  }\n\n  return messages\n}\n\nconst EntryDecoder = decoder => {\n  if (!decoder.canReadInt64()) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial message: There isn't enough bytes to read the offset`\n    )\n  }\n\n  const offset = decoder.readInt64().toString()\n\n  if (!decoder.canReadInt32()) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial message: There isn't enough bytes to read the message size`\n    )\n  }\n\n  const size = decoder.readInt32()\n  return MessageDecoder(offset, size, decoder)\n}\n","const Encoder = require('../encoder')\nconst MessageProtocol = require('../message')\nconst { Types } = require('../message/compression')\n\n/**\n * MessageSet => [Offset MessageSize Message]\n *  Offset => int64\n *  MessageSize => int32\n *  Message => Bytes\n */\n\n/**\n * [\n *   { key: \"<value>\", value: \"<value>\" },\n *   { key: \"<value>\", value: \"<value>\" },\n * ]\n */\nmodule.exports = ({ messageVersion = 0, compression, entries }) => {\n  const isCompressed = compression !== Types.None\n  const Message = MessageProtocol({ version: messageVersion })\n  const encoder = new Encoder()\n\n  // Messages in a message set are __not__ encoded as an array.\n  // They are written in sequence.\n  // https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-Messagesets\n\n  entries.forEach((entry, i) => {\n    const message = Message(entry)\n\n    // This is the offset used in kafka as the log sequence number.\n    // When the producer is sending non compressed messages, it can set the offsets to anything\n    // When the producer is sending compressed messages, to avoid server side recompression, each compressed message\n    // should have offset starting from 0 and increasing by one for each inner message in the compressed message\n    encoder.writeInt64(isCompressed ? i : -1)\n    encoder.writeInt32(message.size())\n\n    encoder.writeEncoder(message)\n  })\n\n  return encoder\n}\n","/**\n * A javascript implementation of the CRC32 checksum that uses\n * the CRC32-C polynomial, the same polynomial used by iSCSI\n *\n * also known as CRC32 Castagnoli\n * based on: https://github.com/ashi009/node-fast-crc32c/blob/master/impls/js_crc32c.js\n */\nconst crc32C = buffer => {\n  let crc = 0 ^ -1\n  for (let i = 0; i < buffer.length; i++) {\n    crc = T[(crc ^ buffer[i]) & 0xff] ^ (crc >>> 8)\n  }\n\n  return (crc ^ -1) >>> 0\n}\n\nmodule.exports = crc32C\n\n// prettier-ignore\nvar T = new Int32Array([\n  0x00000000, 0xf26b8303, 0xe13b70f7, 0x1350f3f4,\n  0xc79a971f, 0x35f1141c, 0x26a1e7e8, 0xd4ca64eb,\n  0x8ad958cf, 0x78b2dbcc, 0x6be22838, 0x9989ab3b,\n  0x4d43cfd0, 0xbf284cd3, 0xac78bf27, 0x5e133c24,\n  0x105ec76f, 0xe235446c, 0xf165b798, 0x030e349b,\n  0xd7c45070, 0x25afd373, 0x36ff2087, 0xc494a384,\n  0x9a879fa0, 0x68ec1ca3, 0x7bbcef57, 0x89d76c54,\n  0x5d1d08bf, 0xaf768bbc, 0xbc267848, 0x4e4dfb4b,\n  0x20bd8ede, 0xd2d60ddd, 0xc186fe29, 0x33ed7d2a,\n  0xe72719c1, 0x154c9ac2, 0x061c6936, 0xf477ea35,\n  0xaa64d611, 0x580f5512, 0x4b5fa6e6, 0xb93425e5,\n  0x6dfe410e, 0x9f95c20d, 0x8cc531f9, 0x7eaeb2fa,\n  0x30e349b1, 0xc288cab2, 0xd1d83946, 0x23b3ba45,\n  0xf779deae, 0x05125dad, 0x1642ae59, 0xe4292d5a,\n  0xba3a117e, 0x4851927d, 0x5b016189, 0xa96ae28a,\n  0x7da08661, 0x8fcb0562, 0x9c9bf696, 0x6ef07595,\n  0x417b1dbc, 0xb3109ebf, 0xa0406d4b, 0x522bee48,\n  0x86e18aa3, 0x748a09a0, 0x67dafa54, 0x95b17957,\n  0xcba24573, 0x39c9c670, 0x2a993584, 0xd8f2b687,\n  0x0c38d26c, 0xfe53516f, 0xed03a29b, 0x1f682198,\n  0x5125dad3, 0xa34e59d0, 0xb01eaa24, 0x42752927,\n  0x96bf4dcc, 0x64d4cecf, 0x77843d3b, 0x85efbe38,\n  0xdbfc821c, 0x2997011f, 0x3ac7f2eb, 0xc8ac71e8,\n  0x1c661503, 0xee0d9600, 0xfd5d65f4, 0x0f36e6f7,\n  0x61c69362, 0x93ad1061, 0x80fde395, 0x72966096,\n  0xa65c047d, 0x5437877e, 0x4767748a, 0xb50cf789,\n  0xeb1fcbad, 0x197448ae, 0x0a24bb5a, 0xf84f3859,\n  0x2c855cb2, 0xdeeedfb1, 0xcdbe2c45, 0x3fd5af46,\n  0x7198540d, 0x83f3d70e, 0x90a324fa, 0x62c8a7f9,\n  0xb602c312, 0x44694011, 0x5739b3e5, 0xa55230e6,\n  0xfb410cc2, 0x092a8fc1, 0x1a7a7c35, 0xe811ff36,\n  0x3cdb9bdd, 0xceb018de, 0xdde0eb2a, 0x2f8b6829,\n  0x82f63b78, 0x709db87b, 0x63cd4b8f, 0x91a6c88c,\n  0x456cac67, 0xb7072f64, 0xa457dc90, 0x563c5f93,\n  0x082f63b7, 0xfa44e0b4, 0xe9141340, 0x1b7f9043,\n  0xcfb5f4a8, 0x3dde77ab, 0x2e8e845f, 0xdce5075c,\n  0x92a8fc17, 0x60c37f14, 0x73938ce0, 0x81f80fe3,\n  0x55326b08, 0xa759e80b, 0xb4091bff, 0x466298fc,\n  0x1871a4d8, 0xea1a27db, 0xf94ad42f, 0x0b21572c,\n  0xdfeb33c7, 0x2d80b0c4, 0x3ed04330, 0xccbbc033,\n  0xa24bb5a6, 0x502036a5, 0x4370c551, 0xb11b4652,\n  0x65d122b9, 0x97baa1ba, 0x84ea524e, 0x7681d14d,\n  0x2892ed69, 0xdaf96e6a, 0xc9a99d9e, 0x3bc21e9d,\n  0xef087a76, 0x1d63f975, 0x0e330a81, 0xfc588982,\n  0xb21572c9, 0x407ef1ca, 0x532e023e, 0xa145813d,\n  0x758fe5d6, 0x87e466d5, 0x94b49521, 0x66df1622,\n  0x38cc2a06, 0xcaa7a905, 0xd9f75af1, 0x2b9cd9f2,\n  0xff56bd19, 0x0d3d3e1a, 0x1e6dcdee, 0xec064eed,\n  0xc38d26c4, 0x31e6a5c7, 0x22b65633, 0xd0ddd530,\n  0x0417b1db, 0xf67c32d8, 0xe52cc12c, 0x1747422f,\n  0x49547e0b, 0xbb3ffd08, 0xa86f0efc, 0x5a048dff,\n  0x8ecee914, 0x7ca56a17, 0x6ff599e3, 0x9d9e1ae0,\n  0xd3d3e1ab, 0x21b862a8, 0x32e8915c, 0xc083125f,\n  0x144976b4, 0xe622f5b7, 0xf5720643, 0x07198540,\n  0x590ab964, 0xab613a67, 0xb831c993, 0x4a5a4a90,\n  0x9e902e7b, 0x6cfbad78, 0x7fab5e8c, 0x8dc0dd8f,\n  0xe330a81a, 0x115b2b19, 0x020bd8ed, 0xf0605bee,\n  0x24aa3f05, 0xd6c1bc06, 0xc5914ff2, 0x37faccf1,\n  0x69e9f0d5, 0x9b8273d6, 0x88d28022, 0x7ab90321,\n  0xae7367ca, 0x5c18e4c9, 0x4f48173d, 0xbd23943e,\n  0xf36e6f75, 0x0105ec76, 0x12551f82, 0xe03e9c81,\n  0x34f4f86a, 0xc69f7b69, 0xd5cf889d, 0x27a40b9e,\n  0x79b737ba, 0x8bdcb4b9, 0x988c474d, 0x6ae7c44e,\n  0xbe2da0a5, 0x4c4623a6, 0x5f16d052, 0xad7d5351\n]);\n","const crc32C = require('./crc32C')\nconst unsigned = value => Uint32Array.from([value])[0]\n\nmodule.exports = buffer => unsigned(crc32C(buffer))\n","module.exports = decoder => ({\n  key: decoder.readVarIntString(),\n  value: decoder.readVarIntBytes(),\n})\n","const Encoder = require('../../../encoder')\n\n/**\n * v0\n * Header => Key Value\n *   Key => varInt|string\n *   Value => varInt|bytes\n */\n\nmodule.exports = ({ key, value }) => {\n  return new Encoder().writeVarIntString(key).writeVarIntBytes(value)\n}\n","const Long = require('../../../../utils/long')\nconst HeaderDecoder = require('../../header/v0/decoder')\nconst TimestampTypes = require('../../../timestampTypes')\n\n/**\n * v0\n * Record =>\n *   Length => Varint\n *   Attributes => Int8\n *   TimestampDelta => Varlong\n *   OffsetDelta => Varint\n *   Key => varInt|Bytes\n *   Value => varInt|Bytes\n *   Headers => [HeaderKey HeaderValue]\n *     HeaderKey => VarInt|String\n *     HeaderValue => VarInt|Bytes\n */\n\nmodule.exports = (decoder, batchContext = {}) => {\n  const {\n    firstOffset,\n    firstTimestamp,\n    magicByte,\n    isControlBatch = false,\n    timestampType,\n    maxTimestamp,\n  } = batchContext\n  const attributes = decoder.readInt8()\n\n  const timestampDelta = decoder.readVarLong()\n  const timestamp =\n    timestampType === TimestampTypes.LOG_APPEND_TIME && maxTimestamp\n      ? maxTimestamp\n      : Long.fromValue(firstTimestamp)\n          .add(timestampDelta)\n          .toString()\n\n  const offsetDelta = decoder.readVarInt()\n  const offset = Long.fromValue(firstOffset)\n    .add(offsetDelta)\n    .toString()\n\n  const key = decoder.readVarIntBytes()\n  const value = decoder.readVarIntBytes()\n  const headers = decoder\n    .readVarIntArray(HeaderDecoder)\n    .reduce((obj, { key, value }) => ({ ...obj, [key]: value }), {})\n\n  return {\n    magicByte,\n    attributes, // Record level attributes are presently unused\n    timestamp,\n    offset,\n    key,\n    value,\n    headers,\n    isControlRecord: isControlBatch,\n    batchContext,\n  }\n}\n","const Encoder = require('../../../encoder')\nconst Header = require('../../header/v0')\n\n/**\n * v0\n * Record =>\n *   Length => Varint\n *   Attributes => Int8\n *   TimestampDelta => Varlong\n *   OffsetDelta => Varint\n *   Key => varInt|Bytes\n *   Value => varInt|Bytes\n *   Headers => [HeaderKey HeaderValue]\n *     HeaderKey => VarInt|String\n *     HeaderValue => VarInt|Bytes\n */\n\n/**\n * @param [offsetDelta=0] {Integer}\n * @param [timestampDelta=0] {Long}\n * @param key {Buffer}\n * @param value {Buffer}\n * @param [headers={}] {Object}\n */\nmodule.exports = ({ offsetDelta = 0, timestampDelta = 0, key, value, headers = {} }) => {\n  const headersArray = Object.keys(headers).map(headerKey => ({\n    key: headerKey,\n    value: headers[headerKey],\n  }))\n\n  const sizeOfBody =\n    1 + // always one byte for attributes\n    Encoder.sizeOfVarLong(timestampDelta) +\n    Encoder.sizeOfVarInt(offsetDelta) +\n    Encoder.sizeOfVarIntBytes(key) +\n    Encoder.sizeOfVarIntBytes(value) +\n    sizeOfHeaders(headersArray)\n\n  return new Encoder()\n    .writeVarInt(sizeOfBody)\n    .writeInt8(0) // no used record attributes at the moment\n    .writeVarLong(timestampDelta)\n    .writeVarInt(offsetDelta)\n    .writeVarIntBytes(key)\n    .writeVarIntBytes(value)\n    .writeVarIntArray(headersArray.map(Header))\n}\n\nconst sizeOfHeaders = headersArray => {\n  let size = Encoder.sizeOfVarInt(headersArray.length)\n\n  for (const header of headersArray) {\n    const keySize = Buffer.byteLength(header.key)\n    const valueSize = Buffer.byteLength(header.value)\n\n    size += Encoder.sizeOfVarInt(keySize) + keySize\n\n    if (header.value === null) {\n      size += Encoder.sizeOfVarInt(-1)\n    } else {\n      size += Encoder.sizeOfVarInt(valueSize) + valueSize\n    }\n  }\n\n  return size\n}\n","const Decoder = require('../../decoder')\nconst { KafkaJSPartialMessageError } = require('../../../errors')\nconst { lookupCodecByAttributes } = require('../../message/compression')\nconst RecordDecoder = require('../record/v0/decoder')\nconst TimestampTypes = require('../../timestampTypes')\n\nconst TIMESTAMP_TYPE_FLAG_MASK = 0x8\nconst TRANSACTIONAL_FLAG_MASK = 0x10\nconst CONTROL_FLAG_MASK = 0x20\n\n/**\n * v0\n * RecordBatch =>\n *  FirstOffset => int64\n *  Length => int32\n *  PartitionLeaderEpoch => int32\n *  Magic => int8\n *  CRC => int32\n *  Attributes => int16\n *  LastOffsetDelta => int32\n *  FirstTimestamp => int64\n *  MaxTimestamp => int64\n *  ProducerId => int64\n *  ProducerEpoch => int16\n *  FirstSequence => int32\n *  Records => [Record]\n */\n\nmodule.exports = async fetchDecoder => {\n  const firstOffset = fetchDecoder.readInt64().toString()\n  const length = fetchDecoder.readInt32()\n  const decoder = fetchDecoder.slice(length)\n  fetchDecoder.forward(length)\n\n  const remainingBytes = Buffer.byteLength(decoder.buffer)\n\n  if (remainingBytes < length) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial record batch: remainingBytes(${remainingBytes}) < recordBatchLength(${length})`\n    )\n  }\n\n  const partitionLeaderEpoch = decoder.readInt32()\n\n  // The magic byte was read by the Fetch protocol to distinguish between\n  // the record batch and the legacy message set. It's not used here but\n  // it has to be read.\n  const magicByte = decoder.readInt8() // eslint-disable-line no-unused-vars\n\n  // The library is currently not performing CRC validations\n  const crc = decoder.readInt32() // eslint-disable-line no-unused-vars\n\n  const attributes = decoder.readInt16()\n  const lastOffsetDelta = decoder.readInt32()\n  const firstTimestamp = decoder.readInt64().toString()\n  const maxTimestamp = decoder.readInt64().toString()\n  const producerId = decoder.readInt64().toString()\n  const producerEpoch = decoder.readInt16()\n  const firstSequence = decoder.readInt32()\n\n  const inTransaction = (attributes & TRANSACTIONAL_FLAG_MASK) > 0\n  const isControlBatch = (attributes & CONTROL_FLAG_MASK) > 0\n  const timestampType =\n    (attributes & TIMESTAMP_TYPE_FLAG_MASK) > 0\n      ? TimestampTypes.LOG_APPEND_TIME\n      : TimestampTypes.CREATE_TIME\n\n  const codec = lookupCodecByAttributes(attributes)\n\n  const recordContext = {\n    firstOffset,\n    firstTimestamp,\n    partitionLeaderEpoch,\n    inTransaction,\n    isControlBatch,\n    lastOffsetDelta,\n    producerId,\n    producerEpoch,\n    firstSequence,\n    maxTimestamp,\n    timestampType,\n  }\n\n  const records = await decodeRecords(codec, decoder, { ...recordContext, magicByte })\n\n  return {\n    ...recordContext,\n    records,\n  }\n}\n\nconst decodeRecords = async (codec, recordsDecoder, recordContext) => {\n  if (!codec) {\n    return recordsDecoder.readArray(decoder => decodeRecord(decoder, recordContext))\n  }\n\n  const length = recordsDecoder.readInt32()\n\n  if (length <= 0) {\n    return []\n  }\n\n  const compressedRecordsBuffer = recordsDecoder.readAll()\n  const decompressedRecordBuffer = await codec.decompress(compressedRecordsBuffer)\n  const decompressedRecordDecoder = new Decoder(decompressedRecordBuffer)\n  const records = new Array(length)\n\n  for (let i = 0; i < length; i++) {\n    records[i] = decodeRecord(decompressedRecordDecoder, recordContext)\n  }\n\n  return records\n}\n\nconst decodeRecord = (decoder, recordContext) => {\n  const recordBuffer = decoder.readVarIntBytes()\n  return RecordDecoder(new Decoder(recordBuffer), recordContext)\n}\n","const Long = require('../../../utils/long')\nconst Encoder = require('../../encoder')\nconst crc32C = require('../crc32C')\nconst {\n  Types: Compression,\n  lookupCodec,\n  COMPRESSION_CODEC_MASK,\n} = require('../../message/compression')\n\nconst MAGIC_BYTE = 2\nconst TIMESTAMP_MASK = 0 // The fourth lowest bit, always set this bit to 0 (since 0.10.0)\nconst TRANSACTIONAL_MASK = 16 // The fifth lowest bit\n\n/**\n * v0\n * RecordBatch =>\n *  FirstOffset => int64\n *  Length => int32\n *  PartitionLeaderEpoch => int32\n *  Magic => int8\n *  CRC => int32\n *  Attributes => int16\n *  LastOffsetDelta => int32\n *  FirstTimestamp => int64\n *  MaxTimestamp => int64\n *  ProducerId => int64\n *  ProducerEpoch => int16\n *  FirstSequence => int32\n *  Records => [Record]\n */\n\nconst RecordBatch = async ({\n  compression = Compression.None,\n  firstOffset = Long.fromInt(0),\n  firstTimestamp = Date.now(),\n  maxTimestamp = Date.now(),\n  partitionLeaderEpoch = 0,\n  lastOffsetDelta = 0,\n  transactional = false,\n  producerId = Long.fromValue(-1), // for idempotent messages\n  producerEpoch = 0, // for idempotent messages\n  firstSequence = 0, // for idempotent messages\n  records = [],\n}) => {\n  const COMPRESSION_CODEC = compression & COMPRESSION_CODEC_MASK\n  const IN_TRANSACTION = transactional ? TRANSACTIONAL_MASK : 0\n  const attributes = COMPRESSION_CODEC | TIMESTAMP_MASK | IN_TRANSACTION\n\n  const batchBody = new Encoder()\n    .writeInt16(attributes)\n    .writeInt32(lastOffsetDelta)\n    .writeInt64(firstTimestamp)\n    .writeInt64(maxTimestamp)\n    .writeInt64(producerId)\n    .writeInt16(producerEpoch)\n    .writeInt32(firstSequence)\n\n  if (compression === Compression.None) {\n    if (records.every(v => typeof v === typeof records[0])) {\n      batchBody.writeArray(records, typeof records[0])\n    } else {\n      batchBody.writeArray(records)\n    }\n  } else {\n    const compressedRecords = await compressRecords(compression, records)\n    batchBody.writeInt32(records.length).writeBuffer(compressedRecords)\n  }\n\n  // CRC32C validation is happening here:\n  // https://github.com/apache/kafka/blob/0.11.0.1/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java#L148\n\n  const batch = new Encoder()\n    .writeInt32(partitionLeaderEpoch)\n    .writeInt8(MAGIC_BYTE)\n    .writeUInt32(crc32C(batchBody.buffer))\n    .writeEncoder(batchBody)\n\n  return new Encoder().writeInt64(firstOffset).writeBytes(batch.buffer)\n}\n\nconst compressRecords = async (compression, records) => {\n  const codec = lookupCodec(compression)\n  const recordsEncoder = new Encoder()\n\n  recordsEncoder.writeEncoderArray(records)\n\n  return codec.compress(recordsEncoder)\n}\n\nmodule.exports = {\n  RecordBatch,\n  MAGIC_BYTE,\n}\n","const Encoder = require('./encoder')\n\nmodule.exports = async ({ correlationId, clientId, request: { apiKey, apiVersion, encode } }) => {\n  const payload = await encode()\n  const requestPayload = new Encoder()\n    .writeInt16(apiKey)\n    .writeInt16(apiVersion)\n    .writeInt32(correlationId)\n    .writeString(clientId)\n    .writeEncoder(payload)\n\n  return new Encoder().writeInt32(requestPayload.size()).writeEncoder(requestPayload)\n}\n","const versions = {\n  0: ({ transactionalId, producerId, producerEpoch, groupId }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ transactionalId, producerId, producerEpoch, groupId }), response }\n  },\n  1: ({ transactionalId, producerId, producerEpoch, groupId }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ transactionalId, producerId, producerEpoch, groupId }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { AddOffsetsToTxn: apiKey } = require('../../apiKeys')\n\n/**\n * AddOffsetsToTxn Request (Version: 0) => transactional_id producer_id producer_epoch group_id\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   group_id => STRING\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, groupId }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'AddOffsetsToTxn',\n  encode: async () => {\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeInt64(producerId)\n      .writeInt16(producerEpoch)\n      .writeString(groupId)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * AddOffsetsToTxn Response (Version: 0) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * AddOffsetsToTxn Request (Version: 1) => transactional_id producer_id producer_epoch group_id\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   group_id => STRING\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, groupId }) =>\n  Object.assign(\n    requestV0({\n      transactionalId,\n      producerId,\n      producerEpoch,\n      groupId,\n    }),\n    { apiVersion: 1 }\n  )\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * AddOffsetsToTxn Response (Version: 1) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ transactionalId, producerId, producerEpoch, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ transactionalId, producerId, producerEpoch, topics }), response }\n  },\n  1: ({ transactionalId, producerId, producerEpoch, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ transactionalId, producerId, producerEpoch, topics }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { AddPartitionsToTxn: apiKey } = require('../../apiKeys')\n\n/**\n * AddPartitionsToTxn Request (Version: 0) => transactional_id producer_id producer_epoch [topics]\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'AddPartitionsToTxn',\n  encode: async () => {\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeInt64(producerId)\n      .writeInt16(producerEpoch)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = partition => {\n  return new Encoder().writeInt32(partition)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * AddPartitionsToTxn Response (Version: 0) => throttle_time_ms [errors]\n *   throttle_time_ms => INT32\n *   errors => topic [partition_errors]\n *     topic => STRING\n *     partition_errors => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errors = await decoder.readArrayAsync(decodeError)\n\n  return {\n    throttleTime,\n    errors,\n  }\n}\n\nconst decodeError = async decoder => ({\n  topic: decoder.readString(),\n  partitionErrors: await decoder.readArrayAsync(decodePartitionError),\n})\n\nconst decodePartitionError = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  const topicsWithErrors = data.errors\n    .map(({ partitionErrors }) => ({\n      partitionsWithErrors: partitionErrors.filter(({ errorCode }) => failure(errorCode)),\n    }))\n    .filter(({ partitionsWithErrors }) => partitionsWithErrors.length)\n\n  if (topicsWithErrors.length > 0) {\n    throw createErrorFromCode(topicsWithErrors[0].partitionsWithErrors[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * AddPartitionsToTxn Request (Version: 1) => transactional_id producer_id producer_epoch [topics]\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, topics }) =>\n  Object.assign(\n    requestV0({\n      transactionalId,\n      producerId,\n      producerEpoch,\n      topics,\n    }),\n    { apiVersion: 1 }\n  )\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * AddPartitionsToTxn Response (Version: 1) => throttle_time_ms [errors]\n *   throttle_time_ms => INT32\n *   errors => topic [partition_errors]\n *     topic => STRING\n *     partition_errors => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ resources, validateOnly }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ resources, validateOnly }), response }\n  },\n  1: ({ resources, validateOnly }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ resources, validateOnly }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { AlterConfigs: apiKey } = require('../../apiKeys')\n\n/**\n * AlterConfigs Request (Version: 0) => [resources] validate_only\n *   resources => resource_type resource_name [config_entries]\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_entries => config_name config_value\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *   validate_only => BOOLEAN\n */\n\n/**\n * @param {Array} resources An array of resources to change\n * @param {boolean} [validateOnly=false]\n */\nmodule.exports = ({ resources, validateOnly = false }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'AlterConfigs',\n  encode: async () => {\n    return new Encoder().writeArray(resources.map(encodeResource)).writeBoolean(validateOnly)\n  },\n})\n\nconst encodeResource = ({ type, name, configEntries }) => {\n  return new Encoder()\n    .writeInt8(type)\n    .writeString(name)\n    .writeArray(configEntries.map(encodeConfigEntries))\n}\n\nconst encodeConfigEntries = ({ name, value }) => {\n  return new Encoder().writeString(name).writeString(value)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * AlterConfigs Response (Version: 0) => throttle_time_ms [resources]\n *   throttle_time_ms => INT32\n *   resources => error_code error_message resource_type resource_name\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     resource_type => INT8\n *     resource_name => STRING\n */\n\nconst decodeResources = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const resources = decoder.readArray(decodeResources)\n\n  return {\n    throttleTime,\n    resources,\n  }\n}\n\nconst parse = async data => {\n  const resourcesWithError = data.resources.filter(({ errorCode }) => failure(errorCode))\n  if (resourcesWithError.length > 0) {\n    throw createErrorFromCode(resourcesWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * AlterConfigs Request (Version: 1) => [resources] validate_only\n *   resources => resource_type resource_name [config_entries]\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_entries => config_name config_value\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *   validate_only => BOOLEAN\n */\n\n/**\n * @param {Array} resources An array of resources to change\n * @param {boolean} [validateOnly=false]\n */\nmodule.exports = ({ resources, validateOnly }) =>\n  Object.assign(\n    requestV0({\n      resources,\n      validateOnly,\n    }),\n    { apiVersion: 1 }\n  )\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * AlterConfigs Response (Version: 1) => throttle_time_ms [resources]\n *   throttle_time_ms => INT32\n *   resources => error_code error_message resource_type resource_name\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     resource_type => INT8\n *     resource_name => STRING\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","module.exports = {\n  Produce: 0,\n  Fetch: 1,\n  ListOffsets: 2,\n  Metadata: 3,\n  LeaderAndIsr: 4,\n  StopReplica: 5,\n  UpdateMetadata: 6,\n  ControlledShutdown: 7,\n  OffsetCommit: 8,\n  OffsetFetch: 9,\n  GroupCoordinator: 10,\n  JoinGroup: 11,\n  Heartbeat: 12,\n  LeaveGroup: 13,\n  SyncGroup: 14,\n  DescribeGroups: 15,\n  ListGroups: 16,\n  SaslHandshake: 17,\n  ApiVersions: 18, // ApiVersions v0 on Kafka 0.10\n  CreateTopics: 19,\n  DeleteTopics: 20,\n  DeleteRecords: 21,\n  InitProducerId: 22,\n  OffsetForLeaderEpoch: 23,\n  AddPartitionsToTxn: 24,\n  AddOffsetsToTxn: 25,\n  EndTxn: 26,\n  WriteTxnMarkers: 27,\n  TxnOffsetCommit: 28,\n  DescribeAcls: 29,\n  CreateAcls: 30,\n  DeleteAcls: 31,\n  DescribeConfigs: 32,\n  AlterConfigs: 33, // ApiVersions v0 and v1 on Kafka 0.11\n  AlterReplicaLogDirs: 34,\n  DescribeLogDirs: 35,\n  SaslAuthenticate: 36,\n  CreatePartitions: 37,\n  CreateDelegationToken: 38,\n  RenewDelegationToken: 39,\n  ExpireDelegationToken: 40,\n  DescribeDelegationToken: 41,\n  DeleteGroups: 42, // ApiVersions v2 on Kafka 1.0\n  ElectPreferredLeaders: 43,\n}\n","const logResponseError = false\n\nconst versions = {\n  0: () => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request(), response, logResponseError: true }\n  },\n  1: () => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request(), response, logResponseError }\n  },\n  2: () => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request(), response, logResponseError }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { ApiVersions: apiKey } = require('../../apiKeys')\n\n/**\n * ApiVersionRequest => ApiKeys\n */\n\nmodule.exports = () => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'ApiVersions',\n  encode: async () => new Encoder(),\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * ApiVersionResponse => ApiVersions\n *   ErrorCode = INT16\n *   ApiVersions = [ApiVersion]\n *     ApiVersion = ApiKey MinVersion MaxVersion\n *       ApiKey = INT16\n *       MinVersion = INT16\n *       MaxVersion = INT16\n */\n\nconst apiVersion = decoder => ({\n  apiKey: decoder.readInt16(),\n  minVersion: decoder.readInt16(),\n  maxVersion: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    errorCode,\n    apiVersions: decoder.readArray(apiVersion),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n// ApiVersions Request after v1 indicates the client can parse throttle_time_ms\n\nmodule.exports = () => ({ ...requestV0(), apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * ApiVersions Response (Version: 1) => error_code [api_versions] throttle_time_ms\n *   error_code => INT16\n *   api_versions => api_key min_version max_version\n *     api_key => INT16\n *     min_version => INT16\n *     max_version => INT16\n *   throttle_time_ms => INT32\n */\n\nconst apiVersion = decoder => ({\n  apiKey: decoder.readInt16(),\n  minVersion: decoder.readInt16(),\n  maxVersion: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  const apiVersions = decoder.readArray(apiVersion)\n\n  /**\n   * The Java client defaults this value to 0 if not present,\n   * even though it is required in the protocol. This is to\n   * work around https://github.com/tulios/kafkajs/issues/491\n   *\n   * See:\n   * https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java#L23-L25\n   */\n  const throttleTime = decoder.canReadInt32() ? decoder.readInt32() : 0\n\n  return {\n    errorCode,\n    apiVersions,\n    throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV0 = require('../v0/request')\n\n// ApiVersions Request after v1 indicates the client can parse throttle_time_ms\n\nmodule.exports = () => ({ ...requestV0(), apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * Starting in version 2, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * ApiVersions Response (Version: 2) => error_code [api_versions] throttle_time_ms\n *   error_code => INT16\n *   api_versions => api_key min_version max_version\n *     api_key => INT16\n *     min_version => INT16\n *     max_version => INT16\n *   throttle_time_ms => INT32\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ creations }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ creations }), response }\n  },\n  1: ({ creations }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ creations }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { CreateAcls: apiKey } = require('../../apiKeys')\n\n/**\n * CreateAcls Request (Version: 0) => [creations]\n *   creations => resource_type resource_name principal host operation permission_type\n *     resource_type => INT8\n *     resource_name => STRING\n *     principal => STRING\n *     host => STRING\n *     operation => INT8\n *     permission_type => INT8\n */\n\nconst encodeCreations = ({\n  resourceType,\n  resourceName,\n  principal,\n  host,\n  operation,\n  permissionType,\n}) => {\n  return new Encoder()\n    .writeInt8(resourceType)\n    .writeString(resourceName)\n    .writeString(principal)\n    .writeString(host)\n    .writeInt8(operation)\n    .writeInt8(permissionType)\n}\n\nmodule.exports = ({ creations }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'CreateAcls',\n  encode: async () => {\n    return new Encoder().writeArray(creations.map(encodeCreations))\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * CreateAcls Response (Version: 0) => throttle_time_ms [creation_responses]\n *   throttle_time_ms => INT32\n *   creation_responses => error_code error_message\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst decodeCreationResponse = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const creationResponses = decoder.readArray(decodeCreationResponse)\n\n  return {\n    throttleTime,\n    creationResponses,\n  }\n}\n\nconst parse = async data => {\n  const creationResponsesWithError = data.creationResponses.filter(({ errorCode }) =>\n    failure(errorCode)\n  )\n\n  if (creationResponsesWithError.length > 0) {\n    throw createErrorFromCode(creationResponsesWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { CreateAcls: apiKey } = require('../../apiKeys')\n\n/**\n * CreateAcls Request (Version: 1) => [creations]\n *   creations => resource_type resource_name resource_pattern_type principal host operation permission_type\n *     resource_type => INT8\n *     resource_name => STRING\n *     resource_pattern_type => INT8\n *     principal => STRING\n *     host => STRING\n *     operation => INT8\n *     permission_type => INT8\n */\n\nconst encodeCreations = ({\n  resourceType,\n  resourceName,\n  resourcePatternType,\n  principal,\n  host,\n  operation,\n  permissionType,\n}) => {\n  return new Encoder()\n    .writeInt8(resourceType)\n    .writeString(resourceName)\n    .writeInt8(resourcePatternType)\n    .writeString(principal)\n    .writeString(host)\n    .writeInt8(operation)\n    .writeInt8(permissionType)\n}\n\nmodule.exports = ({ creations }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'CreateAcls',\n  encode: async () => {\n    return new Encoder().writeArray(creations.map(encodeCreations))\n  },\n})\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * CreateAcls Response (Version: 1) => throttle_time_ms [creation_responses]\n *   throttle_time_ms => INT32\n *   creation_responses => error_code error_message\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ topicPartitions, timeout, validateOnly }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ topicPartitions, timeout, validateOnly }), response }\n  },\n  1: ({ topicPartitions, validateOnly, timeout }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ topicPartitions, validateOnly, timeout }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { CreatePartitions: apiKey } = require('../../apiKeys')\n\n/**\n * CreatePartitions Request (Version: 0) => [topic_partitions] timeout validate_only\n *   topic_partitions => topic new_partitions\n *     topic => STRING\n *     new_partitions => count [assignment]\n *       count => INT32\n *       assignment => ARRAY(INT32)\n *   timeout => INT32\n *   validate_only => BOOLEAN\n */\n\nmodule.exports = ({ topicPartitions, validateOnly = false, timeout = 5000 }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'CreatePartitions',\n  encode: async () => {\n    return new Encoder()\n      .writeArray(topicPartitions.map(encodeTopicPartitions))\n      .writeInt32(timeout)\n      .writeBoolean(validateOnly)\n  },\n})\n\nconst encodeTopicPartitions = ({ topic, count, assignments = [] }) => {\n  return new Encoder()\n    .writeString(topic)\n    .writeInt32(count)\n    .writeNullableArray(assignments.map(encodeAssignments))\n}\n\nconst encodeAssignments = brokerIds => {\n  return new Encoder().writeNullableArray(brokerIds)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/*\n * CreatePartitions Response (Version: 0) => throttle_time_ms [topic_errors]\n *   throttle_time_ms => INT32\n *   topic_errors => topic error_code error_message\n *     topic => STRING\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  return {\n    throttleTime,\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nconst parse = async data => {\n  const topicsWithError = data.topicErrors.filter(({ errorCode }) => failure(errorCode))\n  if (topicsWithError.length > 0) {\n    throw createErrorFromCode(topicsWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * CreatePartitions Request (Version: 1) => [topic_partitions] timeout validate_only\n *   topic_partitions => topic new_partitions\n *     topic => STRING\n *     new_partitions => count [assignment]\n *       count => INT32\n *       assignment => ARRAY(INT32)\n *   timeout => INT32\n *   validate_only => BOOLEAN\n */\n\nmodule.exports = ({ topicPartitions, validateOnly, timeout }) =>\n  Object.assign(requestV0({ topicPartitions, validateOnly, timeout }), { apiVersion: 1 })\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * CreatePartitions Response (Version: 0) => throttle_time_ms [topic_errors]\n *   throttle_time_ms => INT32\n *   topic_errors => topic error_code error_message\n *     topic => STRING\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ topics, timeout }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ topics, timeout }), response }\n  },\n  1: ({ topics, validateOnly, timeout }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ topics, validateOnly, timeout }), response }\n  },\n  2: ({ topics, validateOnly, timeout }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ topics, validateOnly, timeout }), response }\n  },\n  3: ({ topics, validateOnly, timeout }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return { request: request({ topics, validateOnly, timeout }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { CreateTopics: apiKey } = require('../../apiKeys')\n\n/**\n * CreateTopics Request (Version: 0) => [create_topic_requests] timeout\n *   create_topic_requests => topic num_partitions replication_factor [replica_assignment] [config_entries]\n *     topic => STRING\n *     num_partitions => INT32\n *     replication_factor => INT16\n *     replica_assignment => partition [replicas]\n *       partition => INT32\n *       replicas => INT32\n *     config_entries => config_name config_value\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *   timeout => INT32\n */\n\nmodule.exports = ({ topics, timeout = 5000 }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'CreateTopics',\n  encode: async () => {\n    return new Encoder().writeArray(topics.map(encodeTopics)).writeInt32(timeout)\n  },\n})\n\nconst encodeTopics = ({\n  topic,\n  numPartitions = 1,\n  replicationFactor = 1,\n  replicaAssignment = [],\n  configEntries = [],\n}) => {\n  return new Encoder()\n    .writeString(topic)\n    .writeInt32(numPartitions)\n    .writeInt16(replicationFactor)\n    .writeArray(replicaAssignment.map(encodeReplicaAssignment))\n    .writeArray(configEntries.map(encodeConfigEntries))\n}\n\nconst encodeReplicaAssignment = ({ partition, replicas }) => {\n  return new Encoder().writeInt32(partition).writeArray(replicas)\n}\n\nconst encodeConfigEntries = ({ name, value }) => {\n  return new Encoder().writeString(name).writeString(value)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst { KafkaJSAggregateError, KafkaJSCreateTopicError } = require('../../../../errors')\n\n/**\n * CreateTopics Response (Version: 0) => [topic_errors]\n *   topic_errors => topic error_code\n *     topic => STRING\n *     error_code => INT16\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nconst parse = async data => {\n  const topicsWithError = data.topicErrors.filter(({ errorCode }) => failure(errorCode))\n  if (topicsWithError.length > 0) {\n    throw new KafkaJSAggregateError(\n      'Topic creation errors',\n      topicsWithError.map(\n        error => new KafkaJSCreateTopicError(createErrorFromCode(error.errorCode), error.topic)\n      )\n    )\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { CreateTopics: apiKey } = require('../../apiKeys')\n\n/**\n *CreateTopics Request (Version: 1) => [create_topic_requests] timeout validate_only\n *  create_topic_requests => topic num_partitions replication_factor [replica_assignment] [config_entries]\n *    topic => STRING\n *    num_partitions => INT32\n *    replication_factor => INT16\n *    replica_assignment => partition [replicas]\n *      partition => INT32\n *      replicas => INT32\n *    config_entries => config_name config_value\n *      config_name => STRING\n *      config_value => NULLABLE_STRING\n *  timeout => INT32\n *  validate_only => BOOLEAN\n */\n\nmodule.exports = ({ topics, validateOnly = false, timeout = 5000 }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'CreateTopics',\n  encode: async () => {\n    return new Encoder()\n      .writeArray(topics.map(encodeTopics))\n      .writeInt32(timeout)\n      .writeBoolean(validateOnly)\n  },\n})\n\nconst encodeTopics = ({\n  topic,\n  numPartitions = 1,\n  replicationFactor = 1,\n  replicaAssignment = [],\n  configEntries = [],\n}) => {\n  return new Encoder()\n    .writeString(topic)\n    .writeInt32(numPartitions)\n    .writeInt16(replicationFactor)\n    .writeArray(replicaAssignment.map(encodeReplicaAssignment))\n    .writeArray(configEntries.map(encodeConfigEntries))\n}\n\nconst encodeReplicaAssignment = ({ partition, replicas }) => {\n  return new Encoder().writeInt32(partition).writeArray(replicas)\n}\n\nconst encodeConfigEntries = ({ name, value }) => {\n  return new Encoder().writeString(name).writeString(value)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * CreateTopics Response (Version: 1) => [topic_errors]\n *   topic_errors => topic error_code error_message\n *     topic => STRING\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * CreateTopics Request (Version: 2) => [create_topic_requests] timeout validate_only\n *   create_topic_requests => topic num_partitions replication_factor [replica_assignment] [config_entries]\n *     topic => STRING\n *     num_partitions => INT32\n *     replication_factor => INT16\n *     replica_assignment => partition [replicas]\n *       partition => INT32\n *       replicas => INT32\n *     config_entries => config_name config_value\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *   timeout => INT32\n *   validate_only => BOOLEAN\n */\n\nmodule.exports = ({ topics, validateOnly, timeout }) =>\n  Object.assign(requestV1({ topics, validateOnly, timeout }), { apiVersion: 2 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\n\n/**\n * CreateTopics Response (Version: 2) => throttle_time_ms [topic_errors]\n *   throttle_time_ms => INT32\n *   topic_errors => topic error_code error_message\n *     topic => STRING\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const requestV2 = require('../v2/request')\n\n/**\n * CreateTopics Request (Version: 3) => [create_topic_requests] timeout validate_only\n *   create_topic_requests => topic num_partitions replication_factor [replica_assignment] [config_entries]\n *     topic => STRING\n *     num_partitions => INT32\n *     replication_factor => INT16\n *     replica_assignment => partition [replicas]\n *       partition => INT32\n *       replicas => INT32\n *     config_entries => config_name config_value\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *   timeout => INT32\n *   validate_only => BOOLEAN\n */\n\nmodule.exports = ({ topics, validateOnly, timeout }) =>\n  Object.assign(requestV2({ topics, validateOnly, timeout }), { apiVersion: 3 })\n","const { parse, decode: decodeV2 } = require('../v2/response')\n\n/**\n * Starting in version 3, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * CreateTopics Response (Version: 3) => throttle_time_ms [topic_errors]\n *   throttle_time_ms => INT32\n *   topic_errors => topic error_code error_message\n *     topic => STRING\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV2(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ filters }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ filters }), response }\n  },\n  1: ({ filters }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ filters }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DeleteAcls: apiKey } = require('../../apiKeys')\n\n/**\n * DeleteAcls Request (Version: 0) => [filters]\n *   filters => resource_type resource_name principal host operation permission_type\n *     resource_type => INT8\n *     resource_name => NULLABLE_STRING\n *     principal => NULLABLE_STRING\n *     host => NULLABLE_STRING\n *     operation => INT8\n *     permission_type => INT8\n */\n\nconst encodeFilters = ({\n  resourceType,\n  resourceName,\n  principal,\n  host,\n  operation,\n  permissionType,\n}) => {\n  return new Encoder()\n    .writeInt8(resourceType)\n    .writeString(resourceName)\n    .writeString(principal)\n    .writeString(host)\n    .writeInt8(operation)\n    .writeInt8(permissionType)\n}\n\nmodule.exports = ({ filters }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DeleteAcls',\n  encode: async () => {\n    return new Encoder().writeArray(filters.map(encodeFilters))\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * DeleteAcls Response (Version: 0) => throttle_time_ms [filter_responses]\n *   throttle_time_ms => INT32\n *   filter_responses => error_code error_message [matching_acls]\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     matching_acls => error_code error_message resource_type resource_name principal host operation permission_type\n *       error_code => INT16\n *       error_message => NULLABLE_STRING\n *       resource_type => INT8\n *       resource_name => STRING\n *       principal => STRING\n *       host => STRING\n *       operation => INT8\n *       permission_type => INT8\n */\n\nconst decodeMatchingAcls = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n  principal: decoder.readString(),\n  host: decoder.readString(),\n  operation: decoder.readInt8(),\n  permissionType: decoder.readInt8(),\n})\n\nconst decodeFilterResponse = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  matchingAcls: decoder.readArray(decodeMatchingAcls),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const filterResponses = decoder.readArray(decodeFilterResponse)\n\n  return {\n    throttleTime,\n    filterResponses,\n  }\n}\n\nconst parse = async data => {\n  const filterResponsesWithError = data.filterResponses.filter(({ errorCode }) =>\n    failure(errorCode)\n  )\n\n  if (filterResponsesWithError.length > 0) {\n    throw createErrorFromCode(filterResponsesWithError[0].errorCode)\n  }\n\n  for (const filterResponse of data.filterResponses) {\n    const matchingAcls = filterResponse.matchingAcls\n    const matchingAclsWithError = matchingAcls.filter(({ errorCode }) => failure(errorCode))\n\n    if (matchingAclsWithError.length > 0) {\n      throw createErrorFromCode(matchingAclsWithError[0].errorCode)\n    }\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decodeMatchingAcls,\n  decodeFilterResponse,\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { DeleteAcls: apiKey } = require('../../apiKeys')\n\n/**\n * DeleteAcls Request (Version: 1) => [filters]\n *   filters => resource_type resource_name resource_pattern_type_filter principal host operation permission_type\n *     resource_type => INT8\n *     resource_name => NULLABLE_STRING\n *     resource_pattern_type_filter => INT8\n *     principal => NULLABLE_STRING\n *     host => NULLABLE_STRING\n *     operation => INT8\n *     permission_type => INT8\n */\n\nconst encodeFilters = ({\n  resourceType,\n  resourceName,\n  resourcePatternType,\n  principal,\n  host,\n  operation,\n  permissionType,\n}) => {\n  return new Encoder()\n    .writeInt8(resourceType)\n    .writeString(resourceName)\n    .writeInt8(resourcePatternType)\n    .writeString(principal)\n    .writeString(host)\n    .writeInt8(operation)\n    .writeInt8(permissionType)\n}\n\nmodule.exports = ({ filters }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'DeleteAcls',\n  encode: async () => {\n    return new Encoder().writeArray(filters.map(encodeFilters))\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n * Version 1 also introduces a new resource pattern type field.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-290%3A+Support+for+Prefixed+ACLs\n *\n * DeleteAcls Response (Version: 1) => throttle_time_ms [filter_responses]\n *   throttle_time_ms => INT32\n *   filter_responses => error_code error_message [matching_acls]\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     matching_acls => error_code error_message resource_type resource_name resource_pattern_type principal host operation permission_type\n *       error_code => INT16\n *       error_message => NULLABLE_STRING\n *       resource_type => INT8\n *       resource_name => STRING\n *       resource_pattern_type => INT8\n *       principal => STRING\n *       host => STRING\n *       operation => INT8\n *       permission_type => INT8\n */\n\nconst decodeMatchingAcls = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n  resourcePatternType: decoder.readInt8(),\n  principal: decoder.readString(),\n  host: decoder.readString(),\n  operation: decoder.readInt8(),\n  permissionType: decoder.readInt8(),\n})\n\nconst decodeFilterResponse = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  matchingAcls: decoder.readArray(decodeMatchingAcls),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const filterResponses = decoder.readArray(decodeFilterResponse)\n\n  return {\n    throttleTime: 0,\n    clientSideThrottleTime: throttleTime,\n    filterResponses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const versions = {\n  0: groupIds => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request(groupIds), response }\n  },\n  1: groupIds => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request(groupIds), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DeleteGroups: apiKey } = require('../../apiKeys')\n\n/**\n * DeleteGroups Request (Version: 0) => [groups_names]\n *   groups_names => STRING\n */\n\n/**\n */\nmodule.exports = groupIds => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DeleteGroups',\n  encode: async () => {\n    return new Encoder().writeArray(groupIds.map(encodeGroups))\n  },\n})\n\nconst encodeGroups = group => {\n  return new Encoder().writeString(group)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n/**\n * DeleteGroups Response (Version: 0) => throttle_time_ms [results]\n *  throttle_time_ms => INT32\n *  results => group_id error_code\n *    group_id => STRING\n *    error_code => INT16\n */\n\nconst decodeGroup = decoder => ({\n  groupId: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTimeMs = decoder.readInt32()\n  const results = decoder.readArray(decodeGroup)\n\n  for (const result of results) {\n    if (failure(result.errorCode)) {\n      result.error = createErrorFromCode(result.errorCode)\n    }\n  }\n  return {\n    throttleTimeMs,\n    results,\n  }\n}\n\nconst parse = async data => {\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * DeleteGroups Request (Version: 1)\n */\n\nmodule.exports = groupIds => Object.assign(requestV0(groupIds), { apiVersion: 1 })\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * DeleteGroups Response (Version: 1) => throttle_time_ms [results]\n *  throttle_time_ms => INT32\n *  results => group_id error_code\n *    group_id => STRING\n *    error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ topics, timeout }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ topics, timeout }), response: response({ topics }) }\n  },\n  1: ({ topics, timeout }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ topics, timeout }), response: response({ topics }) }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DeleteRecords: apiKey } = require('../../apiKeys')\n\n/**\n * DeleteRecords Request (Version: 0) => [topics] timeout_ms\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset\n *       partition => INT32\n *       offset => INT64\n *   timeout => INT32\n */\nmodule.exports = ({ topics, timeout = 5000 }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DeleteRecords',\n  encode: async () => {\n    return new Encoder()\n      .writeArray(\n        topics.map(({ topic, partitions }) => {\n          return new Encoder().writeString(topic).writeArray(\n            partitions.map(({ partition, offset }) => {\n              return new Encoder().writeInt32(partition).writeInt64(offset)\n            })\n          )\n        })\n      )\n      .writeInt32(timeout)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { KafkaJSDeleteTopicRecordsError } = require('../../../../errors')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * DeleteRecords Response (Version: 0) => throttle_time_ms [topics]\n *  throttle_time_ms => INT32\n *  topics => name [partitions]\n *    name => STRING\n *    partitions => partition low_watermark error_code\n *      partition => INT32\n *      low_watermark => INT64\n *      error_code => INT16\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    topics: decoder\n      .readArray(decoder => ({\n        topic: decoder.readString(),\n        partitions: decoder.readArray(decoder => ({\n          partition: decoder.readInt32(),\n          lowWatermark: decoder.readInt64(),\n          errorCode: decoder.readInt16(),\n        })),\n      }))\n      .sort(topicNameComparator),\n  }\n}\n\nconst parse = requestTopics => async data => {\n  const topicsWithErrors = data.topics\n    .map(({ partitions }) => ({\n      partitionsWithErrors: partitions.filter(({ errorCode }) => failure(errorCode)),\n    }))\n    .filter(({ partitionsWithErrors }) => partitionsWithErrors.length)\n\n  if (topicsWithErrors.length > 0) {\n    // at present we only ever request one topic at a time, so can destructure the arrays\n    const [{ topic }] = data.topics // topic name\n    const [{ partitions: requestPartitions }] = requestTopics // requested offset(s)\n    const [{ partitionsWithErrors }] = topicsWithErrors // partition(s) + error(s)\n\n    throw new KafkaJSDeleteTopicRecordsError({\n      topic,\n      partitions: partitionsWithErrors.map(({ partition, errorCode }) => ({\n        partition,\n        error: createErrorFromCode(errorCode),\n        // attach the original offset from the request, onto the error response\n        offset: requestPartitions.find(p => p.partition === partition).offset,\n      })),\n    })\n  }\n\n  return data\n}\n\nmodule.exports = ({ topics }) => ({\n  decode,\n  parse: parse(topics),\n})\n","const requestV0 = require('../v0/request')\n\n/**\n * DeleteRecords Request (Version: 1) => [topics] timeout_ms\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset\n *       partition => INT32\n *       offset => INT64\n *   timeout => INT32\n */\nmodule.exports = ({ topics, timeout }) =>\n  Object.assign(requestV0({ topics, timeout }), { apiVersion: 1 })\n","const responseV0 = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * DeleteRecords Response (Version: 1) => throttle_time_ms [topics]\n *  throttle_time_ms => INT32\n *  topics => name [partitions]\n *    name => STRING\n *    partitions => partition_index low_watermark error_code\n *      partition_index => INT32\n *      low_watermark => INT64\n *      error_code => INT16\n */\n\nmodule.exports = ({ topics }) => {\n  const { parse, decode: decodeV0 } = responseV0({ topics })\n\n  const decode = async rawData => {\n    const decoded = await decodeV0(rawData)\n\n    return {\n      ...decoded,\n      throttleTime: 0,\n      clientSideThrottleTime: decoded.throttleTime,\n    }\n  }\n\n  return {\n    decode,\n    parse,\n  }\n}\n","const versions = {\n  0: ({ topics, timeout }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ topics, timeout }), response }\n  },\n  1: ({ topics, timeout }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ topics, timeout }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DeleteTopics: apiKey } = require('../../apiKeys')\n\n/**\n * DeleteTopics Request (Version: 0) => [topics] timeout\n *   topics => STRING\n *   timeout => INT32\n */\nmodule.exports = ({ topics, timeout = 5000 }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DeleteTopics',\n  encode: async () => {\n    return new Encoder().writeArray(topics).writeInt32(timeout)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * DeleteTopics Response (Version: 0) => [topic_error_codes]\n *   topic_error_codes => topic error_code\n *     topic => STRING\n *     error_code => INT16\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nconst parse = async data => {\n  const topicsWithError = data.topicErrors.filter(({ errorCode }) => failure(errorCode))\n  if (topicsWithError.length > 0) {\n    throw createErrorFromCode(topicsWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * DeleteTopics Request (Version: 1) => [topics] timeout\n *   topics => STRING\n *   timeout => INT32\n */\n\nmodule.exports = ({ topics, timeout }) =>\n  Object.assign(requestV0({ topics, timeout }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * DeleteTopics Response (Version: 1) => throttle_time_ms [topic_error_codes]\n *   throttle_time_ms => INT32\n *   topic_error_codes => topic error_code\n *     topic => STRING\n *     error_code => INT16\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n\n  return {\n    throttleTime: 0,\n    clientSideThrottleTime: throttleTime,\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const versions = {\n  0: ({ resourceType, resourceName, principal, host, operation, permissionType }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ resourceType, resourceName, principal, host, operation, permissionType }),\n      response,\n    }\n  },\n  1: ({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({\n        resourceType,\n        resourceName,\n        resourcePatternType,\n        principal,\n        host,\n        operation,\n        permissionType,\n      }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DescribeAcls: apiKey } = require('../../apiKeys')\n\n/**\n * DescribeAcls Request (Version: 0) => resource_type resource_name principal host operation permission_type\n *   resource_type => INT8\n *   resource_name => NULLABLE_STRING\n *   principal => NULLABLE_STRING\n *   host => NULLABLE_STRING\n *   operation => INT8\n *   permission_type => INT8\n */\n\nmodule.exports = ({ resourceType, resourceName, principal, host, operation, permissionType }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DescribeAcls',\n  encode: async () => {\n    return new Encoder()\n      .writeInt8(resourceType)\n      .writeString(resourceName)\n      .writeString(principal)\n      .writeString(host)\n      .writeInt8(operation)\n      .writeInt8(permissionType)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * DescribeAcls Response (Version: 0) => throttle_time_ms error_code error_message [resources]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   resources => resource_type resource_name [acls]\n *     resource_type => INT8\n *     resource_name => STRING\n *     acls => principal host operation permission_type\n *       principal => STRING\n *       host => STRING\n *       operation => INT8\n *       permission_type => INT8\n */\n\nconst decodeAcls = decoder => ({\n  principal: decoder.readString(),\n  host: decoder.readString(),\n  operation: decoder.readInt8(),\n  permissionType: decoder.readInt8(),\n})\n\nconst decodeResources = decoder => ({\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n  acls: decoder.readArray(decodeAcls),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const errorMessage = decoder.readString()\n  const resources = decoder.readArray(decodeResources)\n\n  return {\n    throttleTime,\n    errorCode,\n    errorMessage,\n    resources,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { DescribeAcls: apiKey } = require('../../apiKeys')\n\n/**\n * DescribeAcls Request (Version: 1) => resource_type resource_name resource_pattern_type_filter principal host operation permission_type\n *   resource_type => INT8\n *   resource_name => NULLABLE_STRING\n *   resource_pattern_type_filter => INT8\n *   principal => NULLABLE_STRING\n *   host => NULLABLE_STRING\n *   operation => INT8\n *   permission_type => INT8\n */\n\nmodule.exports = ({\n  resourceType,\n  resourceName,\n  resourcePatternType,\n  principal,\n  host,\n  operation,\n  permissionType,\n}) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'DescribeAcls',\n  encode: async () => {\n    return new Encoder()\n      .writeInt8(resourceType)\n      .writeString(resourceName)\n      .writeInt8(resourcePatternType)\n      .writeString(principal)\n      .writeString(host)\n      .writeInt8(operation)\n      .writeInt8(permissionType)\n  },\n})\n","const { parse } = require('../v0/response')\nconst Decoder = require('../../../decoder')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n * Version 1 also introduces a new resource pattern type field.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-290%3A+Support+for+Prefixed+ACLs\n *\n * DescribeAcls Response (Version: 1) => throttle_time_ms error_code error_message [resources]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   resources => resource_type resource_name resource_pattern_type [acls]\n *     resource_type => INT8\n *     resource_name => STRING\n *     resource_pattern_type => INT8\n *     acls => principal host operation permission_type\n *       principal => STRING\n *       host => STRING\n *       operation => INT8\n *       permission_type => INT8\n */\nconst decodeAcls = decoder => ({\n  principal: decoder.readString(),\n  host: decoder.readString(),\n  operation: decoder.readInt8(),\n  permissionType: decoder.readInt8(),\n})\n\nconst decodeResources = decoder => ({\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n  resourcePatternType: decoder.readInt8(),\n  acls: decoder.readArray(decodeAcls),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const errorMessage = decoder.readString()\n  const resources = decoder.readArray(decodeResources)\n\n  return {\n    throttleTime: 0,\n    clientSideThrottleTime: throttleTime,\n    errorCode,\n    errorMessage,\n    resources,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ resources }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ resources }), response }\n  },\n  1: ({ resources, includeSynonyms }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ resources, includeSynonyms }), response }\n  },\n  2: ({ resources, includeSynonyms }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ resources, includeSynonyms }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DescribeConfigs: apiKey } = require('../../apiKeys')\n\n/**\n * DescribeConfigs Request (Version: 0) => [resources]\n *   resources => resource_type resource_name [config_names]\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_names => STRING\n */\n\n/**\n * @param {Array} resources An array of config resources to be returned\n */\nmodule.exports = ({ resources }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DescribeConfigs',\n  encode: async () => {\n    return new Encoder().writeArray(resources.map(encodeResource))\n  },\n})\n\nconst encodeResource = ({ type, name, configNames = [] }) => {\n  return new Encoder()\n    .writeInt8(type)\n    .writeString(name)\n    .writeNullableArray(configNames)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst ConfigSource = require('../../../configSource')\nconst ConfigResourceTypes = require('../../../configResourceTypes')\n\n/**\n * DescribeConfigs Response (Version: 0) => throttle_time_ms [resources]\n *   throttle_time_ms => INT32\n *   resources => error_code error_message resource_type resource_name [config_entries]\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_entries => config_name config_value read_only is_default is_sensitive\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *       read_only => BOOLEAN\n *       is_default => BOOLEAN\n *       is_sensitive => BOOLEAN\n */\n\nconst decodeConfigEntries = (decoder, resourceType) => {\n  const configName = decoder.readString()\n  const configValue = decoder.readString()\n  const readOnly = decoder.readBoolean()\n  const isDefault = decoder.readBoolean()\n  const isSensitive = decoder.readBoolean()\n\n  /**\n   * Backporting ConfigSource value to v0\n   * @see https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/requests/DescribeConfigsResponse.java#L232-L242\n   */\n  let configSource\n  if (isDefault) {\n    configSource = ConfigSource.DEFAULT_CONFIG\n  } else {\n    switch (resourceType) {\n      case ConfigResourceTypes.BROKER:\n        configSource = ConfigSource.STATIC_BROKER_CONFIG\n        break\n      case ConfigResourceTypes.TOPIC:\n        configSource = ConfigSource.TOPIC_CONFIG\n        break\n      default:\n        configSource = ConfigSource.UNKNOWN\n    }\n  }\n\n  return {\n    configName,\n    configValue,\n    readOnly,\n    isDefault,\n    configSource,\n    isSensitive,\n  }\n}\n\nconst decodeResources = decoder => {\n  const errorCode = decoder.readInt16()\n  const errorMessage = decoder.readString()\n  const resourceType = decoder.readInt8()\n  const resourceName = decoder.readString()\n  const configEntries = decoder.readArray(decoder => decodeConfigEntries(decoder, resourceType))\n\n  return {\n    errorCode,\n    errorMessage,\n    resourceType,\n    resourceName,\n    configEntries,\n  }\n}\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const resources = decoder.readArray(decodeResources)\n\n  return {\n    throttleTime,\n    resources,\n  }\n}\n\nconst parse = async data => {\n  const resourcesWithError = data.resources.filter(({ errorCode }) => failure(errorCode))\n  if (resourcesWithError.length > 0) {\n    throw createErrorFromCode(resourcesWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { DescribeConfigs: apiKey } = require('../../apiKeys')\n\n/**\n * DescribeConfigs Request (Version: 1) => [resources] include_synonyms\n *   resources => resource_type resource_name [config_names]\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_names => STRING\n *   include_synonyms => BOOLEAN\n */\n\n/**\n * @param {Array} resources An array of config resources to be returned\n * @param [includeSynonyms=false]\n */\nmodule.exports = ({ resources, includeSynonyms = false }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'DescribeConfigs',\n  encode: async () => {\n    return new Encoder().writeArray(resources.map(encodeResource)).writeBoolean(includeSynonyms)\n  },\n})\n\nconst encodeResource = ({ type, name, configNames = [] }) => {\n  return new Encoder()\n    .writeInt8(type)\n    .writeString(name)\n    .writeNullableArray(configNames)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\nconst { DEFAULT_CONFIG } = require('../../../configSource')\n\n/**\n * DescribeConfigs Response (Version: 1) => throttle_time_ms [resources]\n *   throttle_time_ms => INT32\n *   resources => error_code error_message resource_type resource_name [config_entries]\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_entries => config_name config_value read_only config_source is_sensitive [config_synonyms]\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *       read_only => BOOLEAN\n *       config_source => INT8\n *       is_sensitive => BOOLEAN\n *       config_synonyms => config_name config_value config_source\n *         config_name => STRING\n *         config_value => NULLABLE_STRING\n *         config_source => INT8\n */\n\nconst decodeSynonyms = decoder => ({\n  configName: decoder.readString(),\n  configValue: decoder.readString(),\n  configSource: decoder.readInt8(),\n})\n\nconst decodeConfigEntries = decoder => {\n  const configName = decoder.readString()\n  const configValue = decoder.readString()\n  const readOnly = decoder.readBoolean()\n  const configSource = decoder.readInt8()\n  const isSensitive = decoder.readBoolean()\n  const configSynonyms = decoder.readArray(decodeSynonyms)\n\n  return {\n    configName,\n    configValue,\n    readOnly,\n    isDefault: configSource === DEFAULT_CONFIG,\n    configSource,\n    isSensitive,\n    configSynonyms,\n  }\n}\n\nconst decodeResources = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n  configEntries: decoder.readArray(decodeConfigEntries),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const resources = decoder.readArray(decodeResources)\n\n  return {\n    throttleTime,\n    resources,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * DescribeConfigs Request (Version: 1) => [resources] include_synonyms\n *   resources => resource_type resource_name [config_names]\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_names => STRING\n *   include_synonyms => BOOLEAN\n */\n\n/**\n * @param {Array} resources An array of config resources to be returned\n * @param [includeSynonyms=false]\n */\nmodule.exports = ({ resources, includeSynonyms }) =>\n  Object.assign(requestV1({ resources, includeSynonyms }), { apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * Starting in version 2, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * DescribeConfigs Response (Version: 2) => throttle_time_ms [resources]\n *   throttle_time_ms => INT32\n *   resources => error_code error_message resource_type resource_name [config_entries]\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_entries => config_name config_value read_only config_source is_sensitive [config_synonyms]\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *       read_only => BOOLEAN\n *       config_source => INT8\n *       is_sensitive => BOOLEAN\n *       config_synonyms => config_name config_value config_source\n *         config_name => STRING\n *         config_value => NULLABLE_STRING\n *         config_source => INT8\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ groupIds }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ groupIds }), response }\n  },\n  1: ({ groupIds }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ groupIds }), response }\n  },\n  2: ({ groupIds }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ groupIds }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DescribeGroups: apiKey } = require('../../apiKeys')\n\n/**\n * DescribeGroups Request (Version: 0) => [group_ids]\n *   group_ids => STRING\n */\n\n/**\n * @param {Array} groupIds List of groupIds to request metadata for (an empty groupId array will return empty group metadata)\n */\nmodule.exports = ({ groupIds }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DescribeGroups',\n  encode: async () => {\n    return new Encoder().writeArray(groupIds)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * DescribeGroups Response (Version: 0) => [groups]\n *   groups => error_code group_id state protocol_type protocol [members]\n *     error_code => INT16\n *     group_id => STRING\n *     state => STRING\n *     protocol_type => STRING\n *     protocol => STRING\n *     members => member_id client_id client_host member_metadata member_assignment\n *       member_id => STRING\n *       client_id => STRING\n *       client_host => STRING\n *       member_metadata => BYTES\n *       member_assignment => BYTES\n */\n\nconst decoderMember = decoder => ({\n  memberId: decoder.readString(),\n  clientId: decoder.readString(),\n  clientHost: decoder.readString(),\n  memberMetadata: decoder.readBytes(),\n  memberAssignment: decoder.readBytes(),\n})\n\nconst decodeGroup = decoder => ({\n  errorCode: decoder.readInt16(),\n  groupId: decoder.readString(),\n  state: decoder.readString(),\n  protocolType: decoder.readString(),\n  protocol: decoder.readString(),\n  members: decoder.readArray(decoderMember),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const groups = decoder.readArray(decodeGroup)\n\n  return {\n    groups,\n  }\n}\n\nconst parse = async data => {\n  const groupsWithError = data.groups.filter(({ errorCode }) => failure(errorCode))\n  if (groupsWithError.length > 0) {\n    throw createErrorFromCode(groupsWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * DescribeGroups Request (Version: 1) => [group_ids]\n *   group_ids => STRING\n */\n\nmodule.exports = ({ groupIds }) => Object.assign(requestV0({ groupIds }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * DescribeGroups Response (Version: 1) => throttle_time_ms [groups]\n *   throttle_time_ms => INT32\n *   groups => error_code group_id state protocol_type protocol [members]\n *     error_code => INT16\n *     group_id => STRING\n *     state => STRING\n *     protocol_type => STRING\n *     protocol => STRING\n *     members => member_id client_id client_host member_metadata member_assignment\n *       member_id => STRING\n *       client_id => STRING\n *       client_host => STRING\n *       member_metadata => BYTES\n *       member_assignment => BYTES\n */\n\nconst decoderMember = decoder => ({\n  memberId: decoder.readString(),\n  clientId: decoder.readString(),\n  clientHost: decoder.readString(),\n  memberMetadata: decoder.readBytes(),\n  memberAssignment: decoder.readBytes(),\n})\n\nconst decodeGroup = decoder => ({\n  errorCode: decoder.readInt16(),\n  groupId: decoder.readString(),\n  state: decoder.readString(),\n  protocolType: decoder.readString(),\n  protocol: decoder.readString(),\n  members: decoder.readArray(decoderMember),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const groups = decoder.readArray(decodeGroup)\n\n  return {\n    throttleTime,\n    groups,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * DescribeGroups Request (Version: 2) => [group_ids]\n *   group_ids => STRING\n */\n\nmodule.exports = ({ groupIds }) => Object.assign(requestV1({ groupIds }), { apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * Starting in version 2, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * DescribeGroups Response (Version: 2) => throttle_time_ms [groups]\n *   throttle_time_ms => INT32\n *   groups => error_code group_id state protocol_type protocol [members]\n *     error_code => INT16\n *     group_id => STRING\n *     state => STRING\n *     protocol_type => STRING\n *     protocol => STRING\n *     members => member_id client_id client_host member_metadata member_assignment\n *       member_id => STRING\n *       client_id => STRING\n *       client_host => STRING\n *       member_metadata => BYTES\n *       member_assignment => BYTES\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ transactionalId, producerId, producerEpoch, transactionResult }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ transactionalId, producerId, producerEpoch, transactionResult }),\n      response,\n    }\n  },\n  1: ({ transactionalId, producerId, producerEpoch, transactionResult }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ transactionalId, producerId, producerEpoch, transactionResult }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { EndTxn: apiKey } = require('../../apiKeys')\n\n/**\n * EndTxn Request (Version: 0) => transactional_id producer_id producer_epoch transaction_result\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   transaction_result => BOOLEAN\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, transactionResult }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'EndTxn',\n  encode: async () => {\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeInt64(producerId)\n      .writeInt16(producerEpoch)\n      .writeBoolean(transactionResult)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * EndTxn Response (Version: 0) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * EndTxn Request (Version: 1) => transactional_id producer_id producer_epoch transaction_result\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   transaction_result => BOOLEAN\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, transactionResult }) =>\n  Object.assign(requestV0({ transactionalId, producerId, producerEpoch, transactionResult }), {\n    apiVersion: 1,\n  })\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * EndTxn Response (Version: 1) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const ISOLATION_LEVEL = require('../../isolationLevel')\n\n// For normal consumers, use -1\nconst REPLICA_ID = -1\nconst NETWORK_DELAY = 100\n\n/**\n * The FETCH request can block up to maxWaitTime, which can be bigger than the configured\n * request timeout. It's safer to always use the maxWaitTime\n **/\nconst requestTimeout = timeout =>\n  Number.isSafeInteger(timeout + NETWORK_DELAY) ? timeout + NETWORK_DELAY : timeout\n\nconst versions = {\n  0: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ replicaId, maxWaitTime, minBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  1: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ replicaId, maxWaitTime, minBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  2: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return {\n      request: request({ replicaId, maxWaitTime, minBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  3: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, maxBytes, topics }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({ replicaId, maxWaitTime, minBytes, maxBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  4: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n    return {\n      request: request({ replicaId, isolationLevel, maxWaitTime, minBytes, maxBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  5: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v5/request')\n    const response = require('./v5/response')\n    return {\n      request: request({ replicaId, isolationLevel, maxWaitTime, minBytes, maxBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  6: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v6/request')\n    const response = require('./v6/response')\n    return {\n      request: request({ replicaId, isolationLevel, maxWaitTime, minBytes, maxBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  7: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v7/request')\n    const response = require('./v7/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  8: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v8/request')\n    const response = require('./v8/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  9: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v9/request')\n    const response = require('./v9/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  10: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v10/request')\n    const response = require('./v10/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  11: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n    rackId,\n  }) => {\n    const request = require('./v11/request')\n    const response = require('./v11/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n        rackId,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\n\n/**\n * Fetch Request (Version: 0) => replica_id max_wait_time min_bytes [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       max_bytes => INT32\n */\n\n/**\n * @param {number} replicaId Broker id of the follower\n * @param {number} maxWaitTime Maximum time in ms to wait for the response\n * @param {number} minBytes Minimum bytes to accumulate in the response.\n * @param {Array} topics Topics to fetch\n *                        [\n *                          {\n *                            topic: 'topic-name',\n *                            partitions: [\n *                              {\n *                                partition: 0,\n *                                fetchOffset: '4124',\n *                                maxBytes: 2048\n *                              }\n *                            ]\n *                          }\n *                        ]\n */\nmodule.exports = ({ replicaId, maxWaitTime, minBytes, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { KafkaJSOffsetOutOfRange } = require('../../../../errors')\nconst { failure, createErrorFromCode, errorCodes } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\nconst MessageSetDecoder = require('../../../messageSet/decoder')\n\n/**\n * Fetch Response (Version: 0) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *       record_set => RECORDS\n */\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  messages: await MessageSetDecoder(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    responses,\n  }\n}\n\nconst { code: OFFSET_OUT_OF_RANGE_ERROR_CODE } = errorCodes.find(\n  e => e.type === 'OFFSET_OUT_OF_RANGE'\n)\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(({ topicName, partitions }) => {\n    return partitions\n      .filter(partition => failure(partition.errorCode))\n      .map(partition => Object.assign({}, partition, { topic: topicName }))\n  })\n\n  const errors = flatten(partitionsWithError)\n  if (errors.length > 0) {\n    const { errorCode, topic, partition } = errors[0]\n    if (errorCode === OFFSET_OUT_OF_RANGE_ERROR_CODE) {\n      throw new KafkaJSOffsetOutOfRange(createErrorFromCode(errorCode), { topic, partition })\n    }\n\n    throw createErrorFromCode(errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\nmodule.exports = ({ replicaId, maxWaitTime, minBytes, topics }) => {\n  return Object.assign(requestV0({ replicaId, maxWaitTime, minBytes, topics }), { apiVersion: 1 })\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\nconst MessageSetDecoder = require('../../../messageSet/decoder')\n\n/**\n * Fetch Response (Version: 1) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *       record_set => RECORDS\n */\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  messages: await MessageSetDecoder(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    throttleTime,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const ISOLATION_LEVEL = require('../../../isolationLevel')\nconst requestV9 = require('../v9/request')\n\n/**\n * ZStd Compression\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression\n */\n\n/**\n * Fetch Request (Version: 10) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition current_leader_epoch fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       current_leader_epoch => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) =>\n  Object.assign(\n    requestV9({\n      replicaId,\n      maxWaitTime,\n      minBytes,\n      maxBytes,\n      topics,\n      isolationLevel,\n      sessionId,\n      sessionEpoch,\n      forgottenTopics,\n    }),\n    { apiVersion: 10 }\n  )\n","const { decode, parse } = require('../v9/response')\n\n/**\n * Fetch Response (Version: 10) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Allow consumers to fetch from closest replica\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica\n */\n\n/**\n * Fetch Request (Version: 11) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition current_leader_epoch fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       current_leader_epoch => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n *   rack_id => STRING\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  rackId = '',\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) => ({\n  apiKey,\n  apiVersion: 11,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeInt32(sessionId)\n      .writeInt32(sessionEpoch)\n      .writeArray(topics.map(encodeTopic))\n      .writeArray(forgottenTopics.map(encodeForgottenTopics))\n      .writeString(rackId)\n  },\n})\n\nconst encodeForgottenTopics = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions)\n}\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({\n  partition,\n  currentLeaderEpoch = -1,\n  fetchOffset,\n  logStartOffset = -1,\n  maxBytes,\n}) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(currentLeaderEpoch)\n    .writeInt64(fetchOffset)\n    .writeInt64(logStartOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('../v4/decodeMessages')\n\n/**\n * Fetch Response (Version: 11) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *         preferred_read_replica => INT32\n *       record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  lastStartOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  preferredReadReplica: decoder.readInt32(),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const clientSideThrottleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const sessionId = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  // Report a `throttleTime` of 0: The broker will not have throttled\n  // this request, but if the `clientSideThrottleTime` is >0 then it\n  // expects us to do that -- and it will ignore requests.\n  return {\n    throttleTime: 0,\n    clientSideThrottleTime,\n    errorCode,\n    sessionId,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const requestV0 = require('../v0/request')\n\nmodule.exports = ({ replicaId, maxWaitTime, minBytes, topics }) => {\n  return Object.assign(requestV0({ replicaId, maxWaitTime, minBytes, topics }), { apiVersion: 2 })\n}\n","const { decode, parse } = require('../v1/response')\n\n/**\n * Fetch Response (Version: 2) => throttle_time_ms [responses]\n *  throttle_time_ms => INT32\n *  responses => topic [partition_responses]\n *    topic => STRING\n *    partition_responses => partition_header record_set\n *      partition_header => partition error_code high_watermark\n *        partition => INT32\n *        error_code => INT16\n *        high_watermark => INT64\n *      record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\n\n/**\n * Fetch Request (Version: 3) => replica_id max_wait_time min_bytes max_bytes [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       max_bytes => INT32\n */\n\n/**\n * @param {number} replicaId Broker id of the follower\n * @param {number} maxWaitTime Maximum time in ms to wait for the response\n * @param {number} minBytes Minimum bytes to accumulate in the response.\n * @param {number} maxBytes Maximum bytes to accumulate in the response. Note that this is not an absolute maximum,\n *                          if the first message in the first non-empty partition of the fetch is larger than this value,\n *                          the message will still be returned to ensure that progress can be made.\n * @param {Array} topics Topics to fetch\n *                        [\n *                          {\n *                            topic: 'topic-name',\n *                            partitions: [\n *                              {\n *                                partition: 0,\n *                                fetchOffset: '4124',\n *                                maxBytes: 2048\n *                              }\n *                            ]\n *                          }\n *                        ]\n */\nmodule.exports = ({ replicaId, maxWaitTime, minBytes, maxBytes, topics }) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt32(maxBytes)\n}\n","const { decode, parse } = require('../v1/response')\n\n/**\n * Fetch Response (Version: 3) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *       record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Decoder = require('../../../decoder')\nconst MessageSetDecoder = require('../../../messageSet/decoder')\nconst RecordBatchDecoder = require('../../../recordBatch/v0/decoder')\nconst { MAGIC_BYTE } = require('../../../recordBatch/v0')\n\n// the magic offset is at the same offset for all current message formats, but the 4 bytes\n// between the size and the magic is dependent on the version.\nconst MAGIC_OFFSET = 16\nconst RECORD_BATCH_OVERHEAD = 49\n\nconst decodeMessages = async decoder => {\n  const messagesSize = decoder.readInt32()\n\n  if (messagesSize <= 0 || !decoder.canReadBytes(messagesSize)) {\n    return []\n  }\n\n  const messagesBuffer = decoder.readBytes(messagesSize)\n  const messagesDecoder = new Decoder(messagesBuffer)\n  const magicByte = messagesBuffer.slice(MAGIC_OFFSET).readInt8(0)\n\n  if (magicByte === MAGIC_BYTE) {\n    const records = []\n\n    while (messagesDecoder.canReadBytes(RECORD_BATCH_OVERHEAD)) {\n      try {\n        const recordBatch = await RecordBatchDecoder(messagesDecoder)\n        records.push(...recordBatch.records)\n      } catch (e) {\n        // The tail of the record batches can have incomplete records\n        // due to how maxBytes works. See https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-FetchAPI\n        if (e.name === 'KafkaJSPartialMessageError') {\n          break\n        }\n\n        throw e\n      }\n    }\n\n    return records\n  }\n\n  return MessageSetDecoder(messagesDecoder, messagesSize)\n}\n\nmodule.exports = decodeMessages\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Fetch Request (Version: 4) => replica_id max_wait_time min_bytes max_bytes isolation_level [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       max_bytes => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n}) => ({\n  apiKey,\n  apiVersion: 4,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('./decodeMessages')\n\n/**\n * Fetch Response (Version: 4) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    throttleTime,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Fetch Request (Version: 5) => replica_id max_wait_time min_bytes max_bytes isolation_level [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n}) => ({\n  apiKey,\n  apiVersion: 5,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, logStartOffset = -1, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt64(logStartOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('../v4/decodeMessages')\n\n/**\n * Fetch Response (Version: 5) => throttle_time_ms [responses]\n *  throttle_time_ms => INT32\n *  responses => topic [partition_responses]\n *    topic => STRING\n *    partition_responses => partition_header record_set\n *      partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *        partition => INT32\n *        error_code => INT16\n *        high_watermark => INT64\n *        last_stable_offset => INT64\n *        log_start_offset => INT64\n *        aborted_transactions => producer_id first_offset\n *          producer_id => INT64\n *          first_offset => INT64\n *      record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  lastStartOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    throttleTime,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const ISOLATION_LEVEL = require('../../../isolationLevel')\nconst requestV5 = require('../v5/request')\n\n/**\n * Fetch Request (Version: 6) => replica_id max_wait_time min_bytes max_bytes isolation_level [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n}) =>\n  Object.assign(\n    requestV5({\n      replicaId,\n      maxWaitTime,\n      minBytes,\n      maxBytes,\n      topics,\n      isolationLevel,\n    }),\n    { apiVersion: 6 }\n  )\n","const { decode, parse } = require('../v5/response')\n\n/**\n * Fetch Response (Version: 6) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Sessions are only used by followers\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-227%3A+Introduce+Incremental+FetchRequests+to+Increase+Partition+Scalability\n */\n\n/**\n * Fetch Request (Version: 7) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) => ({\n  apiKey,\n  apiVersion: 7,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeInt32(sessionId)\n      .writeInt32(sessionEpoch)\n      .writeArray(topics.map(encodeTopic))\n      .writeArray(forgottenTopics.map(encodeForgottenTopics))\n  },\n})\n\nconst encodeForgottenTopics = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions)\n}\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, logStartOffset = -1, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt64(logStartOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('../v4/decodeMessages')\n\n/**\n * Fetch Response (Version: 7) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  lastStartOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const sessionId = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    throttleTime,\n    errorCode,\n    sessionId,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const ISOLATION_LEVEL = require('../../../isolationLevel')\nconst requestV7 = require('../v7/request')\n\n/**\n * Quota violation brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n */\n\n/**\n * Fetch Request (Version: 8) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) =>\n  Object.assign(\n    requestV7({\n      replicaId,\n      maxWaitTime,\n      minBytes,\n      maxBytes,\n      topics,\n      isolationLevel,\n      sessionId,\n      sessionEpoch,\n      forgottenTopics,\n    }),\n    { apiVersion: 8 }\n  )\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('../v4/decodeMessages')\n\n/**\n * Fetch Response (Version: 8) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  lastStartOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const clientSideThrottleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const sessionId = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  // Report a `throttleTime` of 0: The broker will not have throttled\n  // this request, but if the `clientSideThrottleTime` is >0 then it\n  // expects us to do that -- and it will ignore requests.\n  return {\n    throttleTime: 0,\n    clientSideThrottleTime,\n    errorCode,\n    sessionId,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Allow fetchers to detect and handle log truncation\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-320%3A+Allow+fetchers+to+detect+and+handle+log+truncation\n */\n\n/**\n * Fetch Request (Version: 9) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition current_leader_epoch fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       current_leader_epoch => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) => ({\n  apiKey,\n  apiVersion: 9,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeInt32(sessionId)\n      .writeInt32(sessionEpoch)\n      .writeArray(topics.map(encodeTopic))\n      .writeArray(forgottenTopics.map(encodeForgottenTopics))\n  },\n})\n\nconst encodeForgottenTopics = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions)\n}\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({\n  partition,\n  currentLeaderEpoch = -1,\n  fetchOffset,\n  logStartOffset = -1,\n  maxBytes,\n}) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(currentLeaderEpoch)\n    .writeInt64(fetchOffset)\n    .writeInt64(logStartOffset)\n    .writeInt32(maxBytes)\n}\n","const { decode, parse } = require('../v8/response')\n\n/**\n * Fetch Response (Version: 9) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const COORDINATOR_TYPES = require('../../coordinatorTypes')\n\nconst versions = {\n  0: ({ groupId }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ groupId }), response }\n  },\n  1: ({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ coordinatorKey: groupId, coordinatorType }), response }\n  },\n  2: ({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ coordinatorKey: groupId, coordinatorType }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { GroupCoordinator: apiKey } = require('../../apiKeys')\n\n/**\n * FindCoordinator Request (Version: 0) => group_id\n *   group_id => STRING\n */\n\nmodule.exports = ({ groupId }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'GroupCoordinator',\n  encode: async () => {\n    return new Encoder().writeString(groupId)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * FindCoordinator Response (Version: 0) => error_code coordinator\n *  error_code => INT16\n *  coordinator => node_id host port\n *    node_id => INT32\n *    host => STRING\n *    port => INT32\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  const coordinator = {\n    nodeId: decoder.readInt32(),\n    host: decoder.readString(),\n    port: decoder.readInt32(),\n  }\n\n  return {\n    errorCode,\n    coordinator,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { GroupCoordinator: apiKey } = require('../../apiKeys')\n\n/**\n * FindCoordinator Request (Version: 1) => coordinator_key coordinator_type\n *   coordinator_key => STRING\n *   coordinator_type => INT8\n */\n\nmodule.exports = ({ coordinatorKey, coordinatorType }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'GroupCoordinator',\n  encode: async () => {\n    return new Encoder().writeString(coordinatorKey).writeInt8(coordinatorType)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * FindCoordinator Response (Version: 1) => throttle_time_ms error_code error_message coordinator\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   coordinator => node_id host port\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  const errorMessage = decoder.readString()\n  const coordinator = {\n    nodeId: decoder.readInt32(),\n    host: decoder.readString(),\n    port: decoder.readInt32(),\n  }\n\n  return {\n    throttleTime,\n    errorCode,\n    errorMessage,\n    coordinator,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * FindCoordinator Request (Version: 2) => coordinator_key coordinator_type\n *   coordinator_key => STRING\n *   coordinator_type => INT8\n */\n\nmodule.exports = ({ coordinatorKey, coordinatorType }) =>\n  Object.assign(requestV1({ coordinatorKey, coordinatorType }), { apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * Starting in version 2, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * FindCoordinator Response (Version: 1) => throttle_time_ms error_code error_message coordinator\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   coordinator => node_id host port\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ groupId, groupGenerationId, memberId }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ groupId, groupGenerationId, memberId }),\n      response,\n    }\n  },\n  1: ({ groupId, groupGenerationId, memberId }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ groupId, groupGenerationId, memberId }),\n      response,\n    }\n  },\n  2: ({ groupId, groupGenerationId, memberId }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return {\n      request: request({ groupId, groupGenerationId, memberId }),\n      response,\n    }\n  },\n  3: ({ groupId, groupGenerationId, memberId, groupInstanceId }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({ groupId, groupGenerationId, memberId, groupInstanceId }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { Heartbeat: apiKey } = require('../../apiKeys')\n\n/**\n * Heartbeat Request (Version: 0) => group_id group_generation_id member_id\n *   group_id => STRING\n *   group_generation_id => INT32\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'Heartbeat',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(groupGenerationId)\n      .writeString(memberId)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * Heartbeat Response (Version: 0) => error_code\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return { errorCode }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * Heartbeat Request (Version: 1) => group_id generation_id member_id\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId }) =>\n  Object.assign(requestV0({ groupId, groupGenerationId, memberId }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Heartbeat Response (Version: 1) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return { throttleTime, errorCode }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * Heartbeat Request (Version: 2) => group_id generation_id member_id\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId }) =>\n  Object.assign(requestV1({ groupId, groupGenerationId, memberId }), { apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * In version 2 on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * Heartbeat Response (Version: 2) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { Heartbeat: apiKey } = require('../../apiKeys')\n\n/**\n * Version 3 adds group_instance_id to indicate member identity across restarts.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances\n *\n * Heartbeat Request (Version: 3) => group_id generation_id member_id group_instance_id\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   group_instance_id => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, groupInstanceId }) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'Heartbeat',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(groupGenerationId)\n      .writeString(memberId)\n      .writeString(groupInstanceId)\n  },\n})\n","const { parse, decode } = require('../v2/response')\n\n/**\n * Heartbeat Response (Version: 3) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\nmodule.exports = {\n  decode,\n  parse,\n}\n","const apiKeys = require('./apiKeys')\nconst { KafkaJSServerDoesNotSupportApiKey, KafkaJSNotImplemented } = require('../../errors')\n\n/**\n * @typedef {(options?: Object) => { request: any, response: any, logResponseErrors?: boolean }} Request\n */\n\n/**\n * @typedef {Object} RequestDefinitions\n * @property {string[]} versions\n * @property {({ version: number }) => Request} protocol\n */\n\n/**\n * @typedef {(apiKey: number, definitions: RequestDefinitions) => Request} Lookup\n */\n\n/** @type {RequestDefinitions} */\nconst noImplementedRequestDefinitions = {\n  versions: [],\n  protocol: () => {\n    throw new KafkaJSNotImplemented()\n  },\n}\n\n/**\n * @type {{[apiName: string]: RequestDefinitions}}\n */\nconst requests = {\n  Produce: require('./produce'),\n  Fetch: require('./fetch'),\n  ListOffsets: require('./listOffsets'),\n  Metadata: require('./metadata'),\n  LeaderAndIsr: noImplementedRequestDefinitions,\n  StopReplica: noImplementedRequestDefinitions,\n  UpdateMetadata: noImplementedRequestDefinitions,\n  ControlledShutdown: noImplementedRequestDefinitions,\n  OffsetCommit: require('./offsetCommit'),\n  OffsetFetch: require('./offsetFetch'),\n  GroupCoordinator: require('./findCoordinator'),\n  JoinGroup: require('./joinGroup'),\n  Heartbeat: require('./heartbeat'),\n  LeaveGroup: require('./leaveGroup'),\n  SyncGroup: require('./syncGroup'),\n  DescribeGroups: require('./describeGroups'),\n  ListGroups: require('./listGroups'),\n  SaslHandshake: require('./saslHandshake'),\n  ApiVersions: require('./apiVersions'),\n  CreateTopics: require('./createTopics'),\n  DeleteTopics: require('./deleteTopics'),\n  DeleteRecords: require('./deleteRecords'),\n  InitProducerId: require('./initProducerId'),\n  OffsetForLeaderEpoch: noImplementedRequestDefinitions,\n  AddPartitionsToTxn: require('./addPartitionsToTxn'),\n  AddOffsetsToTxn: require('./addOffsetsToTxn'),\n  EndTxn: require('./endTxn'),\n  WriteTxnMarkers: noImplementedRequestDefinitions,\n  TxnOffsetCommit: require('./txnOffsetCommit'),\n  DescribeAcls: require('./describeAcls'),\n  CreateAcls: require('./createAcls'),\n  DeleteAcls: require('./deleteAcls'),\n  DescribeConfigs: require('./describeConfigs'),\n  AlterConfigs: require('./alterConfigs'),\n  AlterReplicaLogDirs: noImplementedRequestDefinitions,\n  DescribeLogDirs: noImplementedRequestDefinitions,\n  SaslAuthenticate: require('./saslAuthenticate'),\n  CreatePartitions: require('./createPartitions'),\n  CreateDelegationToken: noImplementedRequestDefinitions,\n  RenewDelegationToken: noImplementedRequestDefinitions,\n  ExpireDelegationToken: noImplementedRequestDefinitions,\n  DescribeDelegationToken: noImplementedRequestDefinitions,\n  DeleteGroups: require('./deleteGroups'),\n}\n\nconst names = Object.keys(apiKeys)\nconst keys = Object.values(apiKeys)\nconst findApiName = apiKey => names[keys.indexOf(apiKey)]\n\n/**\n * @param {import(\"../../../types\").ApiVersions} versions\n * @returns {Lookup}\n */\nconst lookup = versions => (apiKey, definition) => {\n  const version = versions[apiKey]\n  const availableVersions = definition.versions.map(Number)\n  const bestImplementedVersion = Math.max(...availableVersions)\n\n  if (!version || version.maxVersion == null) {\n    throw new KafkaJSServerDoesNotSupportApiKey(\n      `The Kafka server does not support the requested API version`,\n      { apiKey, apiName: findApiName(apiKey) }\n    )\n  }\n\n  const bestSupportedVersion = Math.min(bestImplementedVersion, version.maxVersion)\n  return definition.protocol({ version: bestSupportedVersion })\n}\n\nmodule.exports = {\n  requests,\n  lookup,\n}\n","const versions = {\n  0: ({ transactionalId, transactionTimeout = 5000 }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ transactionalId, transactionTimeout }), response }\n  },\n  1: ({ transactionalId, transactionTimeout = 5000 }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ transactionalId, transactionTimeout }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { InitProducerId: apiKey } = require('../../apiKeys')\n\n/**\n * InitProducerId Request (Version: 0) => transactional_id transaction_timeout_ms\n *   transactional_id => NULLABLE_STRING\n *   transaction_timeout_ms => INT32\n */\n\nmodule.exports = ({ transactionalId, transactionTimeout }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'InitProducerId',\n  encode: async () => {\n    return new Encoder().writeString(transactionalId).writeInt32(transactionTimeout)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * InitProducerId Response (Version: 0) => throttle_time_ms error_code producer_id producer_epoch\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   producer_id => INT64\n *   producer_epoch => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n    producerId: decoder.readInt64().toString(),\n    producerEpoch: decoder.readInt16(),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * InitProducerId Request (Version: 1) => transactional_id transaction_timeout_ms\n *   transactional_id => NULLABLE_STRING\n *   transaction_timeout_ms => INT32\n */\n\nmodule.exports = ({ transactionalId, transactionTimeout }) =>\n  Object.assign(requestV0({ transactionalId, transactionTimeout }), { apiVersion: 1 })\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * InitProducerId Response (Version: 0) => throttle_time_ms error_code producer_id producer_epoch\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   producer_id => INT64\n *   producer_epoch => INT16\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const NETWORK_DELAY = 5000\n\n/**\n * @see https://github.com/apache/kafka/pull/5203\n * The JOIN_GROUP request may block up to sessionTimeout (or rebalanceTimeout in JoinGroupV1),\n * so we should override the requestTimeout to be a bit more than the sessionTimeout\n * NOTE: the sessionTimeout can be configured as Number.MAX_SAFE_INTEGER and overflow when\n * increased, so we have to check for potential overflows\n **/\nconst requestTimeout = ({ rebalanceTimeout, sessionTimeout }) => {\n  const timeout = rebalanceTimeout || sessionTimeout\n  return Number.isSafeInteger(timeout + NETWORK_DELAY) ? timeout + NETWORK_DELAY : timeout\n}\n\nconst logResponseError = memberId => memberId != null && memberId !== ''\n\nconst versions = {\n  0: ({ groupId, sessionTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout: null, sessionTimeout }),\n    }\n  },\n  1: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        rebalanceTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),\n    }\n  },\n  2: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        rebalanceTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),\n    }\n  },\n  3: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        rebalanceTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),\n    }\n  },\n  4: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        rebalanceTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),\n      logResponseError: logResponseError(memberId),\n    }\n  },\n  5: ({\n    groupId,\n    sessionTimeout,\n    rebalanceTimeout,\n    memberId,\n    groupInstanceId,\n    protocolType,\n    groupProtocols,\n  }) => {\n    const request = require('./v5/request')\n    const response = require('./v5/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        rebalanceTimeout,\n        memberId,\n        groupInstanceId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),\n      logResponseError: logResponseError(memberId),\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { JoinGroup: apiKey } = require('../../apiKeys')\n\n/**\n * JoinGroup Request (Version: 0) => group_id session_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({ groupId, sessionTimeout, memberId, protocolType, groupProtocols }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'JoinGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(sessionTimeout)\n      .writeString(memberId)\n      .writeString(protocolType)\n      .writeArray(groupProtocols.map(encodeGroupProtocols))\n  },\n})\n\nconst encodeGroupProtocols = ({ name, metadata = Buffer.alloc(0) }) => {\n  return new Encoder().writeString(name).writeBytes(metadata)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * JoinGroup Response (Version: 0) => error_code generation_id group_protocol leader_id member_id [members]\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    errorCode,\n    generationId: decoder.readInt32(),\n    groupProtocol: decoder.readString(),\n    leaderId: decoder.readString(),\n    memberId: decoder.readString(),\n    members: decoder.readArray(decoder => ({\n      memberId: decoder.readString(),\n      memberMetadata: decoder.readBytes(),\n    })),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { JoinGroup: apiKey } = require('../../apiKeys')\n\n/**\n * JoinGroup Request (Version: 1) => group_id session_timeout rebalance_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   rebalance_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  sessionTimeout,\n  rebalanceTimeout,\n  memberId,\n  protocolType,\n  groupProtocols,\n}) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'JoinGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(sessionTimeout)\n      .writeInt32(rebalanceTimeout)\n      .writeString(memberId)\n      .writeString(protocolType)\n      .writeArray(groupProtocols.map(encodeGroupProtocols))\n  },\n})\n\nconst encodeGroupProtocols = ({ name, metadata = Buffer.alloc(0) }) => {\n  return new Encoder().writeString(name).writeBytes(metadata)\n}\n","const { parse, decode } = require('../v0/response')\n\n/**\n * JoinGroup Response (Version: 1) => error_code generation_id group_protocol leader_id member_id [members]\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * JoinGroup Request (Version: 2) => group_id session_timeout rebalance_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   rebalance_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  sessionTimeout,\n  rebalanceTimeout,\n  memberId,\n  protocolType,\n  groupProtocols,\n}) =>\n  Object.assign(\n    requestV1({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId,\n      protocolType,\n      groupProtocols,\n    }),\n    { apiVersion: 2 }\n  )\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * JoinGroup Response (Version: 2) => throttle_time_ms error_code generation_id group_protocol leader_id member_id [members]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n    generationId: decoder.readInt32(),\n    groupProtocol: decoder.readString(),\n    leaderId: decoder.readString(),\n    memberId: decoder.readString(),\n    members: decoder.readArray(decoder => ({\n      memberId: decoder.readString(),\n      memberMetadata: decoder.readBytes(),\n    })),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV2 = require('../v2/request')\n\n/**\n * JoinGroup Request (Version: 3) => group_id session_timeout rebalance_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   rebalance_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  sessionTimeout,\n  rebalanceTimeout,\n  memberId,\n  protocolType,\n  groupProtocols,\n}) =>\n  Object.assign(\n    requestV2({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId,\n      protocolType,\n      groupProtocols,\n    }),\n    { apiVersion: 3 }\n  )\n","const { parse, decode: decodeV2 } = require('../v2/response')\n\n/**\n * Starting in version 3, on quota violation, brokers send out responses\n * before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * JoinGroup Response (Version: 3) => throttle_time_ms error_code generation_id group_protocol leader_id member_id [members]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\nconst decode = async rawData => {\n  const decoded = await decodeV2(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV3 = require('../v3/request')\n\n/**\n * Starting in version 4, the client needs to issue a second request to join group\n * with assigned id.\n *\n * JoinGroup Request (Version: 4) => group_id session_timeout rebalance_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   rebalance_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  sessionTimeout,\n  rebalanceTimeout,\n  memberId,\n  protocolType,\n  groupProtocols,\n}) =>\n  Object.assign(\n    requestV3({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId,\n      protocolType,\n      groupProtocols,\n    }),\n    { apiVersion: 4 }\n  )\n","const { decode } = require('../v3/response')\nconst { KafkaJSMemberIdRequired } = require('../../../../errors')\nconst { failure, createErrorFromCode, errorCodes } = require('../../../error')\n\n/**\n * JoinGroup Response (Version: 4) => throttle_time_ms error_code generation_id group_protocol leader_id member_id [members]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\n\nconst { code: MEMBER_ID_REQUIRED_ERROR_CODE } = errorCodes.find(\n  e => e.type === 'MEMBER_ID_REQUIRED'\n)\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    if (data.errorCode === MEMBER_ID_REQUIRED_ERROR_CODE) {\n      throw new KafkaJSMemberIdRequired(createErrorFromCode(data.errorCode), {\n        memberId: data.memberId,\n      })\n    }\n\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { JoinGroup: apiKey } = require('../../apiKeys')\n\n/**\n * Version 5 adds group_instance_id to identify members across restarts.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances\n *\n * JoinGroup Request (Version: 5) => group_id session_timeout rebalance_timeout member_id group_instance_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   rebalance_timeout => INT32\n *   member_id => STRING\n *   group_instance_id => NULLABLE_STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  sessionTimeout,\n  rebalanceTimeout,\n  memberId,\n  groupInstanceId = null,\n  protocolType,\n  groupProtocols,\n}) => ({\n  apiKey,\n  apiVersion: 5,\n  apiName: 'JoinGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(sessionTimeout)\n      .writeInt32(rebalanceTimeout)\n      .writeString(memberId)\n      .writeString(groupInstanceId)\n      .writeString(protocolType)\n      .writeArray(groupProtocols.map(encodeGroupProtocols))\n  },\n})\n\nconst encodeGroupProtocols = ({ name, metadata = Buffer.alloc(0) }) => {\n  return new Encoder().writeString(name).writeBytes(metadata)\n}\n","const Decoder = require('../../../decoder')\nconst { KafkaJSMemberIdRequired } = require('../../../../errors')\nconst {\n  failure,\n  createErrorFromCode,\n  errorCodes,\n  failIfVersionNotSupported,\n} = require('../../../error')\n\n/**\n * JoinGroup Response (Version: 5) => throttle_time_ms error_code generation_id group_protocol leader_id member_id [members]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id group_instance_id metadata\n *     member_id => STRING\n *     group_instance_id => NULLABLE_STRING\n *     member_metadata => BYTES\n */\nconst { code: MEMBER_ID_REQUIRED_ERROR_CODE } = errorCodes.find(\n  e => e.type === 'MEMBER_ID_REQUIRED'\n)\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    if (data.errorCode === MEMBER_ID_REQUIRED_ERROR_CODE) {\n      throw new KafkaJSMemberIdRequired(createErrorFromCode(data.errorCode), {\n        memberId: data.memberId,\n      })\n    }\n\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime: 0,\n    clientSideThrottleTime: throttleTime,\n    errorCode,\n    generationId: decoder.readInt32(),\n    groupProtocol: decoder.readString(),\n    leaderId: decoder.readString(),\n    memberId: decoder.readString(),\n    members: decoder.readArray(decoder => ({\n      memberId: decoder.readString(),\n      groupInstanceId: decoder.readString(),\n      memberMetadata: decoder.readBytes(),\n    })),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ groupId, memberId }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ groupId, memberId }),\n      response,\n    }\n  },\n  1: ({ groupId, memberId }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ groupId, memberId }),\n      response,\n    }\n  },\n  2: ({ groupId, memberId }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return {\n      request: request({ groupId, memberId }),\n      response,\n    }\n  },\n  3: ({ groupId, memberId, groupInstanceId }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({ groupId, members: [{ memberId, groupInstanceId }] }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { LeaveGroup: apiKey } = require('../../apiKeys')\n\n/**\n * LeaveGroup Request (Version: 0) => group_id member_id\n *   group_id => STRING\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, memberId }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'LeaveGroup',\n  encode: async () => {\n    return new Encoder().writeString(groupId).writeString(memberId)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * LeaveGroup Response (Version: 0) => error_code\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return { errorCode }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * LeaveGroup Request (Version: 1) => group_id member_id\n *   group_id => STRING\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, memberId }) =>\n  Object.assign(requestV0({ groupId, memberId }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * LeaveGroup Response (Version: 1) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return { throttleTime, errorCode }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * LeaveGroup Request (Version: 2) => group_id member_id\n *   group_id => STRING\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, memberId }) =>\n  Object.assign(requestV1({ groupId, memberId }), { apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * In version 2 on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * LeaveGroup Response (Version: 2) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { LeaveGroup: apiKey } = require('../../apiKeys')\n\n/**\n * Version 3 changes leavegroup to operate on a batch of members\n * and adds group_instance_id to identify members across restarts.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances\n *\n * LeaveGroup Request (Version: 3) => group_id [members]\n *   group_id => STRING\n *   members => member_id group_instance_id\n *     member_id => STRING\n *     group_instance_id => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, members }) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'LeaveGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeArray(members.map(member => encodeMember(member)))\n  },\n})\n\nconst encodeMember = ({ memberId, groupInstanceId = null }) => {\n  return new Encoder().writeString(memberId).writeString(groupInstanceId)\n}\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported, failure, createErrorFromCode } = require('../../../error')\nconst { parse: parseV2 } = require('../v2/response')\n\n/**\n * LeaveGroup Response (Version: 3) => throttle_time_ms error_code [members]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   members => member_id group_instance_id error_code\n *     member_id => STRING\n *     group_instance_id => NULLABLE_STRING\n *     error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const members = decoder.readArray(decodeMembers)\n\n  failIfVersionNotSupported(errorCode)\n\n  return { throttleTime: 0, clientSideThrottleTime: throttleTime, errorCode, members }\n}\n\nconst decodeMembers = decoder => ({\n  memberId: decoder.readString(),\n  groupInstanceId: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  const parsed = parseV2(data)\n\n  const memberWithError = data.members.find(member => failure(member.errorCode))\n  if (memberWithError) {\n    throw createErrorFromCode(memberWithError.errorCode)\n  }\n\n  return parsed\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: () => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request(), response }\n  },\n  1: () => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request(), response }\n  },\n  2: () => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request(), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { ListGroups: apiKey } = require('../../apiKeys')\n\n/**\n * ListGroups Request (Version: 0)\n */\n\n/**\n */\nmodule.exports = () => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'ListGroups',\n  encode: async () => {\n    return new Encoder()\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * ListGroups Response (Version: 0) => error_code [groups]\n *   error_code => INT16\n *   groups => group_id protocol_type\n *     group_id => STRING\n *     protocol_type => STRING\n */\n\nconst decodeGroup = decoder => ({\n  groupId: decoder.readString(),\n  protocolType: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n  const groups = decoder.readArray(decodeGroup)\n\n  return {\n    errorCode,\n    groups,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decodeGroup,\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * ListGroups Request (Version: 1)\n */\n\nmodule.exports = () => Object.assign(requestV0(), { apiVersion: 1 })\n","const responseV0 = require('../v0/response')\n\nconst Decoder = require('../../../decoder')\n\n/**\n * ListGroups Response (Version: 1) => error_code [groups]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   groups => group_id protocol_type\n *     group_id => STRING\n *     protocol_type => STRING\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const groups = decoder.readArray(responseV0.decodeGroup)\n\n  return {\n    throttleTime,\n    errorCode,\n    groups,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: responseV0.parse,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * ListGroups Request (Version: 2)\n */\n\nmodule.exports = () => Object.assign(requestV1(), { apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * In version 2 on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * ListGroups Response (Version: 2) => error_code [groups]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   groups => group_id protocol_type\n *     group_id => STRING\n *     protocol_type => STRING\n */\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const ISOLATION_LEVEL = require('../../isolationLevel')\n\n// For normal consumers, use -1\nconst REPLICA_ID = -1\n\nconst versions = {\n  0: ({ replicaId = REPLICA_ID, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ replicaId, topics }), response }\n  },\n  1: ({ replicaId = REPLICA_ID, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ replicaId, topics }), response }\n  },\n  2: ({ replicaId = REPLICA_ID, isolationLevel = ISOLATION_LEVEL.READ_COMMITTED, topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ replicaId, isolationLevel, topics }), response }\n  },\n  3: ({ replicaId = REPLICA_ID, isolationLevel = ISOLATION_LEVEL.READ_COMMITTED, topics }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return { request: request({ replicaId, isolationLevel, topics }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { ListOffsets: apiKey } = require('../../apiKeys')\n\n/**\n * ListOffsets Request (Version: 0) => replica_id [topics]\n *   replica_id => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition timestamp max_num_offsets\n *       partition => INT32\n *       timestamp => INT64\n *       max_num_offsets => INT32\n */\n\n/**\n * @param {number} replicaId\n * @param {object} topics use timestamp=-1 for latest offsets and timestamp=-2 for earliest.\n *                        Default timestamp=-1. Example:\n *                          {\n *                            topics: [\n *                              {\n *                                topic: 'topic-name',\n *                                partitions: [{ partition: 0, timestamp: -1 }]\n *                              }\n *                            ]\n *                          }\n */\nmodule.exports = ({ replicaId, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'ListOffsets',\n  encode: async () => {\n    return new Encoder().writeInt32(replicaId).writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, timestamp = -1, maxNumOffsets = 1 }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(timestamp)\n    .writeInt32(maxNumOffsets)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * Offsets Response (Version: 0) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code [offsets]\n *       partition => INT32\n *       error_code => INT16\n *       offsets => INT64\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  offsets: decoder.readArray(decodeOffsets),\n})\n\nconst decodeOffsets = decoder => decoder.readInt64().toString()\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { ListOffsets: apiKey } = require('../../apiKeys')\n\n/**\n * ListOffsets Request (Version: 1) => replica_id [topics]\n *   replica_id => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition timestamp\n *       partition => INT32\n *       timestamp => INT64\n */\nmodule.exports = ({ replicaId, topics }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'ListOffsets',\n  encode: async () => {\n    return new Encoder().writeInt32(replicaId).writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, timestamp = -1 }) => {\n  return new Encoder().writeInt32(partition).writeInt64(timestamp)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * ListOffsets Response (Version: 1) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code timestamp offset\n *       partition => INT32\n *       error_code => INT16\n *       timestamp => INT64\n *       offset => INT64\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n\n  return {\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  timestamp: decoder.readInt64().toString(),\n  offset: decoder.readInt64().toString(),\n})\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { ListOffsets: apiKey } = require('../../apiKeys')\n\n/**\n * ListOffsets Request (Version: 2) => replica_id isolation_level [topics]\n *   replica_id => INT32\n *   isolation_level => INT8\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition timestamp\n *       partition => INT32\n *       timestamp => INT64\n */\nmodule.exports = ({ replicaId, isolationLevel, topics }) => ({\n  apiKey,\n  apiVersion: 2,\n  apiName: 'ListOffsets',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt8(isolationLevel)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, timestamp = -1 }) => {\n  return new Encoder().writeInt32(partition).writeInt64(timestamp)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * ListOffsets Response (Version: 2) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code timestamp offset\n *       partition => INT32\n *       error_code => INT16\n *       timestamp => INT64\n *       offset => INT64\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n\n  return {\n    throttleTime: decoder.readInt32(),\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  timestamp: decoder.readInt64().toString(),\n  offset: decoder.readInt64().toString(),\n})\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV2 = require('../v2/request')\n\n/**\n * ListOffsets Request (Version: 3) => replica_id isolation_level [topics]\n *   replica_id => INT32\n *   isolation_level => INT8\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition timestamp\n *       partition => INT32\n *       timestamp => INT64\n */\nmodule.exports = ({ replicaId, isolationLevel, topics }) =>\n  Object.assign(requestV2({ replicaId, isolationLevel, topics }), { apiVersion: 3 })\n","const { parse, decode: decodeV2 } = require('../v2/response')\n\n/**\n * In version 3 on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * ListOffsets Response (Version: 3) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code timestamp offset\n *       partition => INT32\n *       error_code => INT16\n *       timestamp => INT64\n *       offset => INT64\n */\nconst decode = async rawData => {\n  const decoded = await decodeV2(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ topics }), response }\n  },\n  1: ({ topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ topics }), response }\n  },\n  2: ({ topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ topics }), response }\n  },\n  3: ({ topics }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return { request: request({ topics }), response }\n  },\n  4: ({ topics, allowAutoTopicCreation }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n    return { request: request({ topics, allowAutoTopicCreation }), response }\n  },\n  5: ({ topics, allowAutoTopicCreation }) => {\n    const request = require('./v5/request')\n    const response = require('./v5/response')\n    return { request: request({ topics, allowAutoTopicCreation }), response }\n  },\n  6: ({ topics, allowAutoTopicCreation }) => {\n    const request = require('./v6/request')\n    const response = require('./v6/response')\n    return { request: request({ topics, allowAutoTopicCreation }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { Metadata: apiKey } = require('../../apiKeys')\n\n/**\n * Metadata Request (Version: 0) => [topics]\n *   topics => STRING\n */\n\nmodule.exports = ({ topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'Metadata',\n  encode: async () => {\n    return new Encoder().writeArray(topics)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * Metadata Response (Version: 0) => [brokers] [topic_metadata]\n *   brokers => node_id host port\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *   topic_metadata => topic_error_code topic [partition_metadata]\n *     topic_error_code => INT16\n *     topic => STRING\n *     partition_metadata => partition_error_code partition_id leader [replicas] [isr]\n *       partition_error_code => INT16\n *       partition_id => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  // leader: The node id for the kafka broker currently acting as leader\n  // for this partition\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    brokers: decoder.readArray(broker),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nconst parse = async data => {\n  const topicsWithErrors = data.topicMetadata.filter(topic => failure(topic.topicErrorCode))\n  if (topicsWithErrors.length > 0) {\n    const { topicErrorCode } = topicsWithErrors[0]\n    throw createErrorFromCode(topicErrorCode)\n  }\n\n  const partitionsWithErrors = data.topicMetadata.map(topic => {\n    return topic.partitionMetadata.filter(partition => failure(partition.partitionErrorCode))\n  })\n\n  const errors = flatten(partitionsWithErrors)\n  if (errors.length > 0) {\n    const { partitionErrorCode } = errors[0]\n    throw createErrorFromCode(partitionErrorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { Metadata: apiKey } = require('../../apiKeys')\n\n/**\n * Metadata Request (Version: 1) => [topics]\n *   topics => STRING\n */\n\nmodule.exports = ({ topics }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'Metadata',\n  encode: async () => {\n    return new Encoder().writeNullableArray(topics)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Metadata Response (Version: 1) => [brokers] controller_id [topic_metadata]\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => topic_error_code topic is_internal [partition_metadata]\n *     topic_error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => partition_error_code partition_id leader [replicas] [isr]\n *       partition_error_code => INT16\n *       partition_id => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n  rack: decoder.readString(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  isInternal: decoder.readBoolean(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    brokers: decoder.readArray(broker),\n    controllerId: decoder.readInt32(),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * Metadata Request (Version: 2) => [topics]\n *   topics => STRING\n */\n\nmodule.exports = ({ topics }) => Object.assign(requestV1({ topics }), { apiVersion: 2 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Metadata Response (Version: 2) => [brokers] cluster_id controller_id [topic_metadata]\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   cluster_id => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => topic_error_code topic is_internal [partition_metadata]\n *     topic_error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => partition_error_code partition_id leader [replicas] [isr]\n *       partition_error_code => INT16\n *       partition_id => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n  rack: decoder.readString(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  isInternal: decoder.readBoolean(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    brokers: decoder.readArray(broker),\n    clusterId: decoder.readString(),\n    controllerId: decoder.readInt32(),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * Metadata Request (Version: 3) => [topics]\n *   topics => STRING\n */\n\nmodule.exports = ({ topics }) => Object.assign(requestV1({ topics }), { apiVersion: 3 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Metadata Response (Version: 3) => throttle_time_ms [brokers] cluster_id controller_id [topic_metadata]\n *   throttle_time_ms => INT32\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   cluster_id => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => error_code topic is_internal [partition_metadata]\n *     error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => error_code partition leader [replicas] [isr]\n *       error_code => INT16\n *       partition => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n  rack: decoder.readString(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  isInternal: decoder.readBoolean(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    brokers: decoder.readArray(broker),\n    clusterId: decoder.readString(),\n    controllerId: decoder.readInt32(),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const Encoder = require('../../../encoder')\nconst { Metadata: apiKey } = require('../../apiKeys')\n\n/**\n * Metadata Request (Version: 4) => [topics] allow_auto_topic_creation\n *   topics => STRING\n *   allow_auto_topic_creation => BOOLEAN\n */\n\nmodule.exports = ({ topics, allowAutoTopicCreation = true }) => ({\n  apiKey,\n  apiVersion: 4,\n  apiName: 'Metadata',\n  encode: async () => {\n    return new Encoder().writeNullableArray(topics).writeBoolean(allowAutoTopicCreation)\n  },\n})\n","const { parse: parseV3, decode: decodeV3 } = require('../v3/response')\n\n/**\n * Metadata Response (Version: 4) => throttle_time_ms [brokers] cluster_id controller_id [topic_metadata]\n *   throttle_time_ms => INT32\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   cluster_id => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => error_code topic is_internal [partition_metadata]\n *     error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => error_code partition leader [replicas] [isr]\n *       error_code => INT16\n *       partition => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nmodule.exports = {\n  parse: parseV3,\n  decode: decodeV3,\n}\n","const requestV4 = require('../v4/request')\n\n/**\n * Metadata Request (Version: 5) => [topics] allow_auto_topic_creation\n *   topics => STRING\n *   allow_auto_topic_creation => BOOLEAN\n */\n\nmodule.exports = ({ topics, allowAutoTopicCreation = true }) =>\n  Object.assign(requestV4({ topics, allowAutoTopicCreation }), { apiVersion: 5 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Metadata Response (Version: 5) => throttle_time_ms [brokers] cluster_id controller_id [topic_metadata]\n *   throttle_time_ms => INT32\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   cluster_id => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => error_code topic is_internal [partition_metadata]\n *     error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => error_code partition leader [replicas] [isr] [offline_replicas]\n *       error_code => INT16\n *       partition => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n *       offline_replicas => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n  rack: decoder.readString(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  isInternal: decoder.readBoolean(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n  offlineReplicas: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    brokers: decoder.readArray(broker),\n    clusterId: decoder.readString(),\n    controllerId: decoder.readInt32(),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV5 = require('../v5/request')\n\n/**\n * Metadata Request (Version: 6) => [topics] allow_auto_topic_creation\n *   topics => STRING\n *   allow_auto_topic_creation => BOOLEAN\n */\n\nmodule.exports = ({ topics, allowAutoTopicCreation = true }) =>\n  Object.assign(requestV5({ topics, allowAutoTopicCreation }), { apiVersion: 6 })\n","const { parse, decode: decodeV1 } = require('../v5/response')\n\n/**\n * In version 6 on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * Metadata Response (Version: 6) => throttle_time_ms [brokers] cluster_id controller_id [topic_metadata]\n *   throttle_time_ms => INT32\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   cluster_id => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => error_code topic is_internal [partition_metadata]\n *     error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => error_code partition leader [replicas] [isr] [offline_replicas]\n *       error_code => INT16\n *       partition => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n *       offline_replicas => INT32\n */\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","// This value signals to the broker that its default configuration should be used.\nconst RETENTION_TIME = -1\n\nconst versions = {\n  0: ({ groupId, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ groupId, topics }), response }\n  },\n  1: ({ groupId, groupGenerationId, memberId, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ groupId, groupGenerationId, memberId, topics }), response }\n  },\n  2: ({ groupId, groupGenerationId, memberId, retentionTime = RETENTION_TIME, topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return {\n      request: request({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      }),\n      response,\n    }\n  },\n  3: ({ groupId, groupGenerationId, memberId, retentionTime = RETENTION_TIME, topics }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      }),\n      response,\n    }\n  },\n  4: ({ groupId, groupGenerationId, memberId, retentionTime = RETENTION_TIME, topics }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n    return {\n      request: request({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      }),\n      response,\n    }\n  },\n  5: ({ groupId, groupGenerationId, memberId, topics }) => {\n    const request = require('./v5/request')\n    const response = require('./v5/response')\n    return {\n      request: request({\n        groupId,\n        groupGenerationId,\n        memberId,\n        topics,\n      }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetCommit: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetCommit Request (Version: 0) => group_id [topics]\n *   group_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'OffsetCommit',\n  encode: async () => {\n    return new Encoder().writeString(groupId).writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, offset, metadata = null }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(offset)\n    .writeString(metadata)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * OffsetCommit Response (Version: 0) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetCommit: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetCommit Request (Version: 1) => group_id group_generation_id member_id [topics]\n *   group_id => STRING\n *   group_generation_id => INT32\n *   member_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset timestamp metadata\n *       partition => INT32\n *       offset => INT64\n *       timestamp => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, topics }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'OffsetCommit',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(groupGenerationId)\n      .writeString(memberId)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, offset, timestamp = Date.now(), metadata = null }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(offset)\n    .writeInt64(timestamp)\n    .writeString(metadata)\n}\n","const { parse, decode } = require('../v0/response')\n\n/**\n * OffsetCommit Response (Version: 1) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetCommit: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetCommit Request (Version: 2) => group_id group_generation_id member_id retention_time [topics]\n *   group_id => STRING\n *   group_generation_id => INT32\n *   member_id => STRING\n *   retention_time => INT64\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, retentionTime, topics }) => ({\n  apiKey,\n  apiVersion: 2,\n  apiName: 'OffsetCommit',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(groupGenerationId)\n      .writeString(memberId)\n      .writeInt64(retentionTime)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, offset, metadata = null }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(offset)\n    .writeString(metadata)\n}\n","const { parse, decode } = require('../v0/response')\n\n/**\n * OffsetCommit Response (Version: 1) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV2 = require('../v2/request')\n\n/**\n * OffsetCommit Request (Version: 3) => group_id generation_id member_id retention_time [topics]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   retention_time => INT64\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, retentionTime, topics }) =>\n  Object.assign(requestV2({ groupId, groupGenerationId, memberId, retentionTime, topics }), {\n    apiVersion: 3,\n  })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * OffsetCommit Response (Version: 3) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n})\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV3 = require('../v3/request')\n\n/**\n * OffsetCommit Request (Version: 4) => group_id generation_id member_id retention_time [topics]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   retention_time => INT64\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, retentionTime, topics }) =>\n  Object.assign(requestV3({ groupId, groupGenerationId, memberId, retentionTime, topics }), {\n    apiVersion: 4,\n  })\n","const { parse, decode: decodeV3 } = require('../v3/response')\n\n/**\n * Starting in version 4, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * OffsetCommit Response (Version: 4) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV3(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetCommit: apiKey } = require('../../apiKeys')\n\n/**\n * Version 5 removes retention_time, as this is controlled by a broker setting\n *\n * OffsetCommit Request (Version: 4) => group_id generation_id member_id [topics]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, topics }) => ({\n  apiKey,\n  apiVersion: 5,\n  apiName: 'OffsetCommit',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(groupGenerationId)\n      .writeString(memberId)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, offset, metadata = null }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(offset)\n    .writeString(metadata)\n}\n","const { parse, decode } = require('../v4/response')\n\n/**\n * OffsetCommit Response (Version: 5) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  1: ({ groupId, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ groupId, topics }), response }\n  },\n  2: ({ groupId, topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ groupId, topics }), response }\n  },\n  3: ({ groupId, topics }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return { request: request({ groupId, topics }), response }\n  },\n  4: ({ groupId, topics }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n    return { request: request({ groupId, topics }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetFetch: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetFetch Request (Version: 1) => group_id [topics]\n *   group_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition\n *       partition => INT32\n */\n\nmodule.exports = ({ groupId, topics }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'OffsetFetch',\n  encode: async () => {\n    return new Encoder().writeString(groupId).writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition }) => {\n  return new Encoder().writeInt32(partition)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * OffsetFetch Response (Version: 1) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition offset metadata error_code\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n *       error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  offset: decoder.readInt64().toString(),\n  metadata: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * OffsetFetch Request (Version: 2) => group_id [topics]\n *   group_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition\n *       partition => INT32\n */\n\nmodule.exports = ({ groupId, topics }) =>\n  Object.assign(requestV1({ groupId, topics }), { apiVersion: 2 })\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * OffsetFetch Response (Version: 2) => [responses] error_code\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition offset metadata error_code\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n *       error_code => INT16\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    responses: decoder.readArray(decodeResponses),\n    errorCode: decoder.readInt16(),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  offset: decoder.readInt64().toString(),\n  metadata: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetFetch: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetFetch Request (Version: 3) => group_id [topics]\n *   group_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition\n *       partition => INT32\n */\n\nmodule.exports = ({ groupId, topics }) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'OffsetFetch',\n  encode: async () => {\n    return new Encoder().writeString(groupId).writeNullableArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition }) => {\n  return new Encoder().writeInt32(partition)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV2 } = require('../v2/response')\n\n/**\n * OffsetFetch Response (Version: 3) => throttle_time_ms [responses] error_code\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition offset metadata error_code\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n *       error_code => INT16\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    responses: decoder.readArray(decodeResponses),\n    errorCode: decoder.readInt16(),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  offset: decoder.readInt64().toString(),\n  metadata: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nmodule.exports = {\n  decode,\n  parse: parseV2,\n}\n","const requestV3 = require('../v3/request')\n\n/**\n * OffsetFetch Request (Version: 4) => group_id [topics]\n *   group_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition\n *       partition => INT32\n */\n\nmodule.exports = ({ groupId, topics }) =>\n  Object.assign(requestV3({ groupId, topics }), { apiVersion: 4 })\n","const { parse, decode: decodeV3 } = require('../v3/response')\n\n/**\n * Starting in version 4, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * OffsetFetch Response (Version: 4) => throttle_time_ms [responses] error_code\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition offset metadata error_code\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n *       error_code => INT16\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV3(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ acks, timeout, topicData }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ acks, timeout, topicData }), response }\n  },\n  1: ({ acks, timeout, topicData }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ acks, timeout, topicData }), response }\n  },\n  2: ({ acks, timeout, topicData, compression }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ acks, timeout, compression, topicData }), response }\n  },\n  3: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n  4: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n  5: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v5/request')\n    const response = require('./v5/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n  6: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v6/request')\n    const response = require('./v6/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n  7: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v7/request')\n    const response = require('./v7/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { Produce: apiKey } = require('../../apiKeys')\nconst MessageSet = require('../../../messageSet')\n\n/**\n * Produce Request (Version: 0) => acks timeout [topic_data]\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set record_set_size\n *       partition => INT32\n *       record_set_size => INT32\n *       record_set => RECORDS\n */\n\n/**\n * MessageV0:\n * {\n *   key: bytes,\n *   value: bytes\n * }\n *\n * MessageSet:\n * [\n *   { key: \"<value>\", value: \"<value>\" },\n *   { key: \"<value>\", value: \"<value>\" },\n * ]\n *\n * TopicData:\n * [\n *   {\n *     topic: 'name1',\n *     partitions: [\n *       {\n *         partition: 0,\n *         messages: [<MessageSet>]\n *       }\n *     ]\n *   }\n * ]\n */\n\n/**\n * @param acks {Integer} This field indicates how many acknowledgements the servers should receive before\n *                       responding to the request. If it is 0 the server will not send any response\n *                       (this is the only case where the server will not reply to a request). If it is 1,\n *                       the server will wait the data is written to the local log before sending a response.\n *                       If it is -1 the server will block until the message is committed by all in sync replicas\n *                       before sending a response.\n *\n * @param timeout {Integer} This provides a maximum time in milliseconds the server can await the receipt of the number\n *                          of acknowledgements in RequiredAcks. The timeout is not an exact limit on the request time\n *                          for a few reasons:\n *                          (1) it does not include network latency,\n *                          (2) the timer begins at the beginning of the processing of this request so if many requests are\n *                              queued due to server overload that wait time will not be included,\n *                          (3) we will not terminate a local write so if the local write time exceeds this timeout it will not\n *                              be respected. To get a hard timeout of this type the client should use the socket timeout.\n *\n * @param topicData {Array}\n */\nmodule.exports = ({ acks, timeout, topicData }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'Produce',\n  expectResponse: () => acks !== 0,\n  encode: async () => {\n    return new Encoder()\n      .writeInt16(acks)\n      .writeInt32(timeout)\n      .writeArray(topicData.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartitions))\n}\n\nconst encodePartitions = ({ partition, messages }) => {\n  const messageSet = MessageSet({ messageVersion: 0, entries: messages })\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(messageSet.size())\n    .writeEncoder(messageSet)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * v0\n * ProduceResponse => [TopicName [Partition ErrorCode Offset]]\n *   TopicName => string\n *   Partition => int32\n *   ErrorCode => int16\n *   Offset => int64\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  offset: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  return {\n    topics,\n  }\n}\n\nconst parse = async data => {\n  const partitionsWithError = data.topics.map(topic => {\n    return topic.partitions.filter(partition => failure(partition.errorCode))\n  })\n\n  const errors = flatten(partitionsWithError)\n  if (errors.length > 0) {\n    const { errorCode } = errors[0]\n    throw createErrorFromCode(errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n// Produce Request on or after v1 indicates the client can parse the quota throttle time\n// in the Produce Response.\n\nmodule.exports = ({ acks, timeout, topicData }) => {\n  return Object.assign(requestV0({ acks, timeout, topicData }), { apiVersion: 1 })\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * v1 (supported in 0.9.0 or later)\n * ProduceResponse => [TopicName [Partition ErrorCode Offset]] ThrottleTime\n *   TopicName => string\n *   Partition => int32\n *   ErrorCode => int16\n *   Offset => int64\n *   ThrottleTime => int32\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  offset: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  const throttleTime = decoder.readInt32()\n\n  return {\n    topics,\n    throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const Encoder = require('../../../encoder')\nconst { Produce: apiKey } = require('../../apiKeys')\nconst MessageSet = require('../../../messageSet')\nconst { Types, lookupCodec } = require('../../../message/compression')\n\n// Produce Request on or after v2 indicates the client can parse the timestamp field\n// in the produce Response.\n\nmodule.exports = ({ acks, timeout, compression = Types.None, topicData }) => ({\n  apiKey,\n  apiVersion: 2,\n  apiName: 'Produce',\n  expectResponse: () => acks !== 0,\n  encode: async () => {\n    const encodeTopic = topicEncoder(compression)\n    const encodedTopicData = []\n\n    for (const data of topicData) {\n      encodedTopicData.push(await encodeTopic(data))\n    }\n\n    return new Encoder()\n      .writeInt16(acks)\n      .writeInt32(timeout)\n      .writeArray(encodedTopicData)\n  },\n})\n\nconst topicEncoder = compression => {\n  const encodePartitions = partitionsEncoder(compression)\n\n  return async ({ topic, partitions }) => {\n    const encodedPartitions = []\n\n    for (const data of partitions) {\n      encodedPartitions.push(await encodePartitions(data))\n    }\n\n    return new Encoder().writeString(topic).writeArray(encodedPartitions)\n  }\n}\n\nconst partitionsEncoder = compression => async ({ partition, messages }) => {\n  const messageSet = MessageSet({ messageVersion: 1, compression, entries: messages })\n\n  if (compression === Types.None) {\n    return new Encoder()\n      .writeInt32(partition)\n      .writeInt32(messageSet.size())\n      .writeEncoder(messageSet)\n  }\n\n  const timestamp = messages[0].timestamp || Date.now()\n\n  const codec = lookupCodec(compression)\n  const compressedValue = await codec.compress(messageSet)\n  const compressedMessageSet = MessageSet({\n    messageVersion: 1,\n    entries: [{ compression, timestamp, value: compressedValue }],\n  })\n\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(compressedMessageSet.size())\n    .writeEncoder(compressedMessageSet)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * v2 (supported in 0.10.0 or later)\n * ProduceResponse => [TopicName [Partition ErrorCode Offset Timestamp]] ThrottleTime\n *   TopicName => string\n *   Partition => int32\n *   ErrorCode => int16\n *   Offset => int64\n *   Timestamp => int64\n *   ThrottleTime => int32\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  offset: decoder.readInt64().toString(),\n  timestamp: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  const throttleTime = decoder.readInt32()\n\n  return {\n    topics,\n    throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const Long = require('../../../../utils/long')\nconst Encoder = require('../../../encoder')\nconst { Produce: apiKey } = require('../../apiKeys')\nconst { Types } = require('../../../message/compression')\nconst Record = require('../../../recordBatch/record/v0')\nconst { RecordBatch } = require('../../../recordBatch/v0')\n\n/**\n * Produce Request (Version: 3) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\n/**\n * @param [transactionalId=null] {String} The transactional id or null if the producer is not transactional\n * @param acks {Integer} See producer request v0\n * @param timeout {Integer} See producer request v0\n * @param [compression=CompressionTypes.None] {CompressionTypes}\n * @param topicData {Array}\n */\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId = null,\n  producerId = Long.fromInt(-1),\n  producerEpoch = 0,\n  compression = Types.None,\n  topicData,\n}) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'Produce',\n  expectResponse: () => acks !== 0,\n  encode: async () => {\n    const encodeTopic = topicEncoder(compression)\n    const encodedTopicData = []\n\n    for (const data of topicData) {\n      encodedTopicData.push(\n        await encodeTopic({ ...data, transactionalId, producerId, producerEpoch })\n      )\n    }\n\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeInt16(acks)\n      .writeInt32(timeout)\n      .writeArray(encodedTopicData)\n  },\n})\n\nconst topicEncoder = compression => async ({\n  topic,\n  partitions,\n  transactionalId,\n  producerId,\n  producerEpoch,\n}) => {\n  const encodePartitions = partitionsEncoder(compression)\n  const encodedPartitions = []\n\n  for (const data of partitions) {\n    encodedPartitions.push(\n      await encodePartitions({ ...data, transactionalId, producerId, producerEpoch })\n    )\n  }\n\n  return new Encoder().writeString(topic).writeArray(encodedPartitions)\n}\n\nconst partitionsEncoder = compression => async ({\n  partition,\n  messages,\n  transactionalId,\n  firstSequence,\n  producerId,\n  producerEpoch,\n}) => {\n  const dateNow = Date.now()\n  const messageTimestamps = messages\n    .map(m => m.timestamp)\n    .filter(timestamp => timestamp != null)\n    .sort()\n\n  const timestamps = messageTimestamps.length === 0 ? [dateNow] : messageTimestamps\n  const firstTimestamp = timestamps[0]\n  const maxTimestamp = timestamps[timestamps.length - 1]\n\n  const records = messages.map((message, i) =>\n    Record({\n      ...message,\n      offsetDelta: i,\n      timestampDelta: (message.timestamp || dateNow) - firstTimestamp,\n    })\n  )\n\n  const recordBatch = await RecordBatch({\n    compression,\n    records,\n    firstTimestamp,\n    maxTimestamp,\n    producerId,\n    producerEpoch,\n    firstSequence,\n    transactional: !!transactionalId,\n    lastOffsetDelta: records.length - 1,\n  })\n\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(recordBatch.size())\n    .writeEncoder(recordBatch)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * Produce Response (Version: 3) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *   throttle_time_ms => INT32\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  baseOffset: decoder.readInt64().toString(),\n  logAppendTime: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  const throttleTime = decoder.readInt32()\n\n  return {\n    topics,\n    throttleTime,\n  }\n}\n\nconst parse = async data => {\n  const partitionsWithError = data.topics.map(response => {\n    return response.partitions.filter(partition => failure(partition.errorCode))\n  })\n\n  const errors = flatten(partitionsWithError)\n  if (errors.length > 0) {\n    const { errorCode } = errors[0]\n    throw createErrorFromCode(errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV3 = require('../v3/request')\n\n/**\n * Produce Request (Version: 4) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId,\n  producerId,\n  producerEpoch,\n  compression,\n  topicData,\n}) =>\n  Object.assign(\n    requestV3({\n      acks,\n      timeout,\n      transactionalId,\n      producerId,\n      producerEpoch,\n      compression,\n      topicData,\n    }),\n    { apiVersion: 4 }\n  )\n","const { decode, parse } = require('../v3/response')\n\n/**\n * Produce Response (Version: 4) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *   throttle_time_ms => INT32\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV3 = require('../v3/request')\n\n/**\n * Produce Request (Version: 5) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId,\n  producerId,\n  producerEpoch,\n  compression,\n  topicData,\n}) =>\n  Object.assign(\n    requestV3({\n      acks,\n      timeout,\n      transactionalId,\n      producerId,\n      producerEpoch,\n      compression,\n      topicData,\n    }),\n    { apiVersion: 5 }\n  )\n","const Decoder = require('../../../decoder')\nconst { parse: parseV3 } = require('../v3/response')\n\n/**\n * Produce Response (Version: 5) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time log_start_offset\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *       log_start_offset => INT64\n *   throttle_time_ms => INT32\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  baseOffset: decoder.readInt64().toString(),\n  logAppendTime: decoder.readInt64().toString(),\n  logStartOffset: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  const throttleTime = decoder.readInt32()\n\n  return {\n    topics,\n    throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV3,\n}\n","const requestV5 = require('../v5/request')\n\n/**\n * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n * @see https://github.com/apache/kafka/blob/9c8f75c4b624084c954b4da69f092211a9ac4689/clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java#L113-L117\n *\n * Produce Request (Version: 6) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId,\n  producerId,\n  producerEpoch,\n  compression,\n  topicData,\n}) =>\n  Object.assign(\n    requestV5({\n      acks,\n      timeout,\n      transactionalId,\n      producerId,\n      producerEpoch,\n      compression,\n      topicData,\n    }),\n    { apiVersion: 6 }\n  )\n","const { parse, decode: decodeV5 } = require('../v5/response')\n\n/**\n * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n * @see https://github.com/apache/kafka/blob/9c8f75c4b624084c954b4da69f092211a9ac4689/clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java#L152-L156\n *\n * Produce Response (Version: 6) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time log_start_offset\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *       log_start_offset => INT64\n *   throttle_time_ms => INT32\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV5(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV6 = require('../v6/request')\n\n/**\n * V7 indicates ZStandard capability (see KIP-110)\n * @see https://github.com/apache/kafka/blob/9c8f75c4b624084c954b4da69f092211a9ac4689/clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java#L118-L121\n *\n * Produce Request (Version: 7) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId,\n  producerId,\n  producerEpoch,\n  compression,\n  topicData,\n}) =>\n  Object.assign(\n    requestV6({\n      acks,\n      timeout,\n      transactionalId,\n      producerId,\n      producerEpoch,\n      compression,\n      topicData,\n    }),\n    { apiVersion: 7 }\n  )\n","const { decode, parse } = require('../v6/response')\n\n/**\n * Produce Response (Version: 7) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time log_start_offset\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *       log_start_offset => INT64\n *   throttle_time_ms => INT32\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ authBytes }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ authBytes }), response }\n  },\n  1: ({ authBytes }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ authBytes }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { SaslAuthenticate: apiKey } = require('../../apiKeys')\n\n/**\n * SaslAuthenticate Request (Version: 0) => sasl_auth_bytes\n *   sasl_auth_bytes => BYTES\n */\n\n/**\n * @param {Buffer} authBytes - SASL authentication bytes from client as defined by the SASL mechanism\n */\nmodule.exports = ({ authBytes }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'SaslAuthenticate',\n  encode: async () => {\n    return new Encoder().writeBuffer(authBytes)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst Encoder = require('../../../encoder')\nconst {\n  failure,\n  createErrorFromCode,\n  failIfVersionNotSupported,\n  errorCodes,\n} = require('../../../error')\n\nconst { KafkaJSProtocolError } = require('../../../../errors')\nconst SASL_AUTHENTICATION_FAILED = 58\nconst protocolAuthError = errorCodes.find(e => e.code === SASL_AUTHENTICATION_FAILED)\n\n/**\n * SaslAuthenticate Response (Version: 0) => error_code error_message sasl_auth_bytes\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   sasl_auth_bytes => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n  const errorMessage = decoder.readString()\n\n  // This is necessary to make the response compatible with the original\n  // mechanism protocols. They expect a byte response, which starts with\n  // the size\n  const authBytesEncoder = new Encoder().writeBytes(decoder.readBytes())\n  const authBytes = authBytesEncoder.buffer\n\n  return {\n    errorCode,\n    errorMessage,\n    authBytes,\n  }\n}\n\nconst parse = async data => {\n  if (data.errorCode === SASL_AUTHENTICATION_FAILED && data.errorMessage) {\n    throw new KafkaJSProtocolError({\n      ...protocolAuthError,\n      message: data.errorMessage,\n    })\n  }\n\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * SaslAuthenticate Request (Version: 1) => sasl_auth_bytes\n *   sasl_auth_bytes => BYTES\n */\n\n/**\n * @param {Buffer} authBytes - SASL authentication bytes from client as defined by the SASL mechanism\n */\nmodule.exports = ({ authBytes }) => Object.assign(requestV0({ authBytes }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst Encoder = require('../../../encoder')\nconst { parse: parseV0 } = require('../v0/response')\nconst { failIfVersionNotSupported } = require('../../../error')\n\n/**\n * SaslAuthenticate Response (Version: 1) => error_code error_message sasl_auth_bytes\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   sasl_auth_bytes => BYTES\n *   session_lifetime_ms => INT64\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n  const errorMessage = decoder.readString()\n\n  // This is necessary to make the response compatible with the original\n  // mechanism protocols. They expect a byte response, which starts with\n  // the size\n  const authBytesEncoder = new Encoder().writeBytes(decoder.readBytes())\n  const authBytes = authBytesEncoder.buffer\n  const sessionLifetimeMs = decoder.readInt64().toString()\n\n  return {\n    errorCode,\n    errorMessage,\n    authBytes,\n    sessionLifetimeMs,\n  }\n}\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const versions = {\n  0: ({ mechanism }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ mechanism }), response }\n  },\n  1: ({ mechanism }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ mechanism }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { SaslHandshake: apiKey } = require('../../apiKeys')\n\n/**\n * SaslHandshake Request (Version: 0) => mechanism\n *    mechanism => STRING\n */\n\n/**\n * @param {string} mechanism - SASL Mechanism chosen by the client\n */\nmodule.exports = ({ mechanism }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'SaslHandshake',\n  encode: async () => new Encoder().writeString(mechanism),\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * SaslHandshake Response (Version: 0) => error_code [enabled_mechanisms]\n *    error_code => INT16\n *    enabled_mechanisms => STRING\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    errorCode,\n    enabledMechanisms: decoder.readArray(decoder => decoder.readString()),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\nmodule.exports = ({ mechanism }) => ({ ...requestV0({ mechanism }), apiVersion: 1 })\n","const { decode: decodeV0, parse: parseV0 } = require('../v0/response')\n\nmodule.exports = {\n  decode: decodeV0,\n  parse: parseV0,\n}\n","const versions = {\n  0: ({ groupId, generationId, memberId, groupAssignment }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ groupId, generationId, memberId, groupAssignment }),\n      response,\n    }\n  },\n  1: ({ groupId, generationId, memberId, groupAssignment }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ groupId, generationId, memberId, groupAssignment }),\n      response,\n    }\n  },\n  2: ({ groupId, generationId, memberId, groupAssignment }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return {\n      request: request({ groupId, generationId, memberId, groupAssignment }),\n      response,\n    }\n  },\n  3: ({ groupId, generationId, memberId, groupInstanceId, groupAssignment }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({ groupId, generationId, memberId, groupInstanceId, groupAssignment }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { SyncGroup: apiKey } = require('../../apiKeys')\n\n/**\n * SyncGroup Request (Version: 0) => group_id generation_id member_id [group_assignment]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   group_assignment => member_id member_assignment\n *     member_id => STRING\n *     member_assignment => BYTES\n */\n\nmodule.exports = ({ groupId, generationId, memberId, groupAssignment }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'SyncGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(generationId)\n      .writeString(memberId)\n      .writeArray(groupAssignment.map(encodeGroupAssignment))\n  },\n})\n\nconst encodeGroupAssignment = ({ memberId, memberAssignment }) => {\n  return new Encoder().writeString(memberId).writeBytes(memberAssignment)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * SyncGroup Response (Version: 0) => error_code member_assignment\n *   error_code => INT16\n *   member_assignment => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    errorCode,\n    memberAssignment: decoder.readBytes(),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * SyncGroup Request (Version: 1) => group_id generation_id member_id [group_assignment]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   group_assignment => member_id member_assignment\n *     member_id => STRING\n *     member_assignment => BYTES\n */\n\nmodule.exports = ({ groupId, generationId, memberId, groupAssignment }) =>\n  Object.assign(requestV0({ groupId, generationId, memberId, groupAssignment }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * SyncGroup Response (Version: 1) => throttle_time_ms error_code member_assignment\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   member_assignment => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n    memberAssignment: decoder.readBytes(),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * SyncGroup Request (Version: 2) => group_id generation_id member_id [group_assignment]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   group_assignment => member_id member_assignment\n *     member_id => STRING\n *     member_assignment => BYTES\n */\n\nmodule.exports = ({ groupId, generationId, memberId, groupAssignment }) =>\n  Object.assign(requestV1({ groupId, generationId, memberId, groupAssignment }), { apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * In version 2, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * SyncGroup Response (Version: 2) => throttle_time_ms error_code member_assignment\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   member_assignment => BYTES\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { SyncGroup: apiKey } = require('../../apiKeys')\n\n/**\n * Version 3 adds group_instance_id to indicate member identity across restarts.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances\n *\n * SyncGroup Request (Version: 3) => group_id generation_id member_id group_instance_id [group_assignment]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   group_instance_id => NULLABLE_STRING\n *   group_assignment => member_id member_assignment\n *     member_id => STRING\n *     member_assignment => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  generationId,\n  memberId,\n  groupInstanceId = null,\n  groupAssignment,\n}) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'SyncGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(generationId)\n      .writeString(memberId)\n      .writeString(groupInstanceId)\n      .writeArray(groupAssignment.map(encodeGroupAssignment))\n  },\n})\n\nconst encodeGroupAssignment = ({ memberId, memberAssignment }) => {\n  return new Encoder().writeString(memberId).writeBytes(memberAssignment)\n}\n","const { decode, parse } = require('../v2/response')\n\n/**\n * SyncGroup Response (Version: 2) => throttle_time_ms error_code member_assignment\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   member_assignment => BYTES\n */\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ transactionalId, groupId, producerId, producerEpoch, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ transactionalId, groupId, producerId, producerEpoch, topics }),\n      response,\n    }\n  },\n  1: ({ transactionalId, groupId, producerId, producerEpoch, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ transactionalId, groupId, producerId, producerEpoch, topics }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { TxnOffsetCommit: apiKey } = require('../../apiKeys')\n\n/**\n * TxnOffsetCommit Request (Version: 0) => transactional_id group_id producer_id producer_epoch [topics]\n *   transactional_id => STRING\n *   group_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ transactionalId, groupId, producerId, producerEpoch, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'TxnOffsetCommit',\n  encode: async () => {\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeString(groupId)\n      .writeInt64(producerId)\n      .writeInt16(producerEpoch)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, offset, metadata }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(offset)\n    .writeString(metadata)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * TxnOffsetCommit Response (Version: 0) => throttle_time_ms [topics]\n *   throttle_time_ms => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const topics = await decoder.readArrayAsync(decodeTopic)\n\n  return {\n    throttleTime,\n    topics,\n  }\n}\n\nconst decodeTopic = async decoder => ({\n  topic: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decodePartition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  const topicsWithErrors = data.topics\n    .map(({ partitions }) => ({\n      partitionsWithErrors: partitions.filter(({ errorCode }) => failure(errorCode)),\n    }))\n    .filter(({ partitionsWithErrors }) => partitionsWithErrors.length)\n\n  if (topicsWithErrors.length > 0) {\n    throw createErrorFromCode(topicsWithErrors[0].partitionsWithErrors[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * TxnOffsetCommit Request (Version: 1) => transactional_id group_id producer_id producer_epoch [topics]\n *   transactional_id => STRING\n *   group_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ transactionalId, groupId, producerId, producerEpoch, topics }) =>\n  Object.assign(requestV0({ transactionalId, groupId, producerId, producerEpoch, topics }), {\n    apiVersion: 1,\n  })\n","const { parse, decode: decodeV1 } = require('../v0/response')\n\n/**\n * In version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * TxnOffsetCommit Response (Version: 1) => throttle_time_ms [topics]\n *   throttle_time_ms => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/resource/PatternType.java#L32\n\n/**\n * @typedef {number} ACLResourcePatternTypes\n *\n * Enum for ACL Resource Pattern Type\n * @readonly\n * @enum {ACLResourcePatternTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any PatternType which this client cannot understand, perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any resource pattern type.\n   */\n  ANY: 1,\n  /**\n   * In a filter, will perform pattern matching.\n   *\n   * e.g. Given a filter of {@code ResourcePatternFilter(TOPIC, \"payments.received\", MATCH)`}, the filter match\n   * any {@link ResourcePattern} that matches topic 'payments.received'. This might include:\n   * <ul>\n   *     <li>A Literal pattern with the same type and name, e.g. {@code ResourcePattern(TOPIC, \"payments.received\", LITERAL)}</li>\n   *     <li>A Wildcard pattern with the same type, e.g. {@code ResourcePattern(TOPIC, \"*\", LITERAL)}</li>\n   *     <li>A Prefixed pattern with the same type and where the name is a matching prefix, e.g. {@code ResourcePattern(TOPIC, \"payments.\", PREFIXED)}</li>\n   * </ul>\n   */\n  MATCH: 2,\n  /**\n   * A literal resource name.\n   *\n   * A literal name defines the full name of a resource, e.g. topic with name 'foo', or group with name 'bob'.\n   *\n   * The special wildcard character {@code *} can be used to represent a resource with any name.\n   */\n  LITERAL: 3,\n  /**\n   * A prefixed resource name.\n   *\n   * A prefixed name defines a prefix for a resource, e.g. topics with names that start with 'foo'.\n   */\n  PREFIXED: 4,\n}\n","const ACLResourceTypes = require('./aclResourceTypes')\n\n/**\n * @deprecated\n * @see https://github.com/tulios/kafkajs/issues/649\n *\n * Use ConfigResourceTypes or AclResourceTypes instead.\n */\nmodule.exports = ACLResourceTypes\n","module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n","const Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity, accessKeyId, secretAccessKey, sessionToken = '' }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, accessKeyId, secretAccessKey, sessionToken].join(US_ASCII_NULL_CHAR)\n    )\n  },\n})\n","module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n","module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n","/**\n * http://www.ietf.org/rfc/rfc5801.txt\n *\n * See org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerClientInitialResponse\n * for official Java client implementation.\n *\n * The mechanism consists of a message from the client to the server.\n * The client sends the \"n,\"\" GS header, followed by the authorizationIdentitty\n * prefixed by \"a=\" (if present), followed by \",\", followed by a US-ASCII SOH\n * character, followed by \"auth=Bearer \", followed by the token value, followed\n * by US-ASCII SOH character, followed by SASL extensions in OAuth \"friendly\"\n * format and then closed by two additionals US-ASCII SOH characters.\n *\n * SASL extensions are optional an must be expressed as key-value pairs in an\n * object. Each expression is converted as, the extension entry key, followed\n * by \"=\", followed by extension entry value. Each extension is separated by a\n * US-ASCII SOH character. If extensions are not present, their relative part\n * in the message, including the US-ASCII SOH character, is omitted.\n *\n * The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication token and verify that the\n * authentication credentials permit the client to login as the authorization\n * identity. If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst SEPARATOR = '\\u0001' // SOH - Start Of Header ASCII\n\nfunction formatExtensions(extensions) {\n  let msg = ''\n\n  if (extensions == null) {\n    return msg\n  }\n\n  let prefix = ''\n  for (const k in extensions) {\n    msg += `${prefix}${k}=${extensions[k]}`\n    prefix = SEPARATOR\n  }\n\n  return msg\n}\n\nmodule.exports = async ({ authorizationIdentity = null }, oauthBearerToken) => {\n  const authzid = authorizationIdentity == null ? '' : `\"a=${authorizationIdentity}`\n  let ext = formatExtensions(oauthBearerToken.extensions)\n  if (ext.length > 0) {\n    ext = `${SEPARATOR}${ext}`\n  }\n\n  const oauthMsg = `n,${authzid},${SEPARATOR}auth=Bearer ${oauthBearerToken.value}${ext}${SEPARATOR}${SEPARATOR}`\n\n  return {\n    encode: async () => {\n      return new Encoder().writeBytes(Buffer.from(oauthMsg))\n    },\n  }\n}\n","module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n","module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n","/**\n * http://www.ietf.org/rfc/rfc2595.txt\n *\n * The mechanism consists of a single message from the client to the\n * server.  The client sends the authorization identity (identity to\n * login as), followed by a US-ASCII NUL character, followed by the\n * authentication identity (identity whose password will be used),\n * followed by a US-ASCII NUL character, followed by the clear-text\n * password.  The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication identity and password with\n * the system authentication database and verify that the authentication\n * credentials permit the client to login as the authorization identity.\n * If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity = null, username, password }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, username, password].join(US_ASCII_NULL_CHAR)\n    )\n  },\n})\n","module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n","const Encoder = require('../../../encoder')\n\nmodule.exports = ({ finalMessage }) => ({\n  encode: async () => new Encoder().writeBytes(finalMessage),\n})\n","module.exports = require('../firstMessage/response')\n","/**\n * https://tools.ietf.org/html/rfc5802\n *\n * First, the client sends the \"client-first-message\" containing:\n *\n *  -> a GS2 header consisting of a flag indicating whether channel\n * binding is supported-but-not-used, not supported, or used, and an\n * optional SASL authorization identity;\n *\n *  -> SCRAM username and a random, unique nonce attributes.\n *\n * Note that the client's first message will always start with \"n\", \"y\",\n * or \"p\"; otherwise, the message is invalid and authentication MUST\n * fail.  This is important, as it allows for GS2 extensibility (e.g.,\n * to add support for security layers).\n */\n\nconst Encoder = require('../../../encoder')\n\nmodule.exports = ({ clientFirstMessage }) => ({\n  encode: async () => new Encoder().writeBytes(clientFirstMessage),\n})\n","/* eslint no-unused-vars: [\"error\", { \"varsIgnorePattern\": \"_\" }] */\n\nconst Decoder = require('../../../decoder')\n\nconst ENTRY_REGEX = /^([rsiev])=(.*)$/\n\nmodule.exports = {\n  decode: async rawData => {\n    return new Decoder(rawData).readBytes()\n  },\n  parse: async data => {\n    const processed = data\n      .toString()\n      .split(',')\n      .map(str => {\n        const [_, key, value] = str.match(ENTRY_REGEX)\n        return [key, value]\n      })\n      .reduce((obj, entry) => ({ ...obj, [entry[0]]: entry[1] }), {})\n\n    return { original: data.toString(), ...processed }\n  },\n}\n","module.exports = {\n  firstMessage: {\n    request: require('./firstMessage/request'),\n    response: require('./firstMessage/response'),\n  },\n  finalMessage: {\n    request: require('./finalMessage/request'),\n    response: require('./finalMessage/response'),\n  },\n}\n","/**\n * Enum for timestamp types\n * @readonly\n * @enum {TimestampType}\n */\nmodule.exports = {\n  // Timestamp type is unknown\n  NO_TIMESTAMP: -1,\n\n  // Timestamp relates to message creation time as set by a Kafka client\n  CREATE_TIME: 0,\n\n  // Timestamp relates to the time a message was appended to a Kafka log\n  LOG_APPEND_TIME: 1,\n}\n","module.exports = {\n  maxRetryTime: 30 * 1000,\n  initialRetryTime: 300,\n  factor: 0.2, // randomization factor\n  multiplier: 2, // exponential factor\n  retries: 5, // max retries\n}\n","module.exports = {\n  maxRetryTime: 1000,\n  initialRetryTime: 50,\n  factor: 0.02, // randomization factor\n  multiplier: 1.5, // exponential factor\n  retries: 15, // max retries\n}\n","const { KafkaJSNumberOfRetriesExceeded, KafkaJSNonRetriableError } = require('../errors')\n\nconst isTestMode = process.env.NODE_ENV === 'test'\nconst RETRY_DEFAULT = isTestMode ? require('./defaults.test') : require('./defaults')\n\nconst random = (min, max) => {\n  return Math.random() * (max - min) + min\n}\n\nconst randomFromRetryTime = (factor, retryTime) => {\n  const delta = factor * retryTime\n  return Math.ceil(random(retryTime - delta, retryTime + delta))\n}\n\nconst UNRECOVERABLE_ERRORS = ['RangeError', 'ReferenceError', 'SyntaxError', 'TypeError']\nconst isErrorUnrecoverable = e => UNRECOVERABLE_ERRORS.includes(e.name)\nconst isErrorRetriable = error =>\n  (error.retriable || error.retriable !== false) && !isErrorUnrecoverable(error)\n\nconst createRetriable = (configs, resolve, reject, fn) => {\n  let aborted = false\n  const { factor, multiplier, maxRetryTime, retries } = configs\n\n  const bail = error => {\n    aborted = true\n    reject(error || new Error('Aborted'))\n  }\n\n  const calculateExponentialRetryTime = retryTime => {\n    return Math.min(randomFromRetryTime(factor, retryTime) * multiplier, maxRetryTime)\n  }\n\n  const retry = (retryTime, retryCount = 0) => {\n    if (aborted) return\n\n    const nextRetryTime = calculateExponentialRetryTime(retryTime)\n    const shouldRetry = retryCount < retries\n\n    const scheduleRetry = () => {\n      setTimeout(() => retry(nextRetryTime, retryCount + 1), retryTime)\n    }\n\n    fn(bail, retryCount, retryTime)\n      .then(resolve)\n      .catch(e => {\n        if (isErrorRetriable(e)) {\n          if (shouldRetry) {\n            scheduleRetry()\n          } else {\n            reject(new KafkaJSNumberOfRetriesExceeded(e, { retryCount, retryTime }))\n          }\n        } else {\n          reject(new KafkaJSNonRetriableError(e))\n        }\n      })\n  }\n\n  return retry\n}\n\n/**\n * @typedef {(fn: (bail: (err: Error) => void, retryCount: number, retryTime: number) => any) => Promise<ReturnType<fn>>} Retrier\n */\n\n/**\n * @param {import(\"../../types\").RetryOptions} [opts]\n * @returns {Retrier}\n */\nmodule.exports = (opts = {}) => fn => {\n  return new Promise((resolve, reject) => {\n    const configs = Object.assign({}, RETRY_DEFAULT, opts)\n    const start = createRetriable(configs, resolve, reject, fn)\n    start(randomFromRetryTime(configs.factor, configs.initialRetryTime))\n  })\n}\n","module.exports = (a, b) => {\n  const result = []\n  const length = a.length\n  let i = 0\n\n  while (i < length) {\n    if (b.indexOf(a[i]) === -1) {\n      result.push(a[i])\n    }\n    i += 1\n  }\n\n  return result\n}\n","const defaultErrorHandler = e => {\n  throw e\n}\n\n/**\n * Generator that processes the given promises, and yields their result in the order of them resolving.\n *\n * @template T\n * @param {Promise<T>[]} promises promises to process\n * @param {(err: Error) => any} [handleError] optional error handler\n * @returns {Generator<Promise<T>>}\n */\nfunction* BufferedAsyncIterator(promises, handleError = defaultErrorHandler) {\n  /** Queue of promises in order of resolution */\n  const promisesQueue = []\n  /** Queue of {resolve, reject} in the same order as `promisesQueue` */\n  const resolveRejectQueue = []\n\n  promises.forEach(promise => {\n    // Create a new promise into the promises queue, and keep the {resolve,reject}\n    // in the resolveRejectQueue\n    let resolvePromise\n    let rejectPromise\n    promisesQueue.push(\n      new Promise((resolve, reject) => {\n        resolvePromise = resolve\n        rejectPromise = reject\n      })\n    )\n    resolveRejectQueue.push({ resolve: resolvePromise, reject: rejectPromise })\n\n    // When the promise resolves pick the next available {resolve, reject}, and\n    // through that resolve the next promise in the queue\n    promise.then(\n      result => {\n        const { resolve } = resolveRejectQueue.pop()\n        resolve(result)\n      },\n      async err => {\n        const { reject } = resolveRejectQueue.pop()\n        try {\n          await handleError(err)\n          reject(err)\n        } catch (newError) {\n          reject(newError)\n        }\n      }\n    )\n  })\n\n  // While there are promises left pick the next one to yield\n  // The caller will then wait for the value to resolve.\n  while (promisesQueue.length > 0) {\n    const nextPromise = promisesQueue.pop()\n    yield nextPromise\n  }\n}\n\nmodule.exports = BufferedAsyncIterator\n","const { KafkaJSNonRetriableError } = require('../errors')\n\nconst REJECTED_ERROR = new KafkaJSNonRetriableError(\n  'Queued function aborted due to earlier promise rejection'\n)\nfunction NOOP() {}\n\nconst concurrency = ({ limit, onChange = NOOP } = {}) => {\n  if (isNaN(limit) || typeof limit !== 'number' || limit < 1) {\n    throw new KafkaJSNonRetriableError(`\"limit\" cannot be less than 1`)\n  }\n\n  let waiting = []\n  let semaphore = 0\n\n  const clear = () => {\n    for (const lazyAction of waiting) {\n      lazyAction((_1, _2, reject) => reject(REJECTED_ERROR))\n    }\n    waiting = []\n    semaphore = 0\n  }\n\n  const next = () => {\n    semaphore--\n    onChange(semaphore)\n\n    if (waiting.length > 0) {\n      const lazyAction = waiting.shift()\n      lazyAction()\n    }\n  }\n\n  const invoke = (action, resolve, reject) => {\n    semaphore++\n    onChange(semaphore)\n\n    action()\n      .then(result => {\n        resolve(result)\n        next()\n      })\n      .catch(error => {\n        reject(error)\n        clear()\n      })\n  }\n\n  const push = (action, resolve, reject) => {\n    if (semaphore < limit) {\n      invoke(action, resolve, reject)\n    } else {\n      waiting.push(override => {\n        const execute = override || invoke\n        execute(action, resolve, reject)\n      })\n    }\n  }\n\n  return action => new Promise((resolve, reject) => push(action, resolve, reject))\n}\n\nmodule.exports = concurrency\n","/**\n * Flatten the given arrays into a new array\n *\n * @param {Array<Array<T>>} arrays\n * @returns {Array<T>}\n * @template T\n */\nfunction flatten(arrays) {\n  return [].concat.apply([], arrays)\n}\n\nmodule.exports = flatten\n","module.exports = async (array, groupFn) => {\n  const result = new Map()\n\n  for (const item of array) {\n    const group = await Promise.resolve(groupFn(item))\n    result.set(group, result.has(group) ? [...result.get(group), item] : [item])\n  }\n\n  return result\n}\n","const { format } = require('util')\nconst { KafkaJSLockTimeout } = require('../errors')\n\nconst PRIVATE = {\n  LOCKED: Symbol('private:Lock:locked'),\n  TIMEOUT: Symbol('private:Lock:timeout'),\n  WAITING: Symbol('private:Lock:waiting'),\n  TIMEOUT_ERROR_MESSAGE: Symbol('private:Lock:timeoutErrorMessage'),\n}\n\nconst TIMEOUT_MESSAGE = 'Timeout while acquiring lock (%d waiting locks)'\n\nmodule.exports = class Lock {\n  constructor({ timeout, description = null } = {}) {\n    if (typeof timeout !== 'number') {\n      throw new TypeError(`'timeout' is not a number, received '${typeof timeout}'`)\n    }\n\n    this[PRIVATE.LOCKED] = false\n    this[PRIVATE.TIMEOUT] = timeout\n    this[PRIVATE.WAITING] = new Set()\n    this[PRIVATE.TIMEOUT_ERROR_MESSAGE] = () => {\n      const timeoutMessage = format(TIMEOUT_MESSAGE, this[PRIVATE.WAITING].size)\n      return description ? `${timeoutMessage}: \"${description}\"` : timeoutMessage\n    }\n  }\n\n  async acquire() {\n    return new Promise((resolve, reject) => {\n      if (!this[PRIVATE.LOCKED]) {\n        this[PRIVATE.LOCKED] = true\n        return resolve()\n      }\n\n      let timeoutId = null\n      const tryToAcquire = async () => {\n        if (!this[PRIVATE.LOCKED]) {\n          this[PRIVATE.LOCKED] = true\n          clearTimeout(timeoutId)\n          this[PRIVATE.WAITING].delete(tryToAcquire)\n          return resolve()\n        }\n      }\n\n      this[PRIVATE.WAITING].add(tryToAcquire)\n      timeoutId = setTimeout(() => {\n        // The message should contain the number of waiters _including_ this one\n        const error = new KafkaJSLockTimeout(this[PRIVATE.TIMEOUT_ERROR_MESSAGE]())\n        this[PRIVATE.WAITING].delete(tryToAcquire)\n        reject(error)\n      }, this[PRIVATE.TIMEOUT])\n    })\n  }\n\n  async release() {\n    this[PRIVATE.LOCKED] = false\n    const waitingLock = this[PRIVATE.WAITING].values().next().value\n\n    if (waitingLock) {\n      return waitingLock()\n    }\n  }\n}\n","/**\n * @exports Long\n * @class A Long class for representing a 64 bit int (BigInt)\n * @param {bigint} value The value of the 64 bit int\n * @constructor\n */\nclass Long {\n  constructor(value) {\n    this.value = value\n  }\n\n  /**\n   * @function isLong\n   * @param {*} obj Object\n   * @returns {boolean}\n   * @inner\n   */\n  static isLong(obj) {\n    return typeof obj.value === 'bigint'\n  }\n\n  /**\n   * @param {number} value\n   * @returns {!Long}\n   * @inner\n   */\n  static fromBits(value) {\n    return new Long(BigInt(value))\n  }\n\n  /**\n   * @param {number} value\n   * @returns {!Long}\n   * @inner\n   */\n  static fromInt(value) {\n    if (isNaN(value)) return Long.ZERO\n\n    return new Long(BigInt.asIntN(64, BigInt(value)))\n  }\n\n  /**\n   * @param {number} value\n   * @returns {!Long}\n   * @inner\n   */\n  static fromNumber(value) {\n    if (isNaN(value)) return Long.ZERO\n\n    return new Long(BigInt(value))\n  }\n\n  /**\n   * @function\n   * @param {bigint|number|string|Long} val\n   * @returns {!Long}\n   * @inner\n   */\n  static fromValue(val) {\n    if (typeof val === 'number') return this.fromNumber(val)\n    if (typeof val === 'string') return this.fromString(val)\n    if (typeof val === 'bigint') return new Long(val)\n    if (this.isLong(val)) return new Long(BigInt(val.value))\n\n    return new Long(BigInt(val))\n  }\n\n  /**\n   * @param {string} str\n   * @returns {!Long}\n   * @inner\n   */\n  static fromString(str) {\n    if (str.length === 0) throw Error('empty string')\n    if (str === 'NaN' || str === 'Infinity' || str === '+Infinity' || str === '-Infinity')\n      return Long.ZERO\n    return new Long(BigInt(str))\n  }\n\n  /**\n   * Tests if this Long's value equals zero.\n   * @returns {boolean}\n   */\n  isZero() {\n    return this.value === BigInt(0)\n  }\n\n  /**\n   * Tests if this Long's value is negative.\n   * @returns {boolean}\n   */\n  isNegative() {\n    return this.value < BigInt(0)\n  }\n\n  /**\n   * Converts the Long to a string.\n   * @returns {string}\n   * @override\n   */\n  toString() {\n    return String(this.value)\n  }\n\n  /**\n   * Converts the Long to the nearest floating-point representation (double, 53-bit mantissa)\n   * @returns {number}\n   * @override\n   */\n  toNumber() {\n    return Number(this.value)\n  }\n\n  /**\n   * Converts the Long to a 32 bit integer, assuming it is a 32 bit integer.\n   * @returns {number}\n   */\n  toInt() {\n    return Number(BigInt.asIntN(32, this.value))\n  }\n\n  /**\n   * Converts the Long to JSON\n   * @returns {string}\n   * @override\n   */\n  toJSON() {\n    return this.toString()\n  }\n\n  /**\n   * Returns this Long with bits shifted to the left by the given amount.\n   * @param {number|bigint} numBits Number of bits\n   * @returns {!Long} Shifted bigint\n   */\n  shiftLeft(numBits) {\n    return new Long(this.value << BigInt(numBits))\n  }\n\n  /**\n   * Returns this Long with bits arithmetically shifted to the right by the given amount.\n   * @param {number|bigint} numBits Number of bits\n   * @returns {!Long} Shifted bigint\n   */\n  shiftRight(numBits) {\n    return new Long(this.value >> BigInt(numBits))\n  }\n\n  /**\n   * Returns the bitwise OR of this Long and the specified.\n   * @param {bigint|number|string} other Other Long\n   * @returns {!Long}\n   */\n  or(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return Long.fromBits(this.value | other.value)\n  }\n\n  /**\n   * Returns the bitwise XOR of this Long and the given one.\n   * @param {bigint|number|string} other Other Long\n   * @returns {!Long}\n   */\n  xor(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return new Long(this.value ^ other.value)\n  }\n\n  /**\n   * Returns the bitwise AND of this Long and the specified.\n   * @param {bigint|number|string} other Other Long\n   * @returns {!Long}\n   */\n  and(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return new Long(this.value & other.value)\n  }\n\n  /**\n   * Returns the bitwise NOT of this Long.\n   * @returns {!Long}\n   */\n  not() {\n    return new Long(~this.value)\n  }\n\n  /**\n   * Returns this Long with bits logically shifted to the right by the given amount.\n   * @param {number|bigint} numBits Number of bits\n   * @returns {!Long} Shifted bigint\n   */\n  shiftRightUnsigned(numBits) {\n    return new Long(this.value >> BigInt.asUintN(64, BigInt(numBits)))\n  }\n\n  /**\n   * Tests if this Long's value equals the specified's.\n   * @param {bigint|number|string} other Other value\n   * @returns {boolean}\n   */\n  equals(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return this.value === other.value\n  }\n\n  /**\n   * Tests if this Long's value is greater than or equal the specified's.\n   * @param {!Long|number|string} other Other value\n   * @returns {boolean}\n   */\n  greaterThanOrEqual(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return this.value >= other.value\n  }\n\n  gte(other) {\n    return this.greaterThanOrEqual(other)\n  }\n\n  notEquals(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return !this.equals(/* validates */ other)\n  }\n\n  /**\n   * Returns the sum of this and the specified Long.\n   * @param {!Long|number|string} addend Addend\n   * @returns {!Long} Sum\n   */\n  add(addend) {\n    if (!Long.isLong(addend)) addend = Long.fromValue(addend)\n    return new Long(this.value + addend.value)\n  }\n\n  /**\n   * Returns the difference of this and the specified Long.\n   * @param {!Long|number|string} subtrahend Subtrahend\n   * @returns {!Long} Difference\n   */\n  subtract(subtrahend) {\n    if (!Long.isLong(subtrahend)) subtrahend = Long.fromValue(subtrahend)\n    return this.add(subtrahend.negate())\n  }\n\n  /**\n   * Returns the product of this and the specified Long.\n   * @param {!Long|number|string} multiplier Multiplier\n   * @returns {!Long} Product\n   */\n  multiply(multiplier) {\n    if (this.isZero()) return Long.ZERO\n    if (!Long.isLong(multiplier)) multiplier = Long.fromValue(multiplier)\n    return new Long(this.value * multiplier.value)\n  }\n\n  /**\n   * Returns this Long divided by the specified. The result is signed if this Long is signed or\n   *  unsigned if this Long is unsigned.\n   * @param {!Long|number|string} divisor Divisor\n   * @returns {!Long} Quotient\n   */\n  divide(divisor) {\n    if (!Long.isLong(divisor)) divisor = Long.fromValue(divisor)\n    if (divisor.isZero()) throw Error('division by zero')\n    return new Long(this.value / divisor.value)\n  }\n\n  /**\n   * Compares this Long's value with the specified's.\n   * @param {!Long|number|string} other Other value\n   * @returns {number} 0 if they are the same, 1 if the this is greater and -1\n   *  if the given one is greater\n   */\n  compare(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    if (this.value === other.value) return 0\n    if (this.value > other.value) return 1\n    if (other.value > this.value) return -1\n  }\n\n  /**\n   * Tests if this Long's value is less than the specified's.\n   * @param {!Long|number|string} other Other value\n   * @returns {boolean}\n   */\n  lessThan(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return this.value < other.value\n  }\n\n  /**\n   * Negates this Long's value.\n   * @returns {!Long} Negated Long\n   */\n  negate() {\n    if (this.equals(Long.MIN_VALUE)) {\n      return Long.MIN_VALUE\n    }\n    return this.not().add(Long.ONE)\n  }\n\n  /**\n   * Gets the high 32 bits as a signed integer.\n   * @returns {number} Signed high bits\n   */\n  getHighBits() {\n    return Number(BigInt.asIntN(32, this.value >> BigInt(32)))\n  }\n\n  /**\n   * Gets the low 32 bits as a signed integer.\n   * @returns {number} Signed low bits\n   */\n  getLowBits() {\n    return Number(BigInt.asIntN(32, this.value))\n  }\n}\n\n/**\n * Minimum signed value.\n * @type {bigint}\n */\nLong.MIN_VALUE = new Long(BigInt('-9223372036854775808'))\n\n/**\n * Maximum signed value.\n * @type {bigint}\n */\nLong.MAX_VALUE = new Long(BigInt('9223372036854775807'))\n\n/**\n * Signed zero.\n * @type {Long}\n */\nLong.ZERO = Long.fromInt(0)\n\n/**\n * Signed one.\n * @type {!Long}\n */\nLong.ONE = Long.fromInt(1)\n\nmodule.exports = Long\n","/**\n * @template T\n * @param { (...args: any) => Promise<T> } [asyncFunction]\n * Promise returning function that will only ever be invoked sequentially.\n * @returns { (...args: any) => Promise<T> }\n * Function that may invoke asyncFunction if there is not a currently executing invocation.\n * Returns promise from the currently executing invocation.\n */\nmodule.exports = asyncFunction => {\n  let promise = null\n\n  return (...args) => {\n    if (promise == null) {\n      promise = asyncFunction(...args).finally(() => (promise = null))\n    }\n    return promise\n  }\n}\n","/**\n * @param {T[]} array\n * @returns T[]\n * @template T\n */\nmodule.exports = array => {\n  if (!Array.isArray(array)) {\n    throw new TypeError(\"'array' is not an array\")\n  }\n\n  if (array.length < 2) {\n    return array\n  }\n\n  const copy = array.slice()\n\n  for (let i = copy.length - 1; i > 0; i--) {\n    const j = Math.floor(Math.random() * (i + 1))\n    const temp = copy[i]\n    copy[i] = copy[j]\n    copy[j] = temp\n  }\n\n  return copy\n}\n","module.exports = timeInMs =>\n  new Promise(resolve => {\n    setTimeout(resolve, timeInMs)\n  })\n","const { keys } = Object\nmodule.exports = object =>\n  keys(object).reduce((result, key) => ({ ...result, [object[key]]: key }), {})\n","const sleep = require('./sleep')\nconst { KafkaJSTimeout } = require('../errors')\n\nmodule.exports = (\n  fn,\n  { delay = 50, maxWait = 10000, timeoutMessage = 'Timeout', ignoreTimeout = false } = {}\n) => {\n  let timeoutId\n  let totalWait = 0\n  let fulfilled = false\n\n  const checkCondition = async (resolve, reject) => {\n    totalWait += delay\n    await sleep(delay)\n\n    try {\n      const result = await fn(totalWait)\n      if (result) {\n        fulfilled = true\n        clearTimeout(timeoutId)\n        return resolve(result)\n      }\n\n      checkCondition(resolve, reject)\n    } catch (e) {\n      fulfilled = true\n      clearTimeout(timeoutId)\n      reject(e)\n    }\n  }\n\n  return new Promise((resolve, reject) => {\n    checkCondition(resolve, reject)\n\n    if (ignoreTimeout) {\n      return\n    }\n\n    timeoutId = setTimeout(() => {\n      if (!fulfilled) {\n        return reject(new KafkaJSTimeout(timeoutMessage))\n      }\n    }, maxWait)\n  })\n}\n","const BASE_URL = 'https://kafka.js.org'\n\nmodule.exports = (path, hash) => `${BASE_URL}/${path}${hash ? '#' + hash : ''}`\n","import crypto from 'crypto'\nimport { urlAlphabet } from './url-alphabet/index.js'\nconst POOL_SIZE_MULTIPLIER = 128\nlet pool, poolOffset\nlet fillPool = bytes => {\n  if (!pool || pool.length < bytes) {\n    pool = Buffer.allocUnsafe(bytes * POOL_SIZE_MULTIPLIER)\n    crypto.randomFillSync(pool)\n    poolOffset = 0\n  } else if (poolOffset + bytes > pool.length) {\n    crypto.randomFillSync(pool)\n    poolOffset = 0\n  }\n  poolOffset += bytes\n}\nlet random = bytes => {\n  fillPool((bytes -= 0))\n  return pool.subarray(poolOffset - bytes, poolOffset)\n}\nlet customRandom = (alphabet, defaultSize, getRandom) => {\n  let mask = (2 << (31 - Math.clz32((alphabet.length - 1) | 1))) - 1\n  let step = Math.ceil((1.6 * mask * defaultSize) / alphabet.length)\n  return (size = defaultSize) => {\n    let id = ''\n    while (true) {\n      let bytes = getRandom(step)\n      let i = step\n      while (i--) {\n        id += alphabet[bytes[i] & mask] || ''\n        if (id.length === size) return id\n      }\n    }\n  }\n}\nlet customAlphabet = (alphabet, size = 21) =>\n  customRandom(alphabet, size, random)\nlet nanoid = (size = 21) => {\n  fillPool((size -= 0))\n  let id = ''\n  for (let i = poolOffset - size; i < poolOffset; i++) {\n    id += urlAlphabet[pool[i] & 63]\n  }\n  return id\n}\nexport { nanoid, customAlphabet, customRandom, urlAlphabet, random }\n","let urlAlphabet =\n  'useandom-26T198340PX75pxJACKVERYMINDBUSHWOLF_GQZbfghjklqvwyzrict'\nexport { urlAlphabet }\n"],"sourceRoot":""}